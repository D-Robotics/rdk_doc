# -*- coding: utf-8 -*-
import requests # å¯¼å…¥ç½‘ç»œè¯·æ±‚åº“
from urllib.parse import urlparse, quote
import os # å¯¼å…¥æ“ä½œç³»ç»Ÿæ¥å£åº“
import re # å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼åº“
import concurrent.futures # å¯¼å…¥å¹¶å‘åº“
from concurrent.futures import ThreadPoolExecutor # ä»å¹¶å‘åº“å¯¼å…¥çº¿ç¨‹æ± æ‰§è¡Œå™¨
from tqdm import tqdm # å¯¼å…¥tqdmåº“ç”¨äºæ˜¾ç¤ºè¿›åº¦æ¡

# --- é…ç½®é¡¹ ---
# åŒ…å« Markdown æ–‡ä»¶çš„æ ¹ç›®å½•åˆ—è¡¨
MARKDOWN_ROOT_DIRS = ["docs", "docs_s", "i18n/en/docusaurus-plugin-content-docs", "i18n/en/docusaurus-plugin-content-docs-docs_s"] # è¯·ä¿®æ”¹ä¸ºæ‚¨çš„ç›®å½•åˆ—è¡¨
# å›¾ç‰‡å°†ä¸‹è½½åˆ°çš„å­ç›®å½•åã€‚
IMAGE_OUTPUT_SUBDIR = "img"
# IMAGE_OUTPUT_SUBDIR çš„çˆ¶æ–‡ä»¶å¤¹å
STATIC_DIR_NAME = "static"

# ç”¨äºæŸ¥æ‰¾æŒ‡å‘é˜¿é‡Œäº‘ OSS çš„ Markdown å›¾ç‰‡é“¾æ¥çš„æ­£åˆ™è¡¨è¾¾å¼
# å®ƒæ•è·:
#   group(1): alt æ–‡æœ¬ (å›¾ç‰‡æ›¿ä»£æ–‡æœ¬)
#   group(2): å®Œæ•´çš„é˜¿é‡Œäº‘ OSS URL
ALIYUN_OSS_IMAGE_PATTERN = re.compile(
    r"!\[(.*?)\]\((https?://(?:[a-zA-Z0-9-]+\.)*aliyuncs\.com(?:/[^\)]*?))\)"
)
# æœ€å¤§å¹¶å‘ä¸‹è½½æ•°
MAX_WORKERS = 5
# è¯·æ±‚è¶…æ—¶æ—¶é—´ (ç§’)
REQUEST_TIMEOUT = 15
# é”™è¯¯æ—¥å¿—æ–‡ä»¶å
ERROR_LOG_FILE = "error.txt"

def log_error(message, error_list):
    """è®°å½•é”™è¯¯åˆ°åˆ—è¡¨å¹¶æ‰“å°åˆ°æ§åˆ¶å°"""
    print(message) # æ§åˆ¶å°å³æ—¶åé¦ˆ
    error_list.append(message)

def download_image(session, url, save_path, error_list):
    """
    ä» URL ä¸‹è½½å›¾ç‰‡å¹¶å°†å…¶ä¿å­˜åˆ°æœ¬åœ°è·¯å¾„ã€‚
    ä½¿ç”¨æä¾›çš„ requests.Session å¯¹è±¡ã€‚
    å¦‚æœä¸‹è½½æˆåŠŸæˆ–å›¾ç‰‡å·²å­˜åœ¨ï¼Œåˆ™è¿”å› Trueï¼Œå¦åˆ™è¿”å› False å¹¶è®°å½•é”™è¯¯ã€‚
    """
    if os.path.exists(save_path):
        # print(f"å›¾ç‰‡å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {save_path}") # å¯ä»¥å–æ¶ˆè¿™è¡Œæ³¨é‡Šæ¥æŸ¥çœ‹è·³è¿‡ä¿¡æ¯
        return True
    try:
        # urlparse ä¼šå°† URL åˆ†è§£ä¸ºå„ä¸ªéƒ¨åˆ† (åè®®, åŸŸå, è·¯å¾„, ç­‰)ã€‚
        parsed_url = urlparse(url)
        # æˆ‘ä»¬åªå¯¹è·¯å¾„éƒ¨åˆ†è¿›è¡Œç¼–ç ï¼Œå¹¶ç¡®ä¿'/'ä¸è¢«ç¼–ç ï¼Œä»¥ä¿ç•™ç›®å½•ç»“æ„ã€‚
        quoted_path = quote(parsed_url.path, safe='/')
        # ä½¿ç”¨ç¼–ç åçš„è·¯å¾„é‡æ–°ç»„è£…æˆä¸€ä¸ªå®‰å…¨çš„ URLã€‚
        safe_url_for_request = parsed_url._replace(path=quoted_path).geturl()

        # print(f"å¼€å§‹ä¸‹è½½: {safe_url_for_request} åˆ° {save_path}")
        # ä½¿ç”¨æ–°ç”Ÿæˆçš„ safe_url_for_request è¿›è¡Œä¸‹è½½
        response = session.get(safe_url_for_request, timeout=REQUEST_TIMEOUT)
        response.raise_for_status()  # å¯¹äºé”™è¯¯çš„HTTPçŠ¶æ€ç ï¼ŒæŠ›å‡ºå¼‚å¸¸

        os.makedirs(os.path.dirname(save_path), exist_ok=True)

        with open(save_path, 'wb') as f:
            f.write(response.content)
        # print(f"ä¸‹è½½æˆåŠŸ: {save_path}")
        return True
    except requests.exceptions.Timeout:
        log_error(f"ä¸‹è½½è¶…æ—¶: {url}", error_list)
    except requests.exceptions.RequestException as e:
        log_error(f"ä¸‹è½½å¤±è´¥: {url}, é”™è¯¯: {e}", error_list)
    except IOError as e:
        log_error(f"æ–‡ä»¶ä¿å­˜å¤±è´¥: {save_path}, é”™è¯¯: {e}", error_list)
    except Exception as e:
        log_error(f"ä¸‹è½½ {url} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", error_list)
    return False

def process_markdown_file(md_path, image_output_abs_dir, pattern_re, session, executor, error_list):
    """
    å¤„ç†å•ä¸ª Markdown æ–‡ä»¶ã€‚
    ä¼šæŸ¥æ‰¾ã€ä¸‹è½½å›¾ç‰‡å¹¶æ›¿æ¢é“¾æ¥ï¼ŒåŒæ—¶è®°å½•è¿‡ç¨‹ä¸­å‘ç”Ÿçš„é”™è¯¯ã€‚
    """
    try:
        with open(md_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except IOError as e:
        log_error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {md_path}, é”™è¯¯: {e}", error_list)
        return

    images_to_download_info = {}
    unique_urls_to_download = {}
    matches = list(pattern_re.finditer(content))

    if not matches:
        return

    for match in matches:
        alt_text = match.group(1)
        original_url = match.group(2)
        try:
            parsed_url = urlparse(original_url)

            # 1. è·å– URL ä¸­çš„å®Œæ•´ç›¸å¯¹è·¯å¾„ï¼Œå¹¶ç§»é™¤å¼€å¤´çš„ '/'
            #    ä¾‹å¦‚ï¼Œä» "/path/to/image.png" å˜ä¸º "path/to/image.png"
            relative_image_path = parsed_url.path.lstrip('/')

            # 2. å®‰å…¨æ€§æ£€æŸ¥ï¼šé˜²æ­¢è·¯å¾„éå†æ”»å‡» (e.g., ../../..)
            #    æˆ‘ä»¬æ‹’ç»ä»»ä½•åŒ…å« '..' æˆ–ä»¥ '/' å¼€å¤´çš„è·¯å¾„æ¥å†™å…¥æ–‡ä»¶ç³»ç»Ÿ
            if '..' in relative_image_path:
                log_error(f"è­¦å‘Š: æ£€æµ‹åˆ°ä¸å®‰å…¨çš„å›¾ç‰‡è·¯å¾„ '{original_url}' (åœ¨ {md_path} ä¸­)ï¼Œå·²è·³è¿‡ã€‚", error_list)
                continue

            # 3. æ„å»º Markdown é“¾æ¥è·¯å¾„å’Œæœ¬åœ°æ–‡ä»¶ä¿å­˜è·¯å¾„
            #    local_md_path ç”¨äºå†™å…¥ Markdown æ–‡ä»¶ï¼Œå¿…é¡»ä½¿ç”¨ '/' ä½œä¸ºåˆ†éš”ç¬¦
            local_md_path = f"/{IMAGE_OUTPUT_SUBDIR}/{relative_image_path}"
            #    full_save_path ç”¨äºä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿï¼Œä½¿ç”¨ os.path.join å…¼å®¹ä¸åŒæ“ä½œç³»ç»Ÿ
            full_save_path = os.path.join(image_output_abs_dir, relative_image_path)

        except Exception as e:
            log_error(f"è­¦å‘Š: è§£æ URL '{original_url}' (åœ¨ {md_path} ä¸­) å¤±è´¥: {e}ï¼Œè·³è¿‡ã€‚", error_list)
            continue

        images_to_download_info[original_url] = {
            "alt_text": alt_text,
            "local_md_path": local_md_path,
            "full_save_path": full_save_path
        }
        if not os.path.exists(full_save_path):
            unique_urls_to_download[original_url] = full_save_path


    future_to_url = {}
    if unique_urls_to_download:
        for original_url, full_save_path in unique_urls_to_download.items():
            future = executor.submit(download_image, session, original_url, full_save_path, error_list)
            future_to_url[future] = original_url

    download_results = {}
    for future in concurrent.futures.as_completed(future_to_url):
        url = future_to_url[future]
        try:
            success = future.result()
            download_results[url] = success
        except Exception as exc:
            log_error(f"æ–‡ä»¶ {md_path}: å›¾ç‰‡ {url} ä¸‹è½½ä»»åŠ¡äº§ç”Ÿå¼‚å¸¸: {exc}", error_list)
            download_results[url] = False

    content_changed_flag = [False]

    def replace_image_link(matchobj):
        alt_text = matchobj.group(1)
        original_url = matchobj.group(2)

        if original_url in images_to_download_info:
            img_info = images_to_download_info[original_url]
            full_save_path = img_info["full_save_path"]

            if os.path.exists(full_save_path): 
                local_md_path = img_info["local_md_path"]
                new_tag = f"![{alt_text}]({local_md_path})"
                if new_tag != matchobj.group(0):
                    content_changed_flag[0] = True
                return new_tag
            else:
                return matchobj.group(0)
        return matchobj.group(0)

    new_content = pattern_re.sub(replace_image_link, content)

    if content_changed_flag[0]:
        try:
            with open(md_path, 'w', encoding='utf-8') as f:
                f.write(new_content)
        except IOError as e:
            log_error(f"å†™å…¥æ–‡ä»¶å¤±è´¥: {md_path}, é”™è¯¯: {e}", error_list)

if __name__ == "__main__":
    error_logs = [] # åˆå§‹åŒ–é”™è¯¯æ—¥å¿—åˆ—è¡¨

    script_dir = os.path.dirname(os.path.abspath(__file__))
    image_output_abs = os.path.abspath(os.path.join(script_dir, STATIC_DIR_NAME, IMAGE_OUTPUT_SUBDIR))

    print(f"ç»Ÿä¸€å›¾ç‰‡è¾“å‡ºç›®å½•: {image_output_abs}")
    os.makedirs(image_output_abs, exist_ok=True)

    all_md_file_paths = []
    print("å¼€å§‹æ‰«æ Markdown æ–‡ä»¶...")
    for root_dir_name in MARKDOWN_ROOT_DIRS:
        markdown_root_abs = os.path.abspath(os.path.join(script_dir, root_dir_name))
        if not os.path.isdir(markdown_root_abs):
            log_error(f"é”™è¯¯: Markdown æ ¹ç›®å½• '{markdown_root_abs}' (æ¥è‡ªé…ç½® '{root_dir_name}') ä¸å­˜åœ¨æˆ–ä¸æ˜¯ä¸€ä¸ªç›®å½•ã€‚", error_logs)
            continue
        print(f"æ‰«æç›®å½•: {markdown_root_abs}")
        for dirpath, _, filenames in os.walk(markdown_root_abs):
            for filename in filenames:
                if filename.endswith(".md"):
                    all_md_file_paths.append(os.path.join(dirpath, filename))

    total_md_files = len(all_md_file_paths)

    if total_md_files == 0:
        print("æœªåœ¨æŒ‡å®šç›®å½•ä¸­æ‰¾åˆ°ä»»ä½• Markdown æ–‡ä»¶è¿›è¡Œå¤„ç†ã€‚")
        if error_logs: # å¦‚æœä»…æœ‰ç›®å½•ä¸å­˜åœ¨çš„é”™è¯¯
             with open(ERROR_LOG_FILE, "w", encoding="utf-8") as f_err:
                for log_entry in error_logs:
                    f_err.write(log_entry + "\n")
             print(f"å‘ç°é”™è¯¯ï¼Œè¯¦æƒ…è¯·è§ {ERROR_LOG_FILE}ã€‚")
        exit(0)

    print(f"å…±æ‰¾åˆ° {total_md_files} ä¸ª Markdown æ–‡ä»¶å¾…å¤„ç†ã€‚")

    with tqdm(total=total_md_files, desc="å¤„ç†MDæ–‡ä»¶", unit="file") as progress_bar:
        with requests.Session() as http_session:
            http_session.headers.update({
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            })
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                for md_path in all_md_file_paths:
                    progress_bar.set_description(f"å¤„ç†ä¸­: {os.path.basename(md_path)}")
                    process_markdown_file(
                        md_path,
                        image_output_abs,
                        ALIYUN_OSS_IMAGE_PATTERN,
                        http_session,
                        executor,
                        error_logs
                    )
                    progress_bar.update(1)

    if error_logs:
        try:
            with open(ERROR_LOG_FILE, "w", encoding="utf-8") as f_err:
                f_err.write("--- é”™è¯¯æ—¥å¿— ---\n")
                for log_entry in error_logs:
                    f_err.write(log_entry + "\n")
            print(f"å¤„ç†å®Œæˆã€‚å‘ç° {len(error_logs)} ä¸ªé”™è¯¯ï¼Œè¯¦æƒ…è¯·è§ {ERROR_LOG_FILE}ã€‚")
        except IOError as e:
            print(f"è‡´å‘½é”™è¯¯: æ— æ³•å†™å…¥é”™è¯¯æ—¥å¿—æ–‡ä»¶ {ERROR_LOG_FILE}ã€‚é”™è¯¯: {e}")
            print("ä»¥ä¸‹æ˜¯æ”¶é›†åˆ°çš„é”™è¯¯ä¿¡æ¯:")
            for log_entry in error_logs:
                print(log_entry)
    else:
        print("ğŸ‰ æ‰€æœ‰å¤„ç†å®Œæˆï¼ŒæœªæŠ¥å‘Šé”™è¯¯ã€‚")