"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[70532],{28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>l});var i=t(96540);const s={},o=i.createContext(s);function r(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(o.Provider,{value:n},e.children)}},83076:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"Advanced_development/toolchain_development/intermediate/runtime_sample","title":"On-board Model Application Development Guide","description":"Model Inference DNN API Usage Example","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/07_Advanced_development/04_toolchain_development/intermediate/runtime_sample.md","sourceDirName":"07_Advanced_development/04_toolchain_development/intermediate","slug":"/Advanced_development/toolchain_development/intermediate/runtime_sample","permalink":"/rdk_doc/en/Advanced_development/toolchain_development/intermediate/runtime_sample","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1751254288000,"sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"supported_op_list","permalink":"/rdk_doc/en/Advanced_development/toolchain_development/intermediate/supported_op_list"},"next":{"title":"7.4.3.4 Advanced Guide","permalink":"/rdk_doc/en/Advanced_development/toolchain_development/expert/"}}');var s=t(74848),o=t(28453);const r={sidebar_position:4},l="On-board Model Application Development Guide",a={},d=[{value:"Model Inference DNN API Usage Example",id:"model-inference-dnn-api-usage-example",level:2},{value:"Overview",id:"overview",level:3},{value:"Sample Code Package Structure Introduction",id:"sample-code-package-structure-introduction",level:3},{value:"Environment Building",id:"environment-building",level:3},{value:"Development Board Preparation",id:"development-board-preparation",level:4},{value:"Compilation",id:"compilation",level:4},{value:"Example Usage",id:"example-usage",level:3},{value:"basic_samples Example",id:"basic_samples-example",level:4},{value:"quick_start",id:"quick_start",level:4},{value:"api_tutorial",id:"api_tutorial",level:4},{value:"advanced_samples",id:"advanced_samples",level:4},{value:"misc",id:"misc",level:4},{value:"Auxiliary tools and common operations",id:"auxiliary-tools-and-common-operations",level:3},{value:"Logs",id:"logs",level:4},{value:"Example Logs",id:"example-logs",level:4},{value:"Model Inference DNN API Logs",id:"model-inference-dnn-api-logs",level:4},{value:"Public model performance accuracy evaluation instructions",id:"public-model-performance-accuracy-evaluation-instructions",level:2},{value:"Public model performance accuracy indicatorsThe table below provides performance and accuracy indicators of typical deep neural network models on the X3 processor.",id:"public-model-performance-accuracy-indicatorsthe-table-below-provides-performance-and-accuracy-indicators-of-typical-deep-neural-network-models-on-the-x3-processor",level:3},{value:"Evaluation Method Description",id:"evaluation-method-description",level:3},{value:"Introduction",id:"introduction",level:4},{value:"Sample Models",id:"sample-models",level:4},{value:"Public Datasets",id:"public-datasets",level:4},{value:"Environment Setup",id:"environment-setup",level:4},{value:"Development Board Preparation",id:"development-board-preparation-1",level:4},{value:"Compiler Environment Preparation",id:"compiler-environment-preparation",level:4},{value:"Evaluation Example Usage Instructions",id:"evaluation-example-usage-instructions",level:4},{value:"Performance Evaluation",id:"performance-evaluation",level:4},{value:"Accuracy Evaluation",id:"accuracy-evaluation",level:4},{value:"Data Preprocessing",id:"data_preprocess",level:4},{value:"Data Mounting",id:"data-mounting",level:4},{value:"Model Inference",id:"model-inference",level:4},{value:"Accuracy Calculation",id:"accuracy-calculation",level:4},{value:"Model Integration",id:"model-integration",level:4},{value:"Adding Post-processing Files",id:"adding-post-processing-files",level:4},{value:"Add model running scripts and configuration files",id:"add-model-running-scripts-and-configuration-files",level:4},{value:"Auxiliary tools and common operations",id:"auxiliary-tools-and-common-operations-1",level:4},{value:"Instructions for using the logging system",id:"instructions-for-using-the-logging-system",level:4},{value:"Dump Tool",id:"dump-tool",level:4},{value:"Instructions for Model-on-Board Analysis Tool",id:"instructions-for-model-on-board-analysis-tool",level:2},{value:"Overview",id:"overview-1",level:3},{value:"Instructions for hrt_model_exec Tool",id:"instructions-for-hrt_model_exec-tool",level:3},{value:"Input Parameter Description",id:"input-parameter-description",level:4},{value:"Instructions",id:"instructions",level:4},{value:"<code>model_info</code>",id:"model_info",level:4},{value:"Supplementary Explanation of Input Parameters",id:"supplementary-explanation-of-input-parameters",level:4},{value:"<code>infer</code>",id:"infer",level:4},{value:"<code>Multi-input Model Explanation</code>",id:"multi-input-model-explanation",level:4},{value:"<code>Supplementary Explanation of Input Parameters</code>",id:"supplementary-explanation-of-input-parameters-1",level:4},{value:"<code>perf</code>",id:"perf",level:4},{value:"<code>Multi-thread Latency Data Explanation</code>",id:"multi-thread-latency-data-explanation",level:4},{value:"Supplementary explanation of input parameters",id:"supplementary-explanation-of-input-parameters-2",level:4},{value:"Instructions for using the hrt_bin_dump tool&quot;hrt_bin_dump&quot; is a layer dump tool for the PTQ debug model, and the output file of the tool is a binary file.",id:"instructions-for-using-the-hrt_bin_dump-toolhrt_bin_dump-is-a-layer-dump-tool-for-the-ptq-debug-model-and-the-output-file-of-the-tool-is-a-binary-file",level:3},{value:"Description of Input Parameters",id:"description-of-input-parameters",level:4},{value:"Instructions",id:"instructions-1",level:4},{value:"Example",id:"example",level:4}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"on-board-model-application-development-guide",children:"On-board Model Application Development Guide"})}),"\n",(0,s.jsx)(n.h2,{id:"model-inference-dnn-api-usage-example",children:"Model Inference DNN API Usage Example"}),"\n",(0,s.jsx)(n.h3,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"This section introduces the specific usage of the horizon_runtime_sample package for on-board model application development. Developers can experience and develop applications based on these examples to lower the development threshold."}),"\n",(0,s.jsx)(n.p,{children:"The sample package provides three types of examples:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Model inference DNN API usage examples"}),"\n",(0,s.jsx)(n.li,{children:"Special features examples such as custom operators (custom OP)"}),"\n",(0,s.jsx)(n.li,{children:"Miscellaneous examples for non-NV12 input models"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Please read on for more details."}),"\n",(0,s.jsx)(n.admonition,{title:"Tip",type:"tip",children:(0,s.jsxs)(n.p,{children:["To obtain the horizon_runtime_sample package, please refer to the ",(0,s.jsx)(n.a,{href:"/rdk_doc/en/Advanced_development/toolchain_development/intermediate/environment_config",children:"Deliverables Instructions"}),"."]})}),"\n",(0,s.jsx)(n.h3,{id:"sample-code-package-structure-introduction",children:"Sample Code Package Structure Introduction"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"  +---horizon_runtime_sample\n  |\u2500\u2500 README.md\n  \u251c\u2500\u2500 code                                      # Sample source code\n  \u2502   \u251c\u2500\u2500 00_quick_start                        # Quick start example: Using mobilenetv1 for single image inference and result parsing with the DNN API.\n  \u2502   \u2502   \u251c\u2500\u2500 CMakeLists.txt\n  \u2502   \u2502   \u251c\u2500\u2500 CMakeLists_x86.txt\n  \u2502   \u2502   \u2514\u2500\u2500 src\n  \u2502   \u251c\u2500\u2500 01_api_tutorial                       # DNN SDK API usage tutorial\n  \u2502   \u2502   \u251c\u2500\u2500 CMakeLists.txt\n  \u2502   \u2502   \u251c\u2500\u2500 mem\n  \u2502   \u2502   \u251c\u2500\u2500 model\n  \u2502   \u2502   \u251c\u2500\u2500 resize\n  \u2502   \u2502   \u251c\u2500\u2500 roi_infer\n  \u2502   \u2502   \u2514\u2500\u2500 tensor\n  \u2502   \u251c\u2500\u2500 02_advanced_samples                   # Special feature examples\n  \u2502   \u2502   \u251c\u2500\u2500 CMakeLists.txt\n  \u2502   \u2502   \u251c\u2500\u2500 custom_identity\n  \u2502   \u2502   \u251c\u2500\u2500 multi_input\n  \u2502   \u2502   \u251c\u2500\u2500 multi_model_batch\n  \u2502   \u2502   \u2514\u2500\u2500 nv12_batch\n  \u2502   \u251c\u2500\u2500 03_misc                               # Miscellaneous examples for non-NV12 input models.\n  \u2502   \u2502   \u251c\u2500\u2500 CMakeLists.txt\n  \u2502   \u2502   \u251c\u2500\u2500 lenet_gray\n  \u2502   \u2502   \u2514\u2500\u2500 resnet_feature\n  \u2502   \u251c\u2500\u2500 CMakeLists.txt\n  \u2502   \u251c\u2500\u2500 build_ultra.sh                        # Script for aarch64 compilation on RDK Ultra\n  \u2502   \u251c\u2500\u2500 build_xj3.sh                          # Script for aarch64 compilation on RDK X3\n  \u2502   \u2514\u2500\u2500 deps_gcc9.3                           # Third-party dependencies required for the sample code. Users should replace or trim as needed for their own projects.\n  \u251c\u2500\u2500 ultra\n  \u2502   \u251c\u2500\u2500 data                                  # Preloaded data files\n  \u2502   \u2502   \u251c\u2500\u2500 cls_images\n  \u2502   \u2502   \u251c\u2500\u2500 custom_identity_data\n  \u2502   \u2502   \u251c\u2500\u2500 det_images\n  \u2502   \u2502   \u2514\u2500\u2500 misc_data\n  \u2502   \u251c\u2500\u2500 model\n  \u2502   \u2502   \u251c\u2500\u2500 README.md\n  \u2502   \u2502   \u2514\u2500\u2500 runtime -> ../../../model_zoo/runtime/horizon_runtime_sample   # Soft link pointing to the model in the OE package. Board-side runtime environment requires users to specify the model path manually.\n  \u2502   \u251c\u2500\u2500 script                                # aarch64 sample execution scripts\n  \u2502   \u2502   \u251c\u2500\u2500 00_quick_start\n  \u2502   \u2502   \u251c\u2500\u2500 01_api_tutorial\n  \u2502   \u2502   \u251c\u2500\u2500 02_advanced_samples\n  \u2502   \u2502   \u251c\u2500\u2500 03_misc\n  \u2502   \u2502   \u2514\u2500\u2500 README.md\n  \u2502   \u2514\u2500\u2500 script_x86                            # x86 sample execution scripts\n  \u2502       \u251c\u2500\u2500 00_quick_start\n  \u2502       \u2514\u2500\u2500 README.md\n  \u251c\u2500\u2500 xj3\n  \u2502   \u251c\u2500\u2500 data                                  # Preloaded data files\n  \u2502   \u2502   \u251c\u2500\u2500 cls_images\n  \u2502   \u2502   \u251c\u2500\u2500 custom_identity_data\n  \u2502   \u2502   \u251c\u2500\u2500 det_images\n  \u2502   \u2502   \u2514\u2500\u2500 misc_data\n  \u2502   \u251c\u2500\u2500 model\n  \u2502   \u2502   \u251c\u2500\u2500 README.md\n  \u2502   \u2502   \u2514\u2500\u2500 runtime -> ../../../model_zoo/runtime/horizon_runtime_sample   # Soft link pointing to the model in the OE package. Board-side runtime environment requires users to specify the model path manually.\n  \u2502   \u251c\u2500\u2500 script                                # aarch64 sample execution scripts\n  \u2502   \u2502   \u251c\u2500\u2500 00_quick_start\n  \u2502   \u2502   \u251c\u2500\u2500 01_api_tutorial\n  \u2502   \u2502   \u251c\u2500\u2500 02_advanced_samples\n  \u2502   \u2502   \u251c\u2500\u2500 03_misc\n  \u2502   \u2502   \u2514\u2500\u2500 README.md\n  \u2502   \u2514\u2500\u2500 script_x86                            # x86 sample execution scripts\n  \u2502       \u251c\u2500\u2500 00_quick_start\n  \u2502       \u2514\u2500\u2500 README.md\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"00_quick_start"}),": Quick start using the ",(0,s.jsx)(n.code,{children:"dnn"})," API, demonstrating single-image inference with MobileNetV1 and result parsing."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"01_api_tutorial"}),": DNN SDK API usage tutorial, including ",(0,s.jsx)(n.strong,{children:"mem"}),", ",(0,s.jsx)(n.strong,{children:"model"}),", ",(0,s.jsx)(n.strong,{children:"resize"}),", ",(0,s.jsx)(n.strong,{children:"roi_infer"}),", and ",(0,s.jsx)(n.strong,{children:"tensor"})," components."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"02_advanced_samples"}),": Special feature examples, such as ",(0,s.jsx)(n.strong,{children:"custom_identity"}),", ",(0,s.jsx)(n.strong,{children:"multi_input"}),", ",(0,s.jsx)(n.strong,{children:"multi_model_batch"}),", and ",(0,s.jsx)(n.strong,{children:"nv12_batch"})," functionality."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"03_misc"}),": Miscellaneous examples for non-NV12 input models."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"xj3"}),": RDK X3 development board sample execution scripts, preloaded with data and related models."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ultra"}),": RDK Ultra development board sample execution scripts, preloaded with data and related models."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"build_xj3.sh"}),": One-click compilation script for RDK X3."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"build_ultra.sh"}),": One-click compilation script for RDK Ultra."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"deps/deps_gcc9.3"}),": Third-party dependencies required for the sample code. Users should replace or trim as needed for their own projects."]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{title:"Note",type:"caution",children:(0,s.jsxs)(n.p,{children:["For running on a private model, please refer to the sample code flow of ",(0,s.jsx)(n.code,{children:"00_quick_start/src/run_mobileNetV1_224x224.cc"})," for code rewriting. After successful compilation, you can test and verify on the development board!"]})}),"\n",(0,s.jsx)(n.h3,{id:"environment-building",children:"Environment Building"}),"\n",(0,s.jsx)(n.h4,{id:"development-board-preparation",children:"Development Board Preparation"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["After receiving the development board, please update the development board image to the latest version. For the upgrade method, please refer to the ",(0,s.jsx)(n.a,{href:"../../../01_Quick_start/install_os.md#flash_system",children:(0,s.jsx)(n.strong,{children:"System Update"})})," chapter."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Ensure that the local development machine and the development board can be connected remotely."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"compilation",children:"Compilation"}),"\n",(0,s.jsxs)(n.p,{children:["Compilation requires the installation of cross-compilation tools: ",(0,s.jsx)(n.code,{children:"aarch64-linux-gnu-g++"}),", ",(0,s.jsx)(n.code,{children:"aarch64-linux-gnu-gcc"}),". Please use the D-Robotics provided development machine Docker image for compilation directly. Please read the ",(0,s.jsx)(n.a,{href:"/rdk_doc/en/Advanced_development/toolchain_development/intermediate/environment_config",children:(0,s.jsx)(n.strong,{children:"Environment Installation"})})," chapter for obtaining and using the development machine Docker environment;\nAccording to the situation of the development board used, please use the ",(0,s.jsx)(n.code,{children:"build_xj3.sh"})," or ",(0,s.jsx)(n.code,{children:"build_ultra.sh"})," script under the horizon_runtime_sample/code directory to compile the executable program in the development board environment with one click. The executable program and corresponding dependencies will be automatically copied to the ",(0,s.jsx)(n.code,{children:"xj3/script"})," directory under the ",(0,s.jsx)(n.code,{children:"aarch64"})," directory or the ",(0,s.jsx)(n.code,{children:"ultra/script"})," directory under the ",(0,s.jsx)(n.code,{children:"aarch64"})," directory."]}),"\n",(0,s.jsx)(n.admonition,{title:"Note",type:"info",children:(0,s.jsxs)(n.p,{children:["The project specifies the path of the cross-compilation tool by obtaining the environment variable ",(0,s.jsx)(n.code,{children:"LINARO_GCC_ROOT"}),", users can check whether the local environment variable is the target cross-compilation tool before use.\nIf you need to specify the path of the cross-compilation tool, you can set the environment variable ",(0,s.jsx)(n.code,{children:"LINARO_GCC_ROOT"}),", or directly modify the script ",(0,s.jsx)(n.code,{children:"build_xj3.sh"})," or ",(0,s.jsx)(n.code,{children:"build_ultra.sh"}),", specify the variables ",(0,s.jsx)(n.code,{children:"CC"})," and ",(0,s.jsx)(n.code,{children:"CXX"}),"."]})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"  export CC=${GCC_ROOT}/bin/aarch64-linux-gnu-gcc\n  export CXX=${GCC_ROOT}/bin/aarch64-linux-gnu-g++\n"})}),"\n",(0,s.jsx)(n.h3,{id:"example-usage",children:"Example Usage"}),"\n",(0,s.jsx)(n.h4,{id:"basic_samples-example",children:"basic_samples Example"}),"\n",(0,s.jsx)(n.p,{children:"The model inference\u200b example script is mainly in the xj3/script and xj3/script_x86 directories. After compiling the program, the directory structure is as follows:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"\n  # RDK X3 usage script information\n    \u251c\u2500script\n      \u251c\u2500\u2500 00_quick_start\n      \u2502   \u251c\u2500\u2500 README.md\n      \u2502   \u2514\u2500\u2500 run_mobilenetV1.sh\n      \u251c\u2500\u2500 01_api_tutorial\n      \u2502   \u251c\u2500\u2500 model.sh\n      \u2502   \u251c\u2500\u2500 README.md\n      \u2502   \u251c\u2500\u2500 resize_bgr.sh\n      \u2502   \u251c\u2500\u2500 resize_y.sh\n      \u2502   \u251c\u2500\u2500 roi_infer.sh\n      \u2502   \u251c\u2500\u2500 sys_mem.sh\n      \u2502   \u2514\u2500\u2500 tensor.sh\n      \u251c\u2500\u2500 02_advanced_samples\n      \u2502   \u251c\u2500\u2500 custom_arm_op_custom_identity.sh\n      \u2502   \u251c\u2500\u2500 README.md\n      \u2502   \u2514\u2500\u2500 run_multi_model_batch.sh\n      \u251c\u2500\u2500 03_misc\n      \u2502   \u251c\u2500\u2500 README.md\n      \u2502   \u251c\u2500\u2500 run_lenet.sh\n      \u2502   \u2514\u2500\u2500 run_resnet50_feature.sh\n      \u251c\u2500\u2500 aarch64\n      \u2502   \u251c\u2500\u2500 bin\n      \u2502   \u2502   \u251c\u2500\u2500 model_example\n      \u2502   \u2502   \u251c\u2500\u2500 resize_bgr_example\n      \u2502   \u2502   \u251c\u2500\u2500 resize_y_example\n      \u2502   \u2502   \u251c\u2500\u2500 roi_infer\n      \u2502   \u2502   \u251c\u2500\u2500 run_custom_op\n      \u2502   \u2502   \u251c\u2500\u2500 run_lenet_gray\n      \u2502   \u2502   \u251c\u2500\u2500 run_mobileNetV1_224x224\n      \u2502   \u2502   \u251c\u2500\u2500 run_multi_model_batch\n      \u2502   \u2502   \u251c\u2500\u2500 run_resnet_feature\n      \u2502   \u2502   \u251c\u2500\u2500 sys_mem_example\n      \u2502   \u2502   \u2514\u2500\u2500 tensor_example\n      \u2502   \u2514\u2500\u2500 lib\n      \u2502       \u251c\u2500\u2500 libdnn.so\n      \u2502       \u251c\u2500\u2500 libhbrt_bernoulli_aarch64.so\n      \u2502       \u2514\u2500\u2500 libopencv_world.so.3.4\n      \u2514\u2500\u2500 README.md\n\n    # RDK Ultra Script Information\n    \u251c\u2500script\n      \u251c\u2500\u2500 00_quick_start\n      \u2502   \u251c\u2500\u2500 README.md\n      \u2502   \u2514\u2500\u2500 run_mobilenetV1.sh\n      \u251c\u2500\u2500 01_api_tutorial\n      \u2502   \u251c\u2500\u2500 model.sh\n      \u2502   \u251c\u2500\u2500 README.md\n      \u2502   \u251c\u2500\u2500 roi_infer.sh\n      \u2502   \u251c\u2500\u2500 sys_mem.sh\n      \u2502   \u2514\u2500\u2500 tensor.sh\n      \u251c\u2500\u2500 02_advanced_samples\n      \u2502   \u251c\u2500\u2500 plugin\n      \u2502   \u2502   \u2514\u2500\u2500 custom_arm_op_custom_identity.sh\n      \u2502   \u251c\u2500\u2500 README.md\n      \u2502   \u251c\u2500\u2500 run_multi_input.sh\n      \u2502   \u251c\u2500\u2500 run_multi_model_batch.sh\n      \u2502   \u2514\u2500\u2500 run_nv12_batch.sh\u251c\u2500\u2500 03_misc\n      \u2502   \u251c\u2500\u2500 README.md\n      \u2502   \u251c\u2500\u2500 run_lenet.sh\n      \u2502   \u2514\u2500\u2500 run_resnet50_feature.sh\n      \u251c\u2500\u2500 aarch64                        # Compilation generates executable programs and dependency libraries\n      \u2502   \u251c\u2500\u2500 bin\n      \u2502   \u2502   \u251c\u2500\u2500 model_example\n      \u2502   \u2502   \u251c\u2500\u2500 roi_infer\n      \u2502   \u2502   \u251c\u2500\u2500 run_custom_op\n      \u2502   \u2502   \u251c\u2500\u2500 run_lenet_gray\n      \u2502   \u2502   \u251c\u2500\u2500 run_mobileNetV1_224x224\n      \u2502   \u2502   \u251c\u2500\u2500 run_multi_input\n      \u2502   \u2502   \u251c\u2500\u2500 run_multi_model_batch\n      \u2502   \u2502   \u251c\u2500\u2500 run_nv12_batch\n      \u2502   \u2502   \u251c\u2500\u2500 run_resnet_feature\n      \u2502   \u2502   \u251c\u2500\u2500 sys_mem_example\n      \u2502   \u2502   \u2514\u2500\u2500 tensor_example\n      \u2502   \u2514\u2500\u2500 lib\n      \u2502       \u251c\u2500\u2500 libdnn.so\n      \u2502       \u251c\u2500\u2500 libhbrt_bayes_aarch64.so\n      \u2502       \u2514\u2500\u2500 libopencv_world.so.3.4\n      \u2514\u2500\u2500 README.md\n\n"})}),"\n",(0,s.jsx)(n.admonition,{title:"Note",type:"info",children:(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.code,{children:"model"})," folder contains the path to the model, where the ",(0,s.jsx)(n.code,{children:"runtime"})," folder is a symbolic link, and the link path is ",(0,s.jsx)(n.code,{children:"../../../model_zoo/runtime/horizon_runtime_sample"}),", which can directly find the model path in the delivery package."]}),"\n",(0,s.jsxs)(n.li,{children:["The board-side running environment needs to place the model in the ",(0,s.jsx)(n.code,{children:"model"})," folder."]}),"\n"]})}),"\n",(0,s.jsx)(n.h4,{id:"quick_start",children:"quick_start"}),"\n",(0,s.jsx)(n.p,{children:"The quick_start directory under 00_quick_start is an example of quick starting the model inference:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"  00_quick_start/\n  \u251c\u2500\u2500 README.md\n  \u2514\u2500\u2500 run_mobilenetV1.sh\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"run_mobilenetV1.sh"}),": This script implements the example function of reading a single image using the MobilenetV1 model for inference."]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"api_tutorial",children:"api_tutorial"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"api_tutorial"})," directory contains examples demonstrating how to use embedded APIs. Its subscripts are as follows:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"|-- model.sh\n|-- resize_bgr.sh\n|-- resize_y.sh\n|-- roi_infer.sh\n|-- sys_mem.sh\n|-- tensor.sh\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"model.sh"}),": This script primarily handles reading model information. To use it, navigate to the ",(0,s.jsx)(n.code,{children:"01_api_tutorial"})," directory and execute ",(0,s.jsx)(n.code,{children:"sh model.sh"}),", like this:"]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{title:"Caution",type:"caution",children:(0,s.jsxs)(n.p,{children:["The following example logs are based on actual testing on the ",(0,s.jsx)(n.strong,{children:"RDK X3"})," development board. Results may differ when using the ",(0,s.jsx)(n.strong,{children:"RDK Ultra"})," board; please refer to your specific test results."]})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:'#!/bin/sh\n/usr/local/ruxin.song/xj3/script/01_api_tutorial# sh model.sh\n../aarch64/bin/model_example --model_file_list=../../model/runtime/mobilenetv1/mobilenetv1_nv12_hybrid_horizonrt.bin\nI0000 00:00:00.000000 24638 vlog_is_on.cc:197] RAW: Set VLOG level for "*" to 3\n[HBRT] set log level as 0. version = 3.12.1\n[BPU_PLAT]BPU Platform Version(1.2.2)!\n[HorizonRT] The model builder version = 1.3.3\nI0108 04:19:27.245879 24638 model_example.cc:104] model count:1, model[0]: mobilenetv1_nv12\nI0108 04:19:27.246064 24638 model_example.cc:112] hbDNNGetModelHandle [mobilenetv1_nv12] success!\nI0108 04:19:27.246139 24638 model_example.cc:189] [mobilenetv1_nv12] Model Info:  input num: 1, input[0] validShape: ( 1, 3, 224, 224 ), alignedShape: ( 1, 4, 224, 224 ), tensorLayout: 2, tensorType: 1, output num: 1, output[0] validShape: ( 1, 1000, 1, 1 ), alignedShape: ( 1, 1000, 1, 1 ), tensorLayout: 2, tensorType: 13\n'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"resize_bgr.sh"}),": This script demonstrates how to use the ",(0,s.jsx)(n.code,{children:"hbDNNResize"})," API. The example code in the script resizes a 1352x900 image, crops a portion with coordinates [5, 19, 340, 343], and saves the resized image (402x416) as ",(0,s.jsx)(n.code,{children:"resize_bgr.jpg"}),". Run it by executing ",(0,s.jsx)(n.code,{children:"sh resize_bgr.sh"}),":"]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{title:"Caution",type:"caution",children:(0,s.jsxs)(n.p,{children:["The following example logs are based on the ",(0,s.jsx)(n.strong,{children:"RDK X3"})," development board. Results may vary for the ",(0,s.jsx)(n.strong,{children:"RDK Ultra"})," board; please refer to your actual test results."]})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:'#!/bin/sh\n\n/usr/local/ruxin.song/xj3/script/01_api_tutorial# sh resize_bgr.sh\n../aarch64/bin/resize_bgr_example --image_file=../../data/det_images/kite.jpg --resize_height=416 --resize_width=402 --resized_image=./resize_bgr.jpg --crop_x1=5 --crop_x2=340 --crop_y1=19 --crop_y2=343\nI0000 00:00:00.000000 24975 vlog_is_on.cc:197] RAW: Set VLOG level for "*" to 3\nI0108 06:58:03.327212 24975 resize_bgr_example.cc:116] Original shape: 1352x900 ,dest shape:402x416 ,aligned shape:402x416\n[HBRT] set log level as 0. version = 3.12.1\n[BPU_PLAT]BPU Platform Version(1.2.2)!\nI0108 06:58:03.328739 24975 resize_bgr_example.cc:139] resize success!\nI0108 06:58:03.335835 24975 resize_bgr_example.cc:143] wait task done finished!\n'})}),"\n",(0,s.jsxs)(n.p,{children:["After execution, the ",(0,s.jsx)(n.code,{children:"resize_bgr.jpg"})," image will be successfully saved in the current directory."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"resize_y.sh"}),": This script guides the usage of the ",(0,s.jsx)(n.code,{children:"hbDNNResize"})," API. It resizes an image to a size of 416x402. Run it by executing ",(0,s.jsx)(n.code,{children:"sh resize_y.sh"}),":"]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{title:"Caution",type:"caution",children:(0,s.jsxs)(n.p,{children:["Logs are from the ",(0,s.jsx)(n.strong,{children:"RDK X3"})," development board. Results may vary for the ",(0,s.jsx)(n.strong,{children:"RDK Ultra"})," board; please refer to your actual test results."]})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:'#!/bin/sh\n\n/usr/local/ruxin.song/xj3/script/01_api_tutorial# sh resize_y.sh\n../aarch64/bin/resize_y_example --image_file=../../data/det_images/kite.jpg --resize_height=416 --resize_width=402 --resized_image=./resize_y.jpg\nI0000 00:00:00.000000 24992 vlog_is_on.cc:197] RAW: Set VLOG level for "*" to 3\nI0108 06:59:36.887241 24992 resize_y_example.cc:101] Original shape: 1352x900 ,dest shape:402x416 ,aligned shape:402x416\n[HBRT] set log level as 0. version = 3.12.1\n[BPU_PLAT]BPU Platform Version(1.2.2)!\nI0108 06:59:36.888770 24992 resize_y_example.cc:119] resize success\nI0108 06:59:36.891711 24992 resize_y_example.cc:123] wait resize success\nI0108 06:59:36.891798 24992 resize_y_example.cc:129] spent time: 0.003463\n'})}),"\n",(0,s.jsxs)(n.p,{children:["After execution, the ",(0,s.jsx)(n.code,{children:"resize_y.jpg"})," image will be successfully saved in the current directory."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"roi_infer.sh"}),": This script demonstrates how to use the ",(0,s.jsx)(n.code,{children:"hbDNNRoiInfer"})," API. It resizes an image to the model input size, converts it to NV12 format, and performs inference (inferencing) with a given ROI (Region of Interest)."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"sys_mem.sh"}),": This script guides the usage of the ",(0,s.jsx)(n.code,{children:"hbSysAllocMem"}),", ",(0,s.jsx)(n.code,{children:"hbSysFlushMem"}),", and ",(0,s.jsx)(n.code,{children:"hbSysFreeMem"})," APIs. Execute it directly within the ",(0,s.jsx)(n.code,{children:"01_api_tutorial"})," directory by running ",(0,s.jsx)(n.code,{children:"sh sys_mem.sh"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"tensor.sh"}),": This script demonstrates how to prepare model inputs and outputs. Execute it in the ",(0,s.jsx)(n.code,{children:"01_api_tutorial"})," directory by running ",(0,s.jsx)(n.code,{children:"sh tensor.sh"}),", as shown below:"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{title:"Caution",type:"caution",children:(0,s.jsxs)(n.p,{children:["Logs are from the ",(0,s.jsx)(n.strong,{children:"RDK X3"})," development board. Results may vary for the ",(0,s.jsx)(n.strong,{children:"RDK Ultra"})," board; please refer to your actual test results."]})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"/usr/local/ruxin.song/xj3/script/01_api_tutorial# sh tensor.sh\nTensor data type:0, Tensor layout: 2, shape:1x1x721x1836, aligned shape:1x1x721x1840\nTensor data type:1, Tensor layout: 2, shape:1x3x773x329, aligned shape:1x3x773x336\nTensor data type:2, Tensor layout: 2, shape:1x3x108x1297, aligned shape:1x3x108x1312\nTensor data type:5, Tensor layout: 2, shape:1x3x858x477, aligned shape:1x3x858x477\nTensor data type:5, Tensor layout: 0, shape:1x920x102x3, aligned shape:1x920x102x3\nTensor\n"})}),"\n",(0,s.jsx)(n.h4,{id:"advanced_samples",children:"advanced_samples"}),"\n",(0,s.jsx)(n.p,{children:"The examples under the 02_advanced_samples directory are used to demonstrate the use of special features in custom operators. The directory includes the following scripts:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shelll",children:"  \u251c\u2500\u2500 custom_arm_op_custom_identity.sh\n  \u2514\u2500\u2500 run_multi_model_batch.sh\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"custom_arm_op_custom_identity.sh"}),": This script is mainly used for custom operator model inference. To use it, enter the ",(0,s.jsx)(n.code,{children:"02_advanced_samples"})," directory and execute ",(0,s.jsx)(n.code,{children:"sh custom_arm_op_custom_identity.sh"})," directly as shown below:"]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{title:"Note",type:"caution",children:(0,s.jsxs)(n.p,{children:["The following example logs are the results of actual tests using ",(0,s.jsx)(n.strong,{children:"RDK X3"})," development board. The log information may vary if using the ",(0,s.jsx)(n.strong,{children:"RDK Ultra"})," development board. Please refer to specific tests for accuracy!"]})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:'/userdata/ruxin.song/xj3/script/02_advanced_samples# sh custom_arm_op_custom_identity.sh\n../aarch64/bin/run_custom_op --model_file=../../model/runtime/custom_op/custom_op_featuremap.bin --input_file=../../data/custom_identity_data/input0.bin,../../data/custom_identity_data/input1.bin\nI0000 00:00:00.000000 30421 vlog_is_on.cc:197] RAW: Set VLOG level for "*" to 3\nI0723 15:06:12.172068 30421 main.cpp:212] hbDNNRegisterLayerCreator success\nI0723 15:06:12.172335 30421 main.cpp:217] hbDNNRegisterLayerCreator success\n[BPU_PLAT]BPU Platform Version(1.3.1)!\n[HBRT] set log level as 0. version = 3.15.3.0\n[DNN] Runtime version = 1.15.2_(3.15.3 HBRT)\n[A][DNN][packed_model.cpp:217](1563865572232) [HorizonRT] The model builder version = 1.13.5\nI0723 15:06:12.240696 30421 main.cpp:232] hbDNNGetModelNameList success\nI0723 15:06:12.240784 30421 main.cpp:239] hbDNNGetModelHandle success\nI0723 15:06:12.240819 30421 main.cpp:245] hbDNNGetInputCount success\nfile length: 602112\nfile length: 602112\nI0723 15:06:12.243616 30421 main.cpp:268] hbDNNGetOutputCount success\nI0723 15:06:12.244102 30421 main.cpp:297] hbDNNInfer success\nI0723 15:06:12.257903 30421 main.cpp:302] task done\nI0723 15:06:14.277941 30421 main.cpp:306] write output tensor\n\nThe output data of the first model is saved in the `output0.txt` file.\n'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"run_multi_model_batch.sh"}),": This script is mainly used for batch inference of multiple small models. To use it, enter the ",(0,s.jsx)(n.code,{children:"02_advanced_samples"})," directory and execute ",(0,s.jsx)(n.code,{children:"sh run_multi_model_batch.sh"})," directly as shown below:"]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{title:"Note",type:"caution",children:(0,s.jsxs)(n.p,{children:["The following example logs are the results of actual tests using ",(0,s.jsx)(n.strong,{children:"RDK X3"})," development board. The log information may vary if using the ",(0,s.jsx)(n.strong,{children:"RDK Ultra"})," development board. Please refer to specific tests for accuracy!"]})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:'root@x3sdbx3-hynix2G-3200:/userdata/chaoliang/xj3/script/02_advanced_samples# sh run_multi_model_batch.sh\n../aarch64/bin/run_multi_model_batch --model_file=../../model/runtime/googlenet/googlenet_224x224_nv12.bin,../../model/runtime/mobilenetv2/mobilenetv2_224x224_nv12.bin --input_file=../../data/cls_images/zebra_cls.jpg,../../data/cls_images/zebra_cls.jpg\nI0000 00:00:00.000000 17060 vlog_is_on.cc:197] RAW: Set VLOG level for "*" to 3\n[HBRT] set log level as 0. version = 3.13.4\n```[BPU_PLAT]BPU Platform Version(1.1.1)!\n    [HorizonRT] The model builder version = 1.3.18\n    [HorizonRT] The model builder version = 1.3.18\n    I0317 12:37:18.249785 17060 main.cpp:119] hbDNNInitializeFromFiles success\n    I0317 12:37:18.250029 17060 main.cpp:127] hbDNNGetModelNameList success\n    I0317 12:37:18.250071 17060 main.cpp:141] hbDNNGetModelHandle success\n    I0317 12:37:18.283633 17060 main.cpp:155] read image to nv12 success\n    I0317 12:37:18.284270 17060 main.cpp:172] prepare input tensor success\n    I0317 12:37:18.284456 17060 main.cpp:184] prepare output tensor success\n    I0317 12:37:18.285344 17060 main.cpp:218] infer success\n    I0317 12:37:18.296559 17060 main.cpp:223] task done\n    I0317 12:37:18.296701 17060 main.cpp:228] googlenet class result id: 340\n    I0317 12:37:18.296805 17060 main.cpp:232] mobilenetv2 class result id: 340\n    I0317 12:37:18.296887 17060 main.cpp:236] release task successI0108 07:23:35.510927 25139 run_lenet_gray.cc:217] TOP 4 result id: 2\n'})}),"\n",(0,s.jsx)(n.h4,{id:"misc",children:"misc"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"03_misc"})," directory contains examples for using models with non-nv12 inputs. It consists of the following scripts:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"  \u251c\u2500\u2500 run_lenet.sh\n  \u2514\u2500\u2500 run_resnet50_feature.sh\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"run_lenet.sh"}),": This script primarily implements the inference functionality of a LeNet model with Y data input. To use it, navigate to the ",(0,s.jsx)(n.code,{children:"03_misc"})," directory and execute ",(0,s.jsx)(n.code,{children:"sh run_lenet.sh"}),", as shown below:"]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{title:"Note",type:"caution",children:(0,s.jsxs)(n.p,{children:["The following example logs are from actual tests on an ",(0,s.jsx)(n.strong,{children:"RDK X3"})," development board. Logs may differ for an ",(0,s.jsx)(n.strong,{children:"RDK Ultra"})," board; refer to your specific test results!"]})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:'    /userdata/ruxin.song/xj3/script/03_misc# sh run_lenet.sh\n    ../aarch64/bin/run_lenet_gray --model_file=../../model/runtime/lenet_gray/lenet_gray_hybrid_horizonrt.bin --data_file=../../data/misc_data/7.bin --image_height=28 --image_width=28 --top_k=5\n    I0000 00:00:00.000000 25139 vlog_is_on.cc:197] RAW: Set VLOG level for "*" to 3\n    [HBRT] set log level as 0. version = 3.12.1\n    [BPU_PLAT]BPU Platform Version(1.2.2)!\n    [HorizonRT] The model builder version = 1.3.3\n    I0108 07:23:35.507514 25139 run_lenet_gray.cc:145] hbDNNInitializeFromFiles success\n    I0108 07:23:35.507737 25139 run_lenet_gray.cc:153] hbDNNGetModelNameList success\n    I0108 07:23:35.507771 25139 run_lenet_gray.cc:160] hbDNNGetModelHandle success\n    I0108 07:23:35.508070 25139 run_lenet_gray.cc:176] prepare y tensor success\n    I0108 07:23:35.508178 25139 run_lenet_gray.cc:189] prepare tensor success\n    I0108 07:23:35.509909 25139 run_lenet_gray.cc:200] infer success\n    I0108 07:23:35.510721 25139 run_lenet_gray.cc:205] task done\n    I0108 07:23:35.510790 25139 run_lenet_gray.cc:210] task post process finished\n    I0108 07:23:35.510832 25139 run_lenet_gray.cc:217] TOP 0 result id: 7\n    I0108 07:23:35.510857 25139 run_lenet_gray.cc:217] TOP 1 result id: 9\n    I0108 07:23:35.510879 25139 run_lenet_gray.cc:217] TOP 2 result id: 3\n    I0108 07:23:35.510903 25139 run_lenet_gray.cc:217] TOP 3 result id: 4\n    I0108 07:23:35.510927 25139 run_lenet_gray.cc:217] TOP 4 result id: 2\n'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"run_resnet50_feature.sh"}),": This script mainly demonstrates the inference functionality of a ResNet50 model with feature data input. The example code preprocesses the feature data by quantizing and padding it to meet the model's requirements before feeding it into the model. To use it, navigate to the ",(0,s.jsx)(n.code,{children:"03_misc"})," directory and run ",(0,s.jsx)(n.code,{children:"sh run_resnet50_feature.sh"}),", as follows:"]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{title:"Note",type:"caution",children:(0,s.jsx)(n.p,{children:"See above note for logs."})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:'    /userdata/ruxin.song/xj3/script/03_misc# sh run_resnet50_feature.sh\n    ../aarch64/bin/run_resnet_feature --model_file=../../model/runtime/resnet50_feature/resnet50_feature_hybrid_horizonrt.bin --data_file=../../data/misc_data/np_0 --top_k=5\n    I0000 00:00:00.000000 25155 vlog_is_on.cc:197] RAW: Set VLOG level for "*" to 3\n    [HBRT] set log level as 0. version = 3.12.1\n    [BPU_PLAT]BPU Platform Version(1.2.2)!\n    [HorizonRT] The model builder version = 1.3.3\n    I0108 07:25:41.300466 25155 run_resnet_feature.cc:136] hbDNNInitializeFromFiles success\n    I0108 07:25:41.300708 25155 run_resnet_feature.cc:144] hbDNNGetModelNameList success\n    I0108 07:25:41.300741 25155 run_resnet_feature.cc:151] hbDNNGetModelHandle success\n    I0108 07:25:41.302760 25155 run_resnet_feature.cc:166] prepare feature tensor success\n    I0108 07:25:41.302919 25155 run_resnet_feature.cc:176] prepare tensor success\n    I0108 07:25:41.304678 25155 run_resnet_feature.cc:187] infer success\n    I0108 07:25:41.373052 25155 run_resnet_feature.cc:192] task done\n    I0108 07:25:41.373328 25155 run_resnet_feature.cc:197] task post process finished\n    I0108 07:25:41.373374 25155 run_resnet_feature.cc:204] TOP 0 result id: 74\n    I0108 07:25:41.373399 25155 run_resnet_feature.cc:204] TOP 1 result id: 815\n    I0108 07:25:41.373422 25155 run_resnet_feature.cc:204] TOP 2 result id: 73\n    I0108 07:25:41.373445 25155 run_resnet_feature.cc:204] TOP 3 result id: 78\n    I0108 07:25:41.373468 25155 run_resnet_feature.cc:204] TOP 4 result id: 72\n'})}),"\n",(0,s.jsx)(n.h3,{id:"auxiliary-tools-and-common-operations",children:"Auxiliary tools and common operations"}),"\n",(0,s.jsx)(n.h4,{id:"logs",children:"Logs"}),"\n",(0,s.jsx)(n.p,{children:'This section mainly includes "Example Logs" and "Model Inference DNN API Logs".\nThe example logs refer to the application logs in the delivery package example code, and all log content will be output.'}),"\n",(0,s.jsx)(n.h4,{id:"example-logs",children:"Example Logs"}),"\n",(0,s.jsx)(n.p,{children:"The example logs mainly use vlog in glog. In the basic_samples of the example code, all log content will be output."}),"\n",(0,s.jsx)(n.h4,{id:"model-inference-dnn-api-logs",children:"Model Inference DNN API Logs"}),"\n",(0,s.jsx)(n.p,{children:'For the configuration of model inference DNN API logs, please refer to the "Configuration Information" section in the "Model Inference DNN API Usage Instructions" document.'}),"\n",(0,s.jsx)(n.h2,{id:"public-model-performance-accuracy-evaluation-instructions",children:"Public model performance accuracy evaluation instructions"}),"\n",(0,s.jsx)(n.h3,{id:"public-model-performance-accuracy-indicatorsthe-table-below-provides-performance-and-accuracy-indicators-of-typical-deep-neural-network-models-on-the-x3-processor",children:"Public model performance accuracy indicatorsThe table below provides performance and accuracy indicators of typical deep neural network models on the X3 processor."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/model_accuracy.png",alt:"model_accuracy"})}),"\n",(0,s.jsx)(n.admonition,{title:"Note:",type:"caution",children:(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The data in the table are measured results on the D-Robotics RDK X3 development board, and the test models are from the horizon_model_convert_sample model example package."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"For the BPU/CPU hybrid heterogeneous models in the model example package, the frame consumption time is mainly composed of the input quantization CPU node, model BPU operators, model CPU operators, output dequantization CPU node, CPU post-processing, etc. The specific explanation is as follows:"}),"\n",(0,s.jsx)(n.p,{children:"a. Input quantization CPU node: completes the float32 to int8 input quantization operation, which is only included in models that use featuremap input. The quantization time is proportional to the input shape size."}),"\n",(0,s.jsx)(n.p,{children:"b. Model CPU operators:"}),"\n",(0,s.jsx)(n.p,{children:"i. Any CPU operators not included in the detection model."}),"\n",(0,s.jsx)(n.p,{children:"ii. The Softmax and Reshape at the tail of the classification model are CPU operators."}),"\n",(0,s.jsx)(n.p,{children:"iii. The Argmax at the tail of the segmentation model DeepLabV3+ is a CPU operator."}),"\n",(0,s.jsx)(n.p,{children:"c. Output dequantization CPU node: completes the int8 to float32 output dequantization operation. The quantization time is proportional to the output shape size."}),"\n",(0,s.jsx)(n.p,{children:"d. D-Robotics currently supports manually removing the quantization/dequantization nodes of a model and integrating them into pre- and post-processing code by users to reduce the overhead of duplicate data traversal. Taking the EfficientDet model as an example, after removing the dequantization node and incorporating it into the post-processing, the inference performance is improved from 66 FPS to 100 FPS."}),"\n",(0,s.jsx)(n.p,{children:"e. Currently, the post-processing of the D-Robotics sample models has not been specifically optimized for performance. You can use optimization methods such as approximate and efficient implementation according to your actual needs to accelerate the code at the code level."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"In practical applications, the BPU and CPU can run concurrently to improve the overall inference speed."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"If the measured performance is inconsistent with the test results in the table above, several factors may cause this situation:"}),"\n",(0,s.jsx)(n.p,{children:"a. The impact of DDR bandwidth, the development board can only run the ai_benchmark program."}),"\n",(0,s.jsx)(n.p,{children:"b. The algorithm toolchain version does not exactly match the system image version. The ideal scenario is to use the algorithm toolchain and system image released together."}),"\n",(0,s.jsxs)(n.p,{children:["c. The impact of CPU frequency reduction, currently the development board enables automatic frequency reduction by default after reboot. To achieve the best performance, you need to execute the command to disable frequency reduction on the development board: ",(0,s.jsx)(n.code,{children:"echo performance > /sys/devices/system/cpu/cpufreq/policy0/scaling_governor"}),"."]}),"\n"]}),"\n"]})}),"\n",(0,s.jsx)(n.h3,{id:"evaluation-method-description",children:"Evaluation Method Description"}),"\n",(0,s.jsx)(n.h4,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"This section describes the specific usage of the ai_benchmark example package for public model accuracy and performance evaluation. The example package provides source code, executable programs, and evaluation scripts. Developers can directly experience and develop embedded applications based on these examples on the D-Robotics development board, reducing the development threshold."}),"\n",(0,s.jsx)(n.p,{children:"The example package provides performance and accuracy evaluation examples for common classification, detection, and segmentation models. Please refer to the following content for details."}),"\n",(0,s.jsxs)(n.admonition,{title:"Tip",type:"tip",children:[(0,s.jsxs)(n.p,{children:["To obtain the ai_benchmark example package for public model accuracy and performance evaluation, please refer to the ",(0,s.jsx)(n.a,{href:"/rdk_doc/en/Advanced_development/toolchain_development/intermediate/environment_config",children:"Deliverables Instructions"}),".\n:::#### Example Code Package Structure"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"  ai_benchmark/code                     # Example source code folder\n  \u251c\u2500\u2500 build_ptq_xj3.sh                  # for RDK X3\n  \u251c\u2500\u2500 build_ptq_ultra.sh                # for RDK Ultra\n  \u251c\u2500\u2500 CMakeLists.txt\n  \u251c\u2500\u2500 deps/deps_gcc9.3                  # Third-party dependency library, gcc9.3 is deps_gcc9.3\n  \u2502\xa0\xa0 \u251c\u2500\u2500 aarch64\n  \u2502\xa0\xa0 \u2514\u2500\u2500 vdsp\n  \u251c\u2500\u2500 include                           # Source header files\n  \u2502   \u251c\u2500\u2500 base\n  \u2502   \u251c\u2500\u2500 input\n  \u2502   \u251c\u2500\u2500 method\n  \u2502   \u251c\u2500\u2500 output\n  \u2502   \u251c\u2500\u2500 plugin\n  \u2502   \u2514\u2500\u2500 utils\n  \u251c\u2500\u2500 README.md\n  \u2514\u2500\u2500 src                               # Example source code\n      \u251c\u2500\u2500 input\n      \u251c\u2500\u2500 method\n      \u251c\u2500\u2500 output\n      \u251c\u2500\u2500 plugin\n      \u251c\u2500\u2500 simple_example.cc             # Example main program\n      \u2514\u2500\u2500 utils\n\n  ai_benchmark/xj3                      # Example package runtime environment\n  \u2514\u2500\u2500 ptq                               # PTQ solution model example\n      \u251c\u2500\u2500 data                          # Model accuracy evaluation dataset\n      \u251c\u2500\u2500 mini_data                     # Model performance evaluation dataset\n      \u2502\xa0\xa0 \u251c\u2500\u2500 cifar10\n      \u2502\xa0\xa0 \u251c\u2500\u2500 cityscapes\n      \u2502\xa0\xa0 \u251c\u2500\u2500 coco\n      \u2502\xa0\xa0 \u251c\u2500\u2500 culane\n      \u2502\xa0\xa0 \u251c\u2500\u2500 flyingchairs\n      \u2502\xa0\xa0 \u251c\u2500\u2500 imagenet\n      \u2502\xa0\xa0 \u251c\u2500\u2500 kitti3d\n      \u2502\xa0\xa0 \u251c\u2500\u2500 mot17\n      \u2502\xa0\xa0 \u251c\u2500\u2500 nuscenes\n      \u2502\xa0\xa0 \u251c\u2500\u2500 nuscenes_lidar\n      \u2502\xa0\xa0 \u2514\u2500\u2500 voc\n      \u251c\u2500\u2500 model                         # PTQ solution nv12 model\n      \u2502   \u251c\u2500\u2500 README.md\n      \u2502   \u2514\u2500\u2500 runtime -> ../../../../model_zoo/runtime/ai_benchmark/ptq   # Symbolic link to the model in the OE package, the board-side runtime environment needs to specify the model path on its own\n      \u251c\u2500\u2500 README.md\n      \u251c\u2500\u2500 script                        # Execution script\n      \u2502   \u251c\u2500\u2500 aarch64                   # Compile executable files and dependent libraries\n      \u2502   \u251c\u2500\u2500 classification            # Classification model example\n      \u2502   \u2502   \u251c\u2500\u2500 efficientnet_lite0\n      \u2502   \u2502   \u251c\u2500\u2500 efficientnet_lite1\n      \u2502   \u2502   \u251c\u2500\u2500 efficientnet_lite2\n      \u2502   \u2502   \u251c\u2500\u2500 efficientnet_lite3\n      \u2502   \u2502   \u251c\u2500\u2500 efficientnet_lite4\n      \u2502   \u2502   \u251c\u2500\u2500 googlenet\n      \u2502   \u2502   \u251c\u2500\u2500 mobilenetv1\n      \u2502   \u2502   \u251c\u2500\u2500 mobilenetv2\n      \u2502   \u2502   \u2514\u2500\u2500 resnet18\n      \u2502   \u251c\u2500\u2500 config                    # Model inference configuration files\n      \u2502   \u2502   \u2514\u2500\u2500 data_name_list  \n      \u2502   \u251c\u2500\u2500 detection                 # Detection model examples\n      \u2502   \u2502   \u251c\u2500\u2500 centernet_resnet50\n      \u2502   \u2502   \u251c\u2500\u2500 efficientdetd0\n      \u2502   \u2502   \u251c\u2500\u2500 fcos_efficientnetb0\n      \u2502   \u2502   \u251c\u2500\u2500 preq_qat_fcos_efficientnetb0 \n      \u2502   \u2502   \u251c\u2500\u2500 preq_qat_fcos_efficientnetb1\n      \u2502   \u2502   \u251c\u2500\u2500 preq_qat_fcos_efficientnetb2\n      \u2502   \u2502   \u251c\u2500\u2500 ssd_mobilenetv1\n      \u2502   \u2502   \u251c\u2500\u2500 yolov2_darknet19\n      \u2502   \u2502   \u251c\u2500\u2500 yolov3_darknet53\n      \u2502   \u2502   \u2514\u2500\u2500 yolov5s\n      \u2502   \u251c\u2500\u2500 segmentation              # Segmentation model examples\n      \u2502   \u2502   \u251c\u2500\u2500 deeplabv3plus_efficientnetb0\n      \u2502   \u2502   \u251c\u2500\u2500 fastscnn_efficientnetb0   \n      \u2502   \u2502   \u2514\u2500\u2500 unet_mobilenet\n      \u2502   \u251c\u2500\u2500 base_config.sh            # Base configuration\n      \u2502   \u2514\u2500\u2500 README.md\n      \u2514\u2500\u2500 tools                         # Accuracy evaluation tools\n          \u251c\u2500\u2500 python_tools\n          \u2514\u2500\u2500 README.md\n\n  ai_benchmark/ultra                       # Example package runtime environment\n  \u2514\u2500\u2500 ptq                               # PTQ solution model examples\n      \u251c\u2500\u2500 data                          # Model accuracy evaluation datasets\n      \u251c\u2500\u2500 mini_data                     # Model performance evaluation datasets\n      \u2502\xa0\xa0 \u251c\u2500\u2500 cifar10\n      \u2502\xa0\xa0 \u251c\u2500\u2500 cityscapes\n      \u2502\xa0\xa0 \u251c\u2500\u2500 coco\n      \u2502\xa0\xa0 \u251c\u2500\u2500 culane\n      \u2502\xa0\xa0 \u251c\u2500\u2500 flyingchairs\n      \u2502\xa0\xa0 \u251c\u2500\u2500 imagenet\n      \u2502\xa0\xa0 \u251c\u2500\u2500 kitti3d\n      \u2502\xa0\xa0 \u251c\u2500\u2500 mot17\n      \u2502\xa0\xa0 \u251c\u2500\u2500 nuscenes\n      \u2502\xa0\xa0 \u251c\u2500\u2500 nuscenes_lidar\n      \u2502\xa0\xa0 \u2514\u2500\u2500 voc\n      \u251c\u2500\u2500 model                         # PTQ solution nv12 model\n      \u2502   \u2502   \u251c\u2500\u2500 README.md\n      \u2502   \u2502   \u2514\u2500\u2500 runtime -> ../../../../model_zoo/runtime/ai_benchmark/ptq   # Symbolic link to the model in the OE package, the model path needs to be specified separately in the board-side runtime environment\n      \u251c\u2500\u2500 README.md\n      \u251c\u2500\u2500 script                        # Execution scripts\u2502   \u251c\u2500\u2500 aarch64                   # Compiled executable files and dependency libraries\n      \u2502   \u251c\u2500\u2500 classification            # Classification model examples\n      \u2502   \u2502   \u251c\u2500\u2500 efficientnasnet_m\n      \u2502   \u2502   \u251c\u2500\u2500 efficientnasnet_s\n      \u2502   \u2502   \u251c\u2500\u2500 efficientnet_lite0\n      \u2502   \u2502   \u251c\u2500\u2500 efficientnet_lite1\n      \u2502   \u2502   \u251c\u2500\u2500 efficientnet_lite2\n      \u2502   \u2502   \u251c\u2500\u2500 efficientnet_lite3\n      \u2502   \u2502   \u251c\u2500\u2500 efficientnet_lite4\n      \u2502   \u2502   \u251c\u2500\u2500 googlenet\n      \u2502   \u2502   \u251c\u2500\u2500 mobilenetv1\n      \u2502   \u2502   \u251c\u2500\u2500 mobilenetv2\n      \u2502   \u2502   \u251c\u2500\u2500 resnet18\n      \u2502   \u2502   \u2514\u2500\u2500 vargconvnet\n      \u2502   \u251c\u2500\u2500 config                    # Model inference configuration files\n      \u2502   \u2502   \u2514\u2500\u2500 model  \n      \u2502   \u251c\u2500\u2500 detection                 # Detection model examples\n      \u2502   \u2502   \u251c\u2500\u2500 centernet_resnet101\n      \u2502   \u2502   \u251c\u2500\u2500 preq_qat_fcos_efficientnetb0\n      \u2502   \u2502   \u251c\u2500\u2500 preq_qat_fcos_efficientnetb2\n      \u2502   \u2502   \u251c\u2500\u2500 preq_qat_fcos_efficientnetb3\n      \u2502   \u2502   \u251c\u2500\u2500 ssd_mobilenetv1\n      \u2502   \u2502   \u251c\u2500\u2500 yolov2_darknet19\n      \u2502   \u2502   \u251c\u2500\u2500 yolov3_darknet53\n      \u2502   \u2502   \u251c\u2500\u2500 yolov3_vargdarknet\n      \u2502   \u2502   \u2514\u2500\u2500 yolov5x\n      \u2502   \u251c\u2500\u2500 segmentation              # Segmentation model examples\n      \u2502   \u2502   \u251c\u2500\u2500 deeplabv3plus_efficientnetb0\n      \u2502   \u2502   \u251c\u2500\u2500 deeplabv3plus_efficientnetm1\n      \u2502   \u2502   \u251c\u2500\u2500 deeplabv3plus_efficientnetm2\n      \u2502   \u2502   \u2514\u2500\u2500 fastscnn_efficientnetb0\n      \u2502   \u251c\u2500\u2500 env.sh                    # Environment script\n      \u2502   \u2514\u2500\u2500 README.md\n      \u2514\u2500\u2500 tools                         # Accuracy assessment tools\n            \u251c\u2500\u2500 python_tools\n            \u2514\u2500\u2500 README.md\n"})}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"code"}),": This directory contains the source code for the model evaluation program, used to assess model performance and accuracy."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"xj3"}),": Provides pre-compiled applications and various benchmark scripts to test the performance and accuracy of multiple models on the D-Robotics BPU, specifically for ",(0,s.jsx)(n.strong,{children:"RDK X3"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ultra"}),": Offers pre-compiled applications and benchmark scripts to evaluate the performance and accuracy of various models on the D-Robotics BPU, designed for ",(0,s.jsx)(n.strong,{children:"RDK Ultra"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"build_ptq_xj3.sh"}),": One-click build script for the development board program (for ",(0,s.jsx)(n.strong,{children:"RDK X3"}),")."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"build_ptq_ultra.sh"}),": One-click build script for the development board program (for ",(0,s.jsx)(n.strong,{children:"RDK Ultra"}),")."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"deps/deps_gcc9.3"}),": Dependencies required for the example code, primarily including:"]}),"\n"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"  gflags  glog  hobotlog nlohmann opencv  rapidjson\n"})}),(0,s.jsx)(n.h4,{id:"sample-models",children:"Sample Models"}),(0,s.jsxs)(n.p,{children:["We provide an open-source model library containing commonly used classification, detection, and segmentation models. The naming convention for these models is: ",(0,s.jsx)(n.code,{children:"{model_name}_{backbone}_{input_size}_{input_type}"}),", which developers can directly utilize."]}),(0,s.jsx)(n.admonition,{title:"Tip",type:"tip",children:(0,s.jsxs)(n.p,{children:["The bin models in the table below are all compiled and converted using the horizon_model_convert_sample model conversion example package. Please refer to the ",(0,s.jsx)(n.a,{href:"../beginner.md#env_install",children:"horizon_model_convert_sample"})," chapter for instructions on how to obtain them."]})})]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"MODEL"}),(0,s.jsx)(n.th,{children:"MODEL NAME"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"centernet_resnet101"}),(0,s.jsx)(n.td,{children:"centernet_resnet101_512x512_nv12.bin"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"deeplabv3plus_efficientnetb0"}),(0,s.jsx)(n.td,{children:"deeplabv3plus_efficientnetb0_1024x2048_nv12.bin"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"deeplabv3plus_efficientnetm1"}),(0,s.jsx)(n.td,{children:"deeplabv3plus_efficientnetm1_1024x2048_nv12.bin"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"efficientnasnet_m"}),(0,s.jsx)(n.td,{children:"efficientnasnet_m_300x300_nv12.bin"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"efficientnet_lite4"}),(0,s.jsx)(n.td,{children:"efficientnet_lite4_300x300_nv12.bin"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"fastscnn_efficientnetb0"}),(0,s.jsx)(n.td,{children:"fastscnn_efficientnetb0_1024x2048_nv12.bin"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"googlenet"}),(0,s.jsx)(n.td,{children:"googlenet_224x224_nv12.bin"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"mobilenetv1"}),(0,s.jsx)(n.td,{children:"mobilenetv1_224x224_nv12.bin"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"mobilenetv2"}),(0,s.jsx)(n.td,{children:"mobilenetv2_224x224_nv12.bin"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"preq_qat_fcos_efficientnetb0"}),(0,s.jsx)(n.td,{children:"fcos_efficientnetb0_512x512_nv12.bin"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"preq_qat_fcos_efficientnetb2"}),(0,s.jsx)(n.td,{children:"fcos_efficientnetb2_768x768_nv12.bin"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"resnet18"}),(0,s.jsx)(n.td,{children:"resnet18_224x224_nv12.bin"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"ssd_mobilenetv1"}),(0,s.jsx)(n.td,{children:"ssd_mobilenetv1_300x300_nv12.bin"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"vargconvnet"}),(0,s.jsx)(n.td,{children:"vargconvnet_224x224_nv12.bin"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"yolov3_darknet53"}),(0,s.jsx)(n.td,{children:"yolov3_darknet53_416x416_nv12.bin"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"yolov5s"}),(0,s.jsx)(n.td,{children:"yolov5s_672x672_nv12.bin"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"yolov5x"}),(0,s.jsx)(n.td,{children:"yolov5x_672x672_nv12.bin"})]})]})]}),"\n",(0,s.jsx)(n.h4,{id:"public-datasets",children:"Public Datasets"}),"\n",(0,s.jsx)(n.p,{children:"The evaluation examples mainly use datasets such as VOC, COCO, ImageNet, Cityscapes, FlyingChairs, KITTI, Culane, Nuscenes, and Mot17."}),"\n",(0,s.jsx)(n.p,{children:"Please download them in a Linux environment using the following links:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"  VOC: http://host.robots.ox.ac.uk/pascal/VOC/ (using the VOC2012 version)\n\n  COCO: https://cocodataset.org/#download\n\n  ImageNet: https://www.image-net.org/download.php\n  \n  Cityscapes: https://github.com/mcordts/cityscapesScripts\n  \n  FlyingChairs: https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html\n\n  KITTI3D: https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d\n\n  CULane: https://xingangpan.github.io/projects/CULane.htmlnuScenes: https://www.nuscenes.org/nuscenes#download\n\n  mot17: https://opendatalab.com/MOT17\n"})}),"\n",(0,s.jsx)(n.h4,{id:"environment-setup",children:"Environment Setup"}),"\n",(0,s.jsx)(n.h4,{id:"development-board-preparation-1",children:"Development Board Preparation"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["After receiving the development board, please update the board image to the latest version. Refer to the chapter ",(0,s.jsx)(n.a,{href:"../..../../../01_Quick_start/install_os.md#flash_system",children:(0,s.jsx)(n.strong,{children:"Installation"})})," for the upgrade method."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Make sure that the local development machine and the development board can be connected remotely."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"compiler-environment-preparation",children:"Compiler Environment Preparation"}),"\n",(0,s.jsxs)(n.p,{children:["The compilation requires the installation of the cross-compilation tool ",(0,s.jsx)(n.code,{children:"gcc-ubuntu-9.3.0-2020.03-x86_64-aarch64-linux-gnu"})," in the current environment. Please use the Docker image provided by D-Robotics for development machine, and directly compile and use it. Read the chapter ",(0,s.jsx)(n.a,{href:"/rdk_doc/en/Advanced_development/toolchain_development/intermediate/environment_config",children:(0,s.jsx)(n.strong,{children:"Environment Installation"})})," for obtaining and using the development machine Docker environment;\nPlease use the script ",(0,s.jsx)(n.code,{children:"build_ptq_xj3.sh"})," or ",(0,s.jsx)(n.code,{children:"build_ptq_ultra.sh"})," under the code directory to compile the executable program in the development board environment. The executable program and its corresponding dependencies will be automatically copied to the directory ",(0,s.jsx)(n.code,{children:"xj3/ptq/script"})," under the ",(0,s.jsx)(n.code,{children:"aarch64"})," directory or the directory ",(0,s.jsx)(n.code,{children:"ultra/ptq/script"})," under the ",(0,s.jsx)(n.code,{children:"aarch64"})," directory."]}),"\n",(0,s.jsx)(n.admonition,{title:"Note",type:"info",children:(0,s.jsxs)(n.p,{children:["Please note that the position of the cross-compilation toolchain specified in the script ",(0,s.jsx)(n.code,{children:"build_ptq_xj3.sh"})," and ",(0,s.jsx)(n.code,{children:"build_ptq_ultra.sh"})," is in the directory ",(0,s.jsx)(n.code,{children:"/opt"}),". If the user installs it in another location, the script can be manually modified."]})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"  export CC=/opt/gcc-ubuntu-9.3.0-2020.03-x86_64-aarch64-linux-gnu/bin/aarch64-linux-gnu-gcc\n  export CXX=/opt/gcc-ubuntu-9.3.0-2020.03-x86_64-aarch64-linux-gnu/bin/aarch64-linux-gnu-g++\n"})}),"\n",(0,s.jsx)(n.h4,{id:"evaluation-example-usage-instructions",children:"Evaluation Example Usage Instructions"}),"\n",(0,s.jsxs)(n.p,{children:["The evaluation example scripts are mainly located in the ",(0,s.jsx)(n.code,{children:"script"})," and ",(0,s.jsx)(n.code,{children:"tools"})," directories. The ",(0,s.jsx)(n.code,{children:"script"})," directory contains evaluation scripts that run on the development board, including common classification, detection, and segmentation models. Each model has three scripts, which represent:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"fps.sh: Uses multi-threading scheduling to calculate frames per second (FPS). Users can freely set the number of threads according to their requirements."}),"\n",(0,s.jsx)(n.li,{children:"latency.sh: Calculates the latency of a single frame (one thread, one frame)."}),"\n",(0,s.jsx)(n.li,{children:"accuracy.sh: Used for accuracy evaluation."}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{title:"Note",type:"caution",children:(0,s.jsxs)(n.p,{children:["The following examples are based on the testing results of the ",(0,s.jsx)(n.strong,{children:"RDK X3"})," development board. If using the ",(0,s.jsx)(n.strong,{children:"RDK Ultra"})," development board, there may be differences in information. Please refer to the specific testing results."]})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"  script:\n\n  \u251c\u2500\u2500 aarch64             # Compiled executable files and dependent libraries\n  \u2502   \u251c\u2500\u2500 bin\n  \u2502   \u2514\u2500\u2500 lib\n  \u251c\u2500\u2500 base_config.sh      # Basic configuration\n  \u251c\u2500\u2500 config              # image_name configuration files\n  \u2502   \u251c\u2500\u2500 data_name_list\n  |   |   \u251c\u2500\u2500 coco_detlist.list\n  \u2502   |   \u251c\u2500\u2500 imagenet.list\n  \u2502   |   \u2514\u2500\u2500 voc_detlist.list\n  \u251c\u2500\u2500 classification      # Classification model evaluation\n  \u2502   \u251c\u2500\u2500 efficientnet_lite0\n  \u2502   \u2502   \u251c\u2500\u2500 accuracy.sh\n  \u2502   \u2502   \u251c\u2500\u2500 fps.sh\n  \u2502   \u2502   \u251c\u2500\u2500 latency.sh\n  \u2502   \u2502   \u251c\u2500\u2500 workflow_accuracy.json\n  \u2502   \u2502   \u251c\u2500\u2500 workflow_fps.json\n  \u2502   \u2502   \u2514\u2500\u2500 workflow_latency.json\n  \u2502   \u251c\u2500\u2500 mobilenetv1\n  \u2502   \u251c\u2500\u2500 .....\n  \u2502   \u2514\u2500\u2500 resnet18\n  \u251c\u2500\u2500 detection           # Detection model\n  |   \u251c\u2500\u2500 centernet_resnet50\n  \u2502   \u2502   \u251c\u2500\u2500 accuracy.sh\n  \u2502   \u2502   \u251c\u2500\u2500 fps.sh\n  \u2502   \u2502   \u251c\u2500\u2500 latency.sh\n  \u2502   \u2502   \u251c\u2500\u2500 workflow_accuracy.json\n  \u2502   \u2502   \u251c\u2500\u2500 workflow_fps.json\n  \u2502   \u2502   \u2514\u2500\u2500 workflow_latency.json\n  \u2502   \u251c\u2500\u2500 yolov2_darknet19\n  \u2502   \u251c\u2500\u2500 yolov3_darknet53\n  \u2502   \u251c\u2500\u2500 ...\n  \u2502   \u2514\u2500\u2500 efficientdetd0\n  \u2514\u2500\u2500 segmentation       # Segmentation model\n      \u251c\u2500\u2500 deeplabv3plus_efficientnetb0\n      \u2502   \u251c\u2500\u2500 accuracy.sh\n      \u2502   \u251c\u2500\u2500 fps.sh\n      \u2502   \u251c\u2500\u2500 latency.sh\n      \u2502   \u251c\u2500\u2500 workflow_accuracy.json\n      \u2502   \u251c\u2500\u2500 workflow_fps.json\n      \u2502   \u2514\u2500\u2500 workflow_latency.json\n      \u251c\u2500\u2500 fastscnn_efficientnetb0\n      \u2514\u2500\u2500 unet_mobilenet\n\n"})}),"\n",(0,s.jsxs)(n.p,{children:["The scripts required for accuracy evaluation are in the tools directory. It mainly includes the accuracy calculation scripts under ",(0,s.jsx)(n.code,{children:"python_tools"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"  tools:\n\n  python_tools\n    \u2514\u2500\u2500 accuracy_tools\n        \u251c\u2500\u2500 cityscapes_metric.py\n        \u251c\u2500\u2500 cls_eval.py\n        \u251c\u2500\u2500 coco_metric.py\n        \u251c\u2500\u2500 coco_det_eval.py\n        \u251c\u2500\u2500 config.py\n        \u251c\u2500\u2500 parsing_eval.py\n        \u251c\u2500\u2500 voc_det_eval.py### voc_metric.py\n\n"})}),"\n",(0,s.jsx)(n.admonition,{title:"Caution",type:"caution",children:(0,s.jsxs)(n.p,{children:["Before evaluation, execute the following commands to copy the ",(0,s.jsx)(n.code,{children:"ptq"})," directory to the development board, and then copy the ",(0,s.jsx)(n.code,{children:"model_zoo/runtime"})," to the ",(0,s.jsx)(n.code,{children:"ptq/model"})," directory."]})}),"\n",(0,s.jsx)(n.admonition,{title:"Caution",type:"caution",children:(0,s.jsxs)(n.p,{children:["The following examples are the test results using the ",(0,s.jsx)(n.strong,{children:"RDK X3"})," development board. If the ",(0,s.jsx)(n.strong,{children:"RDK Ultra"})," development board is used, the information may vary. Please refer to the specific test results!"]})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"  scp -r ai_toolchain_package/Ai_Toolchain_Package-release-vX.X.X-OE-vX.X.X/ai_benchmark/xj3/ptq root@192.168.1.10:/userdata/ptq/\n\n  scp -r model_zoo/runtime root@192.168.1.10:/userdata/ptq/model/\n"})}),"\n",(0,s.jsx)(n.h4,{id:"performance-evaluation",children:"Performance Evaluation"}),"\n",(0,s.jsx)(n.p,{children:"Performance evaluation includes latency and fps."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Usage of the evaluation script"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Enter the directory of the model to be evaluated and execute ",(0,s.jsx)(n.code,{children:"sh latency.sh"})," to test the latency of a single frame. As shown in the following figure:"]}),"\n",(0,s.jsx)(n.admonition,{title:"Caution",type:"caution",children:(0,s.jsxs)(n.p,{children:["The following examples are the test results using the ",(0,s.jsx)(n.strong,{children:"RDK X3"})," development board. If the ",(0,s.jsx)(n.strong,{children:"RDK Ultra"})," development board is used, the information may vary. Please refer to the specific test results!"]})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"  I0419 02:35:07.041095 39124 output_plugin.cc:80]  Infer latency:  [avg:  13.124ms,  max:  13.946ms,  min:  13.048ms], Post process latency: [avg:  3.584ms,  max:  3.650ms,  min:  3.498ms].\n"})}),"\n",(0,s.jsx)(n.admonition,{title:"Note",type:"info",children:(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"infer"})," represents the inference time of the model."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"Post process"})," represents the post-processing time."]}),"\n"]})}),"\n",(0,s.jsxs)(n.p,{children:["Enter the directory of the model to be evaluated and execute ",(0,s.jsx)(n.code,{children:"sh fps.sh"})," to test the frame rate. As shown in the following figure:"]}),"\n",(0,s.jsx)(n.admonition,{title:"Caution",type:"caution",children:(0,s.jsxs)(n.p,{children:["The following examples are the test results using the ",(0,s.jsx)(n.strong,{children:"RDK X3"})," development board. If the ",(0,s.jsx)(n.strong,{children:"RDK Ultra"})," development board is used, the information may vary. Please refer to the specific test results!"]})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"  I0419 02:35:00.044417 39094 output_plugin.cc:109]  Throughput: 176.39fps      # Model frame rate\n"})}),"\n",(0,s.jsx)(n.admonition,{title:"Remarks",type:"info",children:(0,s.jsx)(n.p,{children:"This function uses multi-threading and concurrency to achieve optimal performance on BPU. Due to the nature of multi-threading and data sampling, the frame rate value may be lower during the startup phase, and will gradually increase and stabilize over time. The fluctuation range of frame rate is controlled within 0.5%."})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Command line arguments"}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{title:"Note",type:"caution",children:(0,s.jsxs)(n.p,{children:["The following example is based on the test results using ",(0,s.jsx)(n.strong,{children:"RDK X3"})," development board. If using ",(0,s.jsx)(n.strong,{children:"RDK Ultra"})," development board, the information may vary. Please refer to specific test results for accuracy."]})}),"\n",(0,s.jsxs)(n.p,{children:["Content of ",(0,s.jsx)(n.code,{children:"fps.sh"})," script:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"  #!/bin/sh\n\n  source ../../base_config.sh\n  export SHOW_FPS_LOG=1\n  export STAT_CYCLE=10                             # Set environment variable, FPS statistics cycle\n\n  ${app} \\\n    --config_file=workflow_fps.json \\\n    --log_level=1\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Content of ",(0,s.jsx)(n.code,{children:"latency.sh"})," script:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"  #!/bin/sh\n\n  source ../../base_config.sh\n  export SHOW_LATENCY_LOG=1                            # Set environment variable, print LATENCY level log\n  export STAT_CYCLE=5                                  # Set environment variable, LATENCY statistics cycle\n\n  ${app} \\\n    --config_file=workflow_latency.json \\\n    --log_level=1\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Configuration file explanation"}),"\n"]}),"\n",(0,s.jsxs)(n.admonition,{title:"Remarks",type:"info",children:[(0,s.jsx)(n.p,{children:"Note: When the max_cache parameter is in effect, the images will be preprocessed and read into memory. To ensure the stable operation of your program, please do not set a value that is too large. We recommend setting the value to no more than 30.\n:::Taking the fcos_efficientnetb0 model as an example, the contents of the workflow_fps.json configuration file are as follows:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:' {\n    "input_config": {\n      "input_type": "image",                                    # Input data format, supports image or bin file\n      "height": 512,                                            # Input data height\n      "width": 512,                                             # Input data width\n      "data_type": 1,                                           # Input data type: HB_DNN_IMG_TYPE_NV12\n      "image_list_file": "../../../mini_data/coco/coco.lst",    # Path to the preprocessed dataset lst file\n      "need_pre_load": true,                                    # Whether to use preloading to read the dataset\n      "limit": 10,\n      "need_loop": true,                                        # Whether to read the data for evaluation in a loop\n      "max_cache": 10\n    },\n    "output_config": {\n      "output_type": "image",                                   # Visualized output data type\n      "image_list_enable": true,\n      "in_order": false                                         # Whether to output in order\n    },\n    "workflow": [\n      {\n        "method_type": "InferMethod",                           # Infer inference mode\n        "unique_name": "InferMethod",\n        "method_config": {\n          "core": 0,                                            # Inference core id\n          "model_file": "../../../model/runtime/fcos_efficientnetb0/fcos_efficientnetb0_512x512_nv12.bin" # Model file\n        }\n      },\n      {\n        "thread_count": 4,                                      # Post-processing thread count\n        "method_type": "PTQFcosPostProcessMethod",              # Post-processing method\n        "unique_name": "PTQFcosPostProcessMethod",\n        "method_config": {                                      # Post-processing parameters\n          "strides": [\n            8,\n            16,\n            32,\n            64,\n            128\n          ],\n          "class_num": 80,\n          "score_threshold": 0.5,\n          "topk": 1000,\n          "det_name_list": "../../config/data_name_list/coco_detlist.list"\n        }\n      }\n    ]\n  }\n'})}),(0,s.jsx)(n.p,{children:"workflow_latency.json as follows:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n   "input_config":{\n      "input_type":"image",\n      "height":512,\n      "width":512,\n      "data_type":1,\n      "image_list_file":"../../../mini_data/coco/coco.lst",\n      "need_pre_load":true,\n      "limit":1,\n      "need_loop":true,\n      "max_cache":10\n   },\n   "output_config":{\n      "output_type":"image",\n      "image_list_enable":true\n   },\n   "workflow":[\n      {\n         "method_type":"InferMethod",\n         "unique_name":"InferMethod",\n         "method_config":{\n            "core":0,\n            "model_file":"../../../model/runtime/fcos_efficientnetb0/fcos_efficientnetb0_512x512_nv12.bin"\n         }\n      },\n      {\n         "thread_count":1,\n         "method_type":"PTQFcosPostProcessMethod",\n         "unique_name":"PTQFcosPostProcessMethod",\n         "method_config":{\n            "strides":[\n               8,\n               16,\n               32,\n               64,\n               128\n            ],\n            "class_num":80,\n            "score_threshold":0.5,\n            "topk":1000,\n            "det_name_list":"../../config/data_name_list/coco_detlist.list"\n         }\n      }\n   ]\n}\n'})}),(0,s.jsx)(n.h4,{id:"accuracy-evaluation",children:"Accuracy Evaluation"}),(0,s.jsx)(n.p,{children:"The accuracy evaluation of the model consists of four steps:"}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Data preprocessing."}),"\n",(0,s.jsx)(n.li,{children:"Data loading."}),"\n",(0,s.jsx)(n.li,{children:"Model inference."}),"\n",(0,s.jsx)(n.li,{children:"Accuracy calculation."}),"\n"]}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.h4,{id:"data_preprocess",children:"Data Preprocessing"}),"\n"]}),"\n"]}),(0,s.jsx)(n.p,{children:'For PTQ models: Data preprocessing needs to be performed on an x86 development machine using the "hb_eval_preprocess" tool to preprocess the dataset.\nData preprocessing refers to specific operations performed on image data before feeding it into the model, such as image resizing, cropping, and padding.\nThis tool is integrated into the environment of the model conversion and compilation on the development machine. After the original dataset is preprocessed using this tool, it will generate a corresponding preprocessing binary file (.bin file) for the model.\nRunning "hb_eval_preprocess --help" directly will display the usage rules of this tool.'}),(0,s.jsx)(n.admonition,{title:"Tips",type:"tip",children:(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:['To learn about the command-line parameters of the "hb_eval_preprocess" tool, you can enter "hb_eval_preprocess -h" or refer to the section ',(0,s.jsx)(n.a,{href:"/rdk_doc/en/Advanced_development/toolchain_development/intermediate/ptq_process",children:(0,s.jsx)(n.strong,{children:"hb_eval_preprocess Tool"})})," for an explanation of PTQ quantization principles and steps."]}),"\n"]})})]}),"\n",(0,s.jsx)(n.p,{children:"Next, we will provide a detailed introduction to the dataset corresponding to each model in the example package and the preprocessing operations for the corresponding dataset:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'"VOC dataset": This dataset is mainly used for evaluating the ssd_mobilenetv1 model.\nIts directory structure is as follows, and the example mainly uses the val.txt file under "Main", source images under "JPEGImages", and annotation data under "Annotations":'}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"  .\n  \u2514\u2500\u2500 VOCdevkit                  # Root directory\n      \u2514\u2500\u2500 VOC2012                # Datasets of different years, only 2012 is downloaded here, there are other years like 2007 as well\n          \u251c\u2500\u2500 Annotations        # Stores XML files, each corresponding to a image in JPEGImages, explains the content of the image, etc.\n          \u251c\u2500\u2500 ImageSets          # This directory stores text files, and each line in the text file contains the name of an image, followed by \xb11 indicating positive or negative samples\n          \u2502   \u251c\u2500\u2500 Action\n          \u2502   \u251c\u2500\u2500 Layout\n          \u2502   \u251c\u2500\u2500 Main\n          \u2502   \u2514\u2500\u2500 Segmentation\n          \u251c\u2500\u2500 JPEGImages         # Stores source images\n          \u251c\u2500\u2500 SegmentationClass  # Stores images related to semantic segmentation\n          \u2514\u2500\u2500 SegmentationObject # Stores images related to instance segmentation\n"})}),"\n",(0,s.jsx)(n.p,{children:"Perform data preprocessing on the dataset:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"  hb_eval_preprocess -m ssd_mobilenetv1 -i VOCdevkit/VOC2012/JPEGImages -v VOCdevkit/VOC2012/ImageSets/Main/val.txt -o ./pre_ssd_mobilenetv1\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'"COCO dataset": This dataset is mainly used for evaluating detection models such as yolov2_darknet19, yolov3_darknet53, yolov5s, efficientdetd0, fcos_efficientnetb0, and centernet_resnet50, etc.The directory structure is as follows, and the example mainly uses the "instances_val2017.json" annotation file in the "annotations" folder and the images in the "images" folder:'}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"  .\n  \u251c\u2500\u2500 annotations    # stores annotation data\n  \u2514\u2500\u2500 images         # stores source images\n"})}),"\n",(0,s.jsx)(n.p,{children:"Preprocess the dataset:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"  hb_eval_preprocess -m model_name -i coco/coco_val2017/images -o ./pre_model_name\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'"ImageNet dataset": This dataset is mainly used for the evaluation of classification models such as EfficientNet_lite0, EfficientNet_Lite1, EfficientNet_Lite2, EfficientNet_Lite3, EfficientNet_Lite4, MobileNet, GoogleNet, ResNet, etc. In the example, the "val.txt" annotation file and the source images in the "val" directory are mainly used.'}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"  .\n  \u251c\u2500\u2500 val.txt\n  \u2514\u2500\u2500 val\n"})}),"\n",(0,s.jsx)(n.p,{children:"Preprocess the dataset:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"  hb_eval_preprocess -m model_name -i imagenet/val -o ./pre_model_name\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'"Cityscapes dataset": This dataset is used for the evaluation of segmentation models such as deeplabv3plus_efficientnetb0, deeplabv3plus_efficientnetm1, deeplabv3plus_efficientnetm2, fastscnn_efficientnetb0, etc. In the example, the annotation files in "./gtFine/val" and the source images in "./leftImg8bit/val" are mainly used.'}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"  .\n  \u251c\u2500\u2500 gtFine\n  \u2502   \u2514\u2500\u2500 val\n  \u2502       \u251c\u2500\u2500 frankfurt\n  \u2502       \u251c\u2500\u2500 lindau\n  \u2502       \u2514\u2500\u2500 munster\n  \u2514\u2500\u2500 leftImg8bit\n      \u2514\u2500\u2500 val\n          \u251c\u2500\u2500 frankfurt\n          \u251c\u2500\u2500 lindau\n          \u2514\u2500\u2500 munster\n"})}),"\n",(0,s.jsx)(n.p,{children:"Preprocess the dataset:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"  hb_eval_preprocess -m unet_mobilenet -i cityscapes/leftImg8bit/val -o ./pre_unet_mobilenet\n"})}),"\n",(0,s.jsx)(n.p,{children:'The workflow for running the accuracy calculation script in the example is:1. According to the value of the "image_list_file" parameter in "workflow_accurary.json", search for the corresponding "lst" file of the dataset.'}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsx)(n.li,{children:'Load each pre-processing file according to the preprocessing file path information stored in the "lst" file, and then perform inference.'}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:'Therefore, after generating the pre-processing file, it is necessary to generate the corresponding lst file and write the path of each pre-processing file into the lst file, which is related to the storage location of the dataset on the development board.\nHere we recommend placing it in the same directory as the "script" folder, as follows:'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"\n  # For RDK X3, use the following format:\n  |-- ptq\n  |   |-- data\n  |   |   |-- cityscapes\n  |   |   |   -- xxxx.bin             # Pre-processed binary file\n  |   |   |   -- ....\n  |   |   |   -- cityscapes.lst       # lst file: record the path of each pre-processing file\n  |   |   |-- coco\n  |   |   |   -- xxxx.bin\n  |   |   |   -- ....\n  |   |   |   -- coco.lst\n  |   |   |-- imagenet\n  |   |   |   -- xxxx.bin\n  |   |   |   -- ....\n  |   |   |   -- imagenet.lst\n  |   |   `-- voc\n  |   |   |   -- xxxx.bin\n  |   |   |   -- ....\n  |   |       `-- voc.lst\n  |   |-- model\n  |   |   |-- ...\n  |   |-- script\n  |   |   |-- ...\n\n  # For RDK Ultra, use the following format:\n  |-- ptq\n  |   |-- data\n  |   |   |-- cityscapes\n  |   |   |   |-- pre_deeplabv3plus_efficientnetb0\n  |   |   |   |   |-- xxxx.bin                            # Pre-processed binary file\n  |   |   |   |   |-- ....\n  |   |   |   |-- pre_deeplabv3plus_efficientnetb0.lst    # lst file: record the path of each pre-processing file\n  |   |   |   |-- pre_deeplabv3plus_efficientnetm1\n  |   |   |   |-- pre_deeplabv3plus_efficientnetm1.lst\n  |   |   |   |-- pre_deeplabv3plus_efficientnetm2\n  |   |   |   |-- pre_deeplabv3plus_efficientnetm2.lst\n  |   |   |   |-- pre_fastscnn_efficientnetb0\n  |   |   |   |-- pre_fastscnn_efficientnetb0.lst\n  |   |   |-- coco\n  |   |   |   |-- pre_centernet_resnet101\n  |   |   |   |   |-- xxxx.bin|   |   |   |   |-- ....\n  |   |   |   |-- pre_centernet_resnet101.lst\n  |   |   |   |-- pre_yolov3_darknet53\n  |   |   |   |-- pre_yolov3_darknet53.lst\n  |   |   |   |-- pre_yolov3_vargdarknet\n  |   |   |   |-- pre_yolov3_vargdarknet.lst\n  |   |   |   |-- pre_yolov5x\n  |   |   |   |-- pre_yolov5x.lst\n  |   |   |   |-- pre_preq_qat_fcos_efficientnetb0\n  |   |   |   |-- pre_preq_qat_fcos_efficientnetb0.lst\n  |   |   |   |-- pre_preq_qat_fcos_efficientnetb2\n  |   |   |   |-- pre_preq_qat_fcos_efficientnetb2.lst\n  |   |   |   |-- pre_preq_qat_fcos_efficientnetb3\n  |   |   |   |-- pre_preq_qat_fcos_efficientnetb3.lst\n  |   |   |-- imagenet\n  |   |   |   |-- pre_efficientnasnet_m\n  |   |   |   |   |-- xxxx.bin\n  |   |   |   |   |-- ....\n  |   |   |   |-- pre_efficientnasnet_m.lst\n  |   |   |   |-- pre_efficientnasnet_s\n  |   |   |   |-- pre_efficientnasnet_s.lst\n  |   |   |   |-- pre_efficientnet_lite4\n  |   |   |   |-- pre_efficientnet_lite4.lst\n  |   |   |   |-- pre_googlenet\n  |   |   |   |-- pre_googlenet.lst\n  |   |   |   |-- pre_mobilenetv1\n  |   |   |   |-- pre_mobilenetv1.lst\n  |   |   |   |-- pre_mobilenetv2\n  |   |   |   |-- pre_mobilenetv2.lst\n  |   |   |   |-- pre_resnet18\n  |   |   |   |-- pre_resnet18.lst\n  |   |   |   |-- pre_vargconvnet\n  |   |   |   |-- pre_vargconvnet.lst\n  |   |   |-- voc\n  |   |   |   |-- pre_ssd_mobilenetv1\n  |   |   |   |   |-- xxxx.bin\n  |   |   |   |   |-- ....\n  |   |   |   |-- pre_ssd_mobilenetv1.lst\n  |-- model\n  |   |-- ...\n  |-- script\n  |   |-- ...\n\n"})}),"\n",(0,s.jsx)(n.p,{children:"To generate the corresponding lst files, refer to the following method:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:'\n  # For RDK X3, please use the following command:\n  find ../../../data/coco/fcos -name "*bin*" > ../../../data/coco/coco.lst# RDK Ultra Please use the following command:\n  find ../../../data/coco/pre_centernet_resnet101 -name "*bin*" > ../../../data/coco/pre_centernet_resnet101.lst\n\n'})}),"\n",(0,s.jsxs)(n.p,{children:["The generated ",(0,s.jsx)(n.code,{children:"lst"})," file stores a relative path: ",(0,s.jsx)(n.code,{children:"../../../data/"})," or ",(0,s.jsx)(n.code,{children:"../../../data/coco/pre_centernet_resnet101/"}),", which can match the default configuration path of ",(0,s.jsx)(n.code,{children:"workflow_accuracy.json"}),".\nIf you need to change the storage location of the pre-processing dataset, make sure that the corresponding ",(0,s.jsx)(n.code,{children:"lst"})," file can be read by ",(0,s.jsx)(n.code,{children:"workflow_accuracy.json"}),". Secondly, ensure that the program can read the corresponding pre-processing file based on the path information in ",(0,s.jsx)(n.code,{children:"lst"}),"."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.h4,{id:"data-mounting",children:"Data Mounting"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Since the dataset is relatively large and not suitable to be placed directly on the development board, it can be mounted via nfs for the board to read."}),"\n",(0,s.jsx)(n.p,{children:"Host PC (requires root permission):"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Edit /etc/exports and add a line:\n",(0,s.jsx)(n.code,{children:"/nfs *(insecure,rw,sync,all_squash,anonuid=1000,anongid=1000,no_subtree_check)"}),".\n",(0,s.jsx)(n.code,{children:"/nfs"})," represents the mount path on the local machine, which can be replaced with a user-specified directory."]}),"\n",(0,s.jsxs)(n.li,{children:["Execute the command ",(0,s.jsx)(n.code,{children:"exportfs -a -r"})," to make /etc/exports take effect."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Board Side:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Create the directory to be mounted: ",(0,s.jsx)(n.code,{children:"mkdir -p /mnt"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"mount -t nfs {PC IP}:/nfs /mnt -o nolock"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The /nfs folder on the PC side is mounted to the /mnt folder on the board side. In this way, mount the folder containing the pre-processing data to the board side, and create a soft link from the /data directory to the /ptq directory on the board side, which is at the same level as the /script directory."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.h4,{id:"model-inference",children:"Model Inference"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{title:"Note",type:"caution",children:(0,s.jsxs)(n.p,{children:["The following example is the actual test result using ",(0,s.jsx)(n.strong,{children:"RDK X3"})," development board. If using ",(0,s.jsx)(n.strong,{children:"RDK Ultra"})," development board, there may be differences in the information. Please refer to the specific test for accuracy!"]})}),"\n",(0,s.jsxs)(n.p,{children:["After mounting the data, please log in to the development board. For the login method, please read the ",(0,s.jsx)(n.a,{href:"/rdk_doc/en/Quick_start/remote_login",children:(0,s.jsx)(n.strong,{children:"development board login"})})," section. After successful login, execute the ",(0,s.jsx)(n.code,{children:"accuracy.sh"})," script in the ",(0,s.jsx)(n.code,{children:"fcos_efficientnetb0/"})," directory as shown below:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'  /userdata/ptq/script/detection/fcos# sh accuracy.sh\n  ../../aarch64/bin/example --config_file=workflow_accuracy.json --log_level=2\n  ...\n  I0419 03:14:51.158655 39555 infer_method.cc:107] Predict DoProcess finished.\n  I0419 03:14:51.187361 39556 ptq_fcos_post_process_method.cc:123] PTQFcosPostProcessMethod DoProcess finished, predict result: [{"bbox":[-1.518860,71.691170,574.934631,638.294922],"prob":0.750647,"label":21,"class_name":"\n  I0118 14:02:43.636204 24782 ptq_fcos_post_process_method.cc:123] PTQFcosPostProcessMethod DoProcess finished, predict result: [{"bbox":[3.432283,164.936249,157.480042,264.276825],"prob":0.544454,"label":62,"class_name":"\n  ...\n'})}),"\n",(0,s.jsxs)(n.p,{children:["The program on the board side will generate an ",(0,s.jsx)(n.code,{children:"eval.log"})," file in the current directory, which is the result file of the prediction."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.h4,{id:"accuracy-calculation",children:"Accuracy Calculation"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{title:"Caution",type:"caution",children:(0,s.jsx)(n.p,{children:'Please perform the accuracy calculation in the environment of the "development machine" model conversion.'})}),"\n",(0,s.jsx)(n.p,{children:'The script for accuracy calculation is located in the "python_tools" directory. Among them, in "accuracy_tools":\ncls_eval.py is used to calculate the accuracy of classification models;\ncoco_det_eval.py is used to calculate the accuracy of detection models evaluated using the COCO dataset;\nparsing_eval.py is used to calculate the accuracy of segmentation models evaluated using the Cityscapes dataset;\nvoc_det_eval.py is used to calculate the accuracy of detection models evaluated using the VOC dataset.'}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Classification Model"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The calculation method for classification models using the CIFAR-10 dataset and the ImageNet dataset is as follows:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"  #!/bin/sh\n\n  python3 cls_eval.py --log_file=eval.log --gt_file=val.txt\n"})}),"\n",(0,s.jsx)(n.admonition,{title:"Note",type:"info",children:(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'"log_file": The prediction result file of the classification model.'}),"\n",(0,s.jsx)(n.li,{children:'"gt_file": The annotation file of the CIFAR-10 and ImageNet datasets.'}),"\n"]})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Detection Model"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The calculation method for detection models using the COCO dataset is as follows:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"  #!/bin/sh\n\n  python3 coco_det_eval.py --eval_result_path=eval.log --annotation_path=instances_val2017.json\n"})}),"\n",(0,s.jsx)(n.admonition,{title:"Note",type:"info",children:(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'"eval_result_path": The prediction result file of the detection model.'}),"\n",(0,s.jsx)(n.li,{children:'"annotation_path": The annotation file of the COCO dataset.'}),"\n"]})}),"\n",(0,s.jsx)(n.p,{children:"The calculation method for detection models using the VOC dataset is as follows:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"  #!/bin/sh\n\n  python3 voc_det_eval.py --eval_result_path=eval.log --annotation_path=../Annotations --val_txt_path=../val.txt\n"})}),"\n",(0,s.jsx)(n.admonition,{title:"Note",type:"info",children:(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:['"eval_result_path": The prediction result file of the detection model.- ',(0,s.jsx)(n.code,{children:"annotation_path"}),": The annotation file of the VOC dataset."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"val_txt_path"}),": The val.txt file in the ImageSets/Main folder of the VOC dataset."]}),"\n"]})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Segmentation Model"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The accuracy calculation method for the segmentation model using the Cityscapes dataset is as follows:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"  #!/bin/sh\n\n  python3 parsing_eval.py --width=output_width --height=output_height --log_file=eval.log --gt_path=cityscapes/gtFine/val\n"})}),"\n",(0,s.jsx)(n.admonition,{title:"Remark",type:"info",children:(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"width"}),": The output width of the segmentation model."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"height"}),": The output height of the segmentation model."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"log_file"}),": The prediction result file of the segmentation model."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"gt_path"}),": The annotation file of the Cityscapes dataset."]}),"\n"]})}),"\n",(0,s.jsx)(n.h4,{id:"model-integration",children:"Model Integration"}),"\n",(0,s.jsx)(n.p,{children:"Model post-processing integration mainly consists of 2 steps, taking centernet_resnet50 model integration as an example:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Add the post-processing file ",(0,s.jsx)(n.code,{children:"ptq_centernet_post_process_method.cc"})," and the header file ",(0,s.jsx)(n.code,{children:"ptq_centernet_post_process_method.h"}),"."]}),"\n",(0,s.jsx)(n.li,{children:"Add the model execution script and configuration file."}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"adding-post-processing-files",children:"Adding Post-processing Files"}),"\n",(0,s.jsx)(n.admonition,{title:"Note",type:"caution",children:(0,s.jsx)(n.p,{children:"The following example is based on the RDK X3 development board. If you are using the RDK Ultra development board, the information may vary. Please refer to the specific instructions."})}),"\n",(0,s.jsxs)(n.p,{children:["The post-processing code file can be directly reused from any post-processing file in the src/method directory. The main modifications are the ",(0,s.jsx)(n.code,{children:"InitFromJsonString"})," function and the ",(0,s.jsx)(n.code,{children:"PostProcess"})," function."]}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"InitFromJsonString"})," function is mainly used to read the post-processing related parameter configurations from the workflow.json file. Users can customize the input parameters accordingly.\nThe ",(0,s.jsx)(n.code,{children:"PostProcess"})," function completes the logic of post-processing."]}),"\n",(0,s.jsxs)(n.p,{children:["The .cc file should be placed in the ",(0,s.jsx)(n.code,{children:"ai_benchmark/code/src/method/"})," path,\nThe .h header file should be placed in the ",(0,s.jsx)(n.code,{children:"ai_benchmark/code/include/method/"})," path:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"  |--ai_benchmark\n  |  |--code                                                 # Example source code\n  |  |  |--include\n  |  |  |  |--method                                         # Add header files in this folder\n  |  |  |  |  |--ptq_centernet_post_process_method.h\n  |  |  |  |  |--......\n|--ptq_yolo5_post_process_method.h\n  |--src\n  |  |--method                                         # Add post-processing .cc files in this folder\n  |  |  |--ptq_centernet_post_process_method.cc\n  |  |  |  |--......\n  |  |  |  |--ptq_yolo5_post_process_method.cc\n"})}),"\n",(0,s.jsx)(n.h4,{id:"add-model-running-scripts-and-configuration-files",children:"Add model running scripts and configuration files"}),"\n",(0,s.jsx)(n.p,{children:"The directory structure of the scripts is as follows:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"|--ai_benchmark\n  |--xj3/ptq/script                                      # Example script folder\n  |  |--detection\n  |  |  |--centernet_resnet50\n  |  |  |  |--accuracy.sh                                # Accuracy testing script\n  |  |  |  |--fps.sh                                     # Performance testing script\n  |  |  |  |--latency.sh                                 # Single frame latency example script\n  |  |  |  |--workflow_accuracy.json                     # Accuracy configuration file\n  |  |  |  |--workflow_fps.json                          # Performance configuration file\n  |  |  |  |--workflow_latency.json                      # Single frame latency configuration file\n"})}),"\n",(0,s.jsx)(n.h4,{id:"auxiliary-tools-and-common-operations-1",children:"Auxiliary tools and common operations"}),"\n",(0,s.jsx)(n.h4,{id:"instructions-for-using-the-logging-system",children:"Instructions for using the logging system"}),"\n",(0,s.jsx)(n.p,{children:'The logging system mainly includes "sample logs" and "model inference API DNN logs".\nThe sample logs refer to the application logs in the delivery package.\nUsers can obtain different logs based on their needs.'}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Sample logs"}),"\n"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Log levels. Sample logs mainly use vlog from glog, and there are four custom levels:"}),"\n"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'"0" (SYSTEM), which is mainly used to output error messages;'}),"\n",(0,s.jsx)(n.li,{children:'"1" (REPORT), which is mainly used in the sample code to output performance data;'}),"\n",(0,s.jsx)(n.li,{children:'"2" (DETAIL), which is mainly used in the sample code to output system status information;'}),"\n",(0,s.jsx)(n.li,{children:'"3" (DEBUG), which is mainly used in the sample code to output debug information.\nLogging level setting rules: Suppose the level is set to "P", if there is a level "Q" lower than "P",\nit can be enabled, or else it will be suppressed; DEBUG>DETAIL>REPORT>SYSTEM by default.'}),"\n"]}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsx)(n.li,{children:'Log level setting. Set the log level through the "log_level" parameter. When running the sample, specify the "log_level" parameter to set the level.\nFor example, specifying "log_level=0" outputs SYSTEM logs; if specifying "log_level=3",\nit outputs DEBUG, DETAIL, REPORT, and SYSTEM logs.'}),"\n"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Model inference API DNN logs"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:['For the configuration of model inference DNN API logs, please refer to the "Configuration Information" section in the ',(0,s.jsx)(n.a,{href:"../../../04_Algorithm_Application/02_cdev_dnn_api/cdev_dnn_api.md",children:"Model Inference DNN API Usage Guide"}),".#### Operator Time Explanation"]}),"\n",(0,s.jsxs)(n.p,{children:["The statistics on the performance of the model operators (OP) are achieved by setting the environment variable ",(0,s.jsx)(n.code,{children:"HB_DNN_PROFILER_LOG_PATH"}),". This section introduces the performance analysis of the model's inference, which helps developers to understand the real inference performance of the model.\nThe type and value of this variable are described as follows:"]}),"\n",(0,s.jsx)(n.admonition,{title:"Note",type:"info",children:(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"export HB_DNN_PROFILER_LOG_PATH=${path}"}),": represents the output path where the dumped OP nodes are located. After the program runs normally and exits, a profiler.log file will be generated."]})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Example Explanation"}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{title:"Note",type:"caution",children:(0,s.jsxs)(n.p,{children:["The following example uses ",(0,s.jsx)(n.strong,{children:"RDK X3"})," development board as an example. If using ",(0,s.jsx)(n.strong,{children:"RDK Ultra"})," development board, there may be differences in the information. Please refer to the specific board used!"]})}),"\n",(0,s.jsxs)(n.p,{children:["The following code block takes the mobilenetv1 model as an example, starts running the model with a single thread, and sets ",(0,s.jsx)(n.code,{children:"export HB_DNN_PROFILER_LOG_PATH=./"}),". The statistical output information is as follows:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-c",children:'  {\n    "perf_result": {\n      "FPS": 677.6192525182025,\n      "average_latency": 11.506142616271973\n    },\n    "running_condition": {\n      "core_id": 0,\n      "frame_count": 200,\n      "model_name": "mobilenetv1_224x224_nv12",\n      "run_time": 295.151,\n      "thread_num": 1\n    }\n  }\n  ***\n  {\n    "chip_latency": {\n      "BPU_inference_time_cost": {\n        "avg_time": 11.09122,\n        "max_time": 11.54,\n        "min_time": 3.439\n      },\n      "CPU_inference_time_cost": {\n        "avg_time": 0.18836999999999998,\n        "max_time": 0.4630000000000001,\n        "min_time": 0.127\n      }\n    },\n    "model_latency": {\n    "BPU_MOBILENET_subgraph_0": {\n    "avg_time": 11.09122,\n    "max_time": 11.54,\n    "min_time": 3.439\n  },\n  "Dequantize_fc7_1_HzDequantize": {\n    "avg_time": 0.07884999999999999,\n    "max_time": 0.158,\n    "min_time": 0.068\n  },\n  "MOBILENET_subgraph_0_output_layout_convert": {\n    "avg_time": 0.018765,\n    "max_time": 0.08,\n    "min_time": 0.01\n  },\n  "Preprocess": {\n    "avg_time": 0.0065,\n    "max_time": 0.058,\n    "min_time": 0.003\n  },\n  "Softmax_prob": {\n    "avg_time": 0.084255,\n    "max_time": 0.167,\n    "min_time": 0.046\n  }\n},\n"task_latency": {\n  "TaskPendingTime": {\n    "avg_time": 0.029375,\n    "max_time": 0.059,\n    "min_time": 0.009\n  },\n  "TaskRunningTime": {\n    "avg_time": 11.40324,\n    "max_time": 11.801,\n    "min_time": 4.008\n  }\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:'The above output shows the "model_latency" and "task_latency". The "model_latency" outputs the time consumption for each OP in the model, while the "task_latency" outputs the time consumption for each task module in the model.'}),"\n",(0,s.jsx)(n.admonition,{title:"Note",type:"info",children:(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The profiler.log file will only be generated if the program exits normally."}),"\n"]})}),"\n",(0,s.jsx)(n.h4,{id:"dump-tool",children:"Dump Tool"}),"\n",(0,s.jsxs)(n.p,{children:["This section mainly introduces the method of enabling the dump tool. Generally, it is not necessary to pay attention to it. The dump tool is only used when the model accuracy is abnormal.\nBy setting the environment variable ",(0,s.jsx)(n.code,{children:"export HB_DNN_DUMP_PATH=${path}"}),", the input and output of each node in the model inference process can be dumped. Based on the dumped output, it is possible to check whether there are consistency issues between the model inference on the development machine simulator and the development board: that is, whether the outputs of the same model and the same input on the development machine simulator and the development board are exactly the same."]}),"\n",(0,s.jsx)(n.h2,{id:"instructions-for-model-on-board-analysis-tool",children:"Instructions for Model-on-Board Analysis Tool"}),"\n",(0,s.jsx)(n.h3,{id:"overview-1",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:'This section introduces a fast verification tool for model inference on D-Robotics Algorithm Toolchain. This tool allows developers to quickly obtain information about the "xxx.bin" model, model inference performance, and model debugging.'}),"\n",(0,s.jsx)(n.h3,{id:"instructions-for-hrt_model_exec-tool",children:"Instructions for hrt_model_exec Tool"}),"\n",(0,s.jsx)(n.p,{children:'The "hrt_model_exec" tool allows for quickly evaluating model inference performance and obtaining model information on the development board.'}),"\n",(0,s.jsx)(n.p,{children:"Currently, the tool provides three types of functions, as shown in the table below:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Number"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Subcommand"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Description"})})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"1"}),(0,s.jsx)(n.td,{children:'"model_info"'}),(0,s.jsx)(n.td,{children:"Get model information, such as input-output information of the model."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"2"}),(0,s.jsx)(n.td,{children:'"infer"'}),(0,s.jsx)(n.td,{children:"Perform model inference and obtain the inference results."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"3"}),(0,s.jsx)(n.td,{children:'"perf"'}),(0,s.jsx)(n.td,{children:"Perform model performance analysis and obtain the performance analysis results."})]})]})]}),"\n",(0,s.jsxs)(n.admonition,{title:"Tips",type:"tip",children:[(0,s.jsx)(n.p,{children:'The tool can also check the version of the "dnn" prediction library using the "-v" or "--version" command.'}),(0,s.jsx)(n.p,{children:"For example: hrt_model_exec -v or hrt_model_exec --version"})]}),"\n",(0,s.jsx)(n.h4,{id:"input-parameter-description",children:"Input Parameter Description"}),"\n",(0,s.jsx)(n.p,{children:'Run "hrt_model_exec" , "hrt_model_exec -h" or "hrt_model_exec --help" on the development board to get the tool\'s usage parameter details.\nAs shown in the figure below:'}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/hrt_model_exec_help.png",alt:"hrt_model_exec_help"})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Number"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Parameter"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Type"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Description"})})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"1"}),(0,s.jsx)(n.td,{children:'"model_file"'}),(0,s.jsx)(n.td,{children:"string"}),(0,s.jsx)(n.td,{children:"Model file path, multiple paths can be separated by commas."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"2"}),(0,s.jsx)(n.td,{children:'"model_name"'}),(0,s.jsx)(n.td,{children:"string"}),(0,s.jsx)(n.td,{children:"Specify the name of a model in the model."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"3"}),(0,s.jsx)(n.td,{children:'"core_id"'}),(0,s.jsx)(n.td,{children:"int"}),(0,s.jsx)(n.td,{children:"Specify the running core."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"4"}),(0,s.jsx)(n.td,{children:'"input_file"'}),(0,s.jsx)(n.td,{children:"string"}),(0,s.jsx)(n.td,{children:"Model input information, multiple can be separated by commas."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"5"}),(0,s.jsx)(n.td,{children:'"roi_infer"'}),(0,s.jsx)(n.td,{children:"bool"}),(0,s.jsx)(n.td,{children:"Enable resizer model inference."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"6"}),(0,s.jsx)(n.td,{children:'"roi"'}),(0,s.jsx)(n.td,{children:"string"}),(0,s.jsx)(n.td,{children:"Specify the ROI area required for inference of the resizer model."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"7"}),(0,s.jsx)(n.td,{children:'"frame_count"'}),(0,s.jsx)(n.td,{children:"int"}),(0,s.jsx)(n.td,{children:"Execute the number of frames for the model run."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"8"}),(0,s.jsx)(n.td,{children:'"dump_intermediate"'}),(0,s.jsx)(n.td,{children:"string"}),(0,s.jsx)(n.td,{children:"Dump the inputs and outputs of each layer of the model."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"9"}),(0,s.jsx)(n.td,{children:'"enable_dump"'}),(0,s.jsx)(n.td,{children:"bool"}),(0,s.jsx)(n.td,{children:"Enable dumping of model inputs and outputs."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"10"}),(0,s.jsx)(n.td,{children:'"dump_precision"'}),(0,s.jsx)(n.td,{children:"int"}),(0,s.jsx)(n.td,{children:"Control the number of decimal places for outputting float data in txt format."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"11"}),(0,s.jsx)(n.td,{children:'"hybrid_dequantize_process"'}),(0,s.jsx)(n.td,{children:"bool"}),(0,s.jsx)(n.td,{children:"Control the output of float data in txt format."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"12"}),(0,s.jsx)(n.td,{children:'"dump_format"'}),(0,s.jsx)(n.td,{children:"string"}),(0,s.jsx)(n.td,{children:"Format for dumping model inputs and outputs."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"13"}),(0,s.jsx)(n.td,{children:'"dump_txt_axis"'}),(0,s.jsx)(n.td,{children:"int"}),(0,s.jsx)(n.td,{children:"Control the line break rules for input and output in txt format."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"14"}),(0,s.jsx)(n.td,{children:'"enable_cls_post_process"'}),(0,s.jsx)(n.td,{children:"bool"}),(0,s.jsx)(n.td,{children:"Enable classification post-processing."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"16"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"thread_num"})}),(0,s.jsx)(n.td,{children:"int"}),(0,s.jsx)(n.td,{children:"Specify the number of threads to run the program."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"17"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"profile_path"})}),(0,s.jsx)(n.td,{children:"string"}),(0,s.jsx)(n.td,{children:"Save path for model performance/scheduling performance statistics."})]})]})]}),"\n",(0,s.jsx)(n.h4,{id:"instructions",children:"Instructions"}),"\n",(0,s.jsxs)(n.p,{children:["This section describes the specific usage of the three subfunctions of the ",(0,s.jsx)(n.code,{children:"hrt_model_exec"})," tool."]}),"\n",(0,s.jsx)(n.h4,{id:"model_info",children:(0,s.jsx)(n.code,{children:"model_info"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Overview"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["This parameter is used to obtain model information. The supported models are QAT models and PTQ models.\nThis parameter is used together with ",(0,s.jsx)(n.code,{children:"model_file"})," to obtain detailed information about the model;\nThe model information includes the model's input and output information ",(0,s.jsx)(n.code,{children:"hbDNNTensorProperties"})," and the model's segment information ",(0,s.jsx)(n.code,{children:"stage"}),"; The model's segment information is: a picture can be inferred in multiple stages. The stage information is [x1, y1, x2, y2], which are the coordinates of the top left and bottom right corners of the picture inference. Currently, D-Robotics RDK Ultra's Bayes architecture supports inference of such segmented models, and the models on RDK X3 are all 1-stage models."]}),"\n",(0,s.jsx)(n.admonition,{title:"Tips",type:"tip",children:(0,s.jsxs)(n.p,{children:["If ",(0,s.jsx)(n.code,{children:"model_name"})," is not specified, all model information in the model will be output. If ",(0,s.jsx)(n.code,{children:"model_name"})," is specified, only the information of the corresponding model will be output."]})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Example"}),"\n"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Single model"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"  hrt_model_exec model_info --model_file=xxx.bin\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsx)(n.li,{children:"Multiple models (output all model information)"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"  hrt_model_exec model_info --model_file=xxx.bin,xxx.bin  \n"})}),"\n",(0,s.jsxs)(n.ol,{start:"3",children:["\n",(0,s.jsx)(n.li,{children:"Multiple models - pack model (output specific model information)"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"  hrt_model_exec model_info --model_file=xxx.bin --model_name=xx\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/model_info.png",alt:"model_info"})}),"\n",(0,s.jsx)(n.h4,{id:"supplementary-explanation-of-input-parameters",children:"Supplementary Explanation of Input Parameters"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Repeat Input"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["If the same parameter input is specified repeatedly, the parameters will be overwritten. For example, when obtaining model information, if two model files are specified repeatedly, the later parameter input ",(0,s.jsx)(n.code,{children:"yyy.bin"})," will be used:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"  hrt_model_exec model_info --model_file=xxx.bin --model_file=yyy.bin\n"})}),"\n",(0,s.jsxs)(n.p,{children:["If input is repeated without adding the command line parameter ",(0,s.jsx)(n.code,{children:"--model_file"}),", the value after the command line parameter will be used, and parameters without arguments will not be recognized. For example, in the following example, ",(0,s.jsx)(n.code,{children:"yyy.bin"})," will be ignored and the parameter value will be ",(0,s.jsx)(n.code,{children:"xxx.bin"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"  hrt_model_exec model_info --model_file=xxx.bin yyy.bin\n"})}),"\n",(0,s.jsx)(n.h4,{id:"infer",children:(0,s.jsx)(n.code,{children:"infer"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Overview"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["This parameter is used to input a custom image, and the model infers one frame and provides the inference result.\nThis parameter needs to be used together with ",(0,s.jsx)(n.code,{children:"input_file"})," to specify the input image path. The tool resizes the image and organizes the model input information based on the model information."]}),"\n",(0,s.jsx)(n.admonition,{title:"Tip",type:"tip",children:(0,s.jsx)(n.p,{children:"The program runs single-threaded to process one frame of data and outputs the time it takes for the model to run."})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Example Explanation"}),"\n"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Single Model"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"  hrt_model_exec infer --model_file=xxx.bin --input_file=xxx.jpg\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsx)(n.li,{children:"Multi Model"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"  hrt_model_exec infer --model_file=xxx.bin,xxx.bin --model_name=xx --input_file=xxx.jpg\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/infer.png",alt:"infer"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Optional Parameters"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameters"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"core_id"})}),(0,s.jsxs)(n.td,{children:["Specify the core id for model inference, 0: all cores, 1: core 0, 2: core 1; default is ",(0,s.jsx)(n.code,{children:"0"}),"."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"roi_infer"})}),(0,s.jsxs)(n.td,{children:["Enable resizer model inference; if the model input contains a resizer source, set to ",(0,s.jsx)(n.code,{children:"true"}),", default is ",(0,s.jsx)(n.code,{children:"false"}),"."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"roi"})}),(0,s.jsxs)(n.td,{children:["Effective when ",(0,s.jsx)(n.code,{children:"roi_infer"})," is ",(0,s.jsx)(n.code,{children:"true"}),", set the ",(0,s.jsx)(n.code,{children:"roi"})," area required for inference of the resizer model, separated by semicolons."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"frame_count"})}),(0,s.jsxs)(n.td,{children:["Set the number of frames the ",(0,s.jsx)(n.code,{children:"infer"})," runs, repeat the inference for one frame, can be used with ",(0,s.jsx)(n.code,{children:"enable_dump"})," to check the consistency of the output, default is ",(0,s.jsx)(n.code,{children:"1"}),"."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"dump_intermediate"})}),(0,s.jsxs)(n.td,{children:["Dump input and output data for each layer of the model, default value is ",(0,s.jsx)(n.code,{children:"0"}),", do not dump data. ",(0,s.jsx)(n.code,{children:"1"}),": output file type is ",(0,s.jsx)(n.code,{children:"bin"}),"; ",(0,s.jsx)(n.code,{children:"2"}),": output type is ",(0,s.jsx)(n.code,{children:"bin"})," and ",(0,s.jsx)(n.code,{children:"txt"}),", with BPU node output as aligned data; ",(0,s.jsx)(n.code,{children:"3"}),": output type is ",(0,s.jsx)(n.code,{children:"bin"})," and ",(0,s.jsx)(n.code,{children:"txt"}),", with BPU node output as valid data."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"enable_dump"})}),(0,s.jsxs)(n.td,{children:["Dump model output data, default is ",(0,s.jsx)(n.code,{children:"false"}),"."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"dump_precision"})}),(0,s.jsxs)(n.td,{children:["Control the decimal places for outputting float type data in txt format, default is ",(0,s.jsx)(n.code,{children:"9"}),"."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"hybrid_dequantize_process"})}),(0,s.jsx)(n.td,{children:"Control the float type data in txt format, if the output is fixed point data, perform reverse quantization processing, currently only supports four-dimensional models."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"dump_format"})}),(0,s.jsxs)(n.td,{children:["Type of output file for dumping the model, optional parameters are ",(0,s.jsx)(n.code,{children:"bin"})," or ",(0,s.jsx)(n.code,{children:"txt"}),", default is ",(0,s.jsx)(n.code,{children:"bin"}),"."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"dump_txt_axis"})}),(0,s.jsxs)(n.td,{children:["Line break rules for dumping model txt format output; if the output dimension is n, the parameter range is [0, n], default is ",(0,s.jsx)(n.code,{children:"4"}),"."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"enable_cls_post_process"})}),(0,s.jsxs)(n.td,{children:["Enable classification post-processing, currently only supports ptq classification models, default is ",(0,s.jsx)(n.code,{children:"false"}),"."]})]})]})]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"multi-input-model-explanation",children:(0,s.jsx)(n.code,{children:"Multi-input Model Explanation"})}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"infer"})," function of the tool supports the inference of multi-input models, supports image input, binary file input, and text file input, and the input data is separated by commas.\nThe input information of the model can be viewed through ",(0,s.jsx)(n.code,{children:"model_info"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"hrt_model_exec infer --model_file=xxx.bin --input_file=xxx.jpg,input.txt\n"})}),"\n",(0,s.jsx)(n.h4,{id:"supplementary-explanation-of-input-parameters-1",children:(0,s.jsx)(n.code,{children:"Supplementary Explanation of Input Parameters"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"input_file"})}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["For image type input, the file name extension must be one of the following: ",(0,s.jsx)(n.code,{children:"bin"})," / ",(0,s.jsx)(n.code,{children:"JPG"})," / ",(0,s.jsx)(n.code,{children:"JPEG"})," / ",(0,s.jsx)(n.code,{children:"jpg"})," / ",(0,s.jsx)(n.code,{children:"jpeg"}),". For feature input, the file name extension must be one of the following: ",(0,s.jsx)(n.code,{children:"bin"})," / ",(0,s.jsx)(n.code,{children:"txt"}),". Each input file needs to be separated by a comma. For example: ",(0,s.jsx)(n.code,{children:"xxx.jpg,input.txt"}),"."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"enable_cls_post_process"})}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Enable classification post-processing. It is only supported in PTQ classification models. When the sub-command is ",(0,s.jsx)(n.code,{children:"infer"}),", the variable should be set to ",(0,s.jsx)(n.code,{children:"true"})," to print the classification results. Refer to the following figure:"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/enable_cls_post_process.png",alt:"enable_cls_post_process"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"roi_infer"})}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["If the model contains a resizer input source, both ",(0,s.jsx)(n.code,{children:"infer"})," and ",(0,s.jsx)(n.code,{children:"perf"})," functions need to set ",(0,s.jsx)(n.code,{children:"roi_infer"})," to true, and configure the ",(0,s.jsx)(n.code,{children:"input_file"})," and ",(0,s.jsx)(n.code,{children:"roi"})," parameters corresponding to the input source. For example, if the model has three inputs with the input source order of [",(0,s.jsx)(n.code,{children:"ddr"}),", ",(0,s.jsx)(n.code,{children:"resizer"}),", ",(0,s.jsx)(n.code,{children:"resizer"}),"], the command line for inferring two sets of input data is as follows:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'  // infer\n  hrt_model_exec infer --model_file=xxx.bin --input_file="xx0.bin,xx1.jpg,xx2.jpg,xx3.bin,xx4.jpg,xx5.jpg"  --roi="2,4,123,125;6,8,111,113;27,46,143,195;16,28,131,183" \n  // perf\n  hrt_model_exec perf --model_file=xxx.bin --input_file="xx0.bin,xx1.jpg,xx2.jpg,xx3.bin,xx4.jpg,xx5.jpg"   --roi="2,4,123,125;6,8,111,113;27,46,143,195;16,28,131,183"\n'})}),"\n",(0,s.jsxs)(n.p,{children:["Each ",(0,s.jsx)(n.code,{children:"roi"})," input needs to be separated by a semicolon."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"dump_intermediate"})}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Dump the input data and output data of each layer node in the model. When ",(0,s.jsx)(n.code,{children:"dump_intermediate=0"}),", the default dump function is turned off. When ",(0,s.jsx)(n.code,{children:"dump_intermediate=1"}),", the input data and output data of each layer node in the model are saved in ",(0,s.jsx)(n.code,{children:"bin"})," format, and the output of ",(0,s.jsx)(n.code,{children:"BPU"})," node is saved as ",(0,s.jsx)(n.code,{children:"aligned"})," data. When ",(0,s.jsx)(n.code,{children:"dump_intermediate=2"}),", the input data and output data of each layer node in the model are saved in both ",(0,s.jsx)(n.code,{children:"bin"})," and ",(0,s.jsx)(n.code,{children:"txt"})," formats, and the output of ",(0,s.jsx)(n.code,{children:"BPU"})," node is saved as ",(0,s.jsx)(n.code,{children:"aligned"})," data. When ",(0,s.jsx)(n.code,{children:"dump_intermediate=3"}),", the input data and output data of each layer node in the model are saved in both ",(0,s.jsx)(n.code,{children:"bin"})," and ",(0,s.jsx)(n.code,{children:"txt"})," formats, and the output of ",(0,s.jsx)(n.code,{children:"BPU"})," node is saved as ",(0,s.jsx)(n.code,{children:"valid"})," data. For example, if the model has two inputs with the input source order of [",(0,s.jsx)(n.code,{children:"pyramid"}),", ",(0,s.jsx)(n.code,{children:"ddr"}),"], and the input and output of each layer node in the model are saved as ",(0,s.jsx)(n.code,{children:"bin"})," files, and the output of ",(0,s.jsx)(n.code,{children:"BPU"})," node is saved as ",(0,s.jsx)(n.code,{children:"aligned"})," type, the inference command line is as follows:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'  hrt_model_exec infer --model_file=xxx.bin --input_file="xx0.jpg,xx1.bin"  --dump_intermediate=1\n'})}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"dump_intermediate"})," parameter supports both ",(0,s.jsx)(n.code,{children:"infer"})," and ",(0,s.jsx)(n.code,{children:"perf"})," modes."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"hybrid_dequantize_process"})}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Control the float type output data in txt format. The ",(0,s.jsx)(n.code,{children:"hybrid_dequantize_process"})," parameter is effective when ",(0,s.jsx)(n.code,{children:"enable_dump=true"}),". When ",(0,s.jsx)(n.code,{children:"enable_dump=true"}),", if ",(0,s.jsx)(n.code,{children:"hybrid_dequantize_process=true"})," is set, the integer output data is dequantized, and all outputs are saved as ",(0,s.jsx)(n.code,{children:"txt"})," files in float format. The output of the model is ",(0,s.jsx)(n.code,{children:"valid"})," data, and it supports configuring ",(0,s.jsx)(n.code,{children:"dump_txt_axis"})," and ",(0,s.jsx)(n.code,{children:"dump_precision"}),". If ",(0,s.jsx)(n.code,{children:"hybrid_dequantize_process=false"})," is set, the ",(0,s.jsx)(n.code,{children:"aligned"})," data of the model output is directly saved without any processing. For example, if the model has three outputs with the order of tensor data types as [",(0,s.jsx)(n.code,{children:"float"}),", ",(0,s.jsx)(n.code,{children:"int32"}),", ",(0,s.jsx)(n.code,{children:"int16"}),"], and the ",(0,s.jsx)(n.code,{children:"valid"})," data of float type in txt format is output, the inference command line is as follows:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'   hrt_model_exec infer --model_file=xxx.bin --input_file="xx0.jpg,xx1.bin"  --dump_intermediate=1\n'})}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"dump_intermediate"})," parameter supports both ",(0,s.jsx)(n.code,{children:"infer"})," and ",(0,s.jsx)(n.code,{children:"perf"}),' modes.// Output float type data\nhrt_model_exec infer --model_file=xxx.bin --input_file="xx.bin" --enable_dump=true --hybrid_dequantize_process=true']}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"hybrid_dequantize_process"})," parameter currently only supports four-dimensional models."]}),"\n",(0,s.jsx)(n.h4,{id:"perf",children:(0,s.jsx)(n.code,{children:"perf"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Overview"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This parameter is used to test the inference performance of the model.\nWith this command tool, users do not need to input data. The program will automatically construct the input tensor of the model based on the model information, and the tensor data will be random.\nThe program runs with a default single thread and 200 frames of data. When the perf_time parameter is specified, the frame_count parameter is invalid, and the program will exit after executing for the specified time.\nAfter the program completes, it will output the number of program threads, frame count, total model inference time, average model inference latency, frame rate information, etc."}),"\n",(0,s.jsx)(n.admonition,{title:"Tips",type:"tip",children:(0,s.jsx)(n.p,{children:"The program prints performance information every 200 frames: maximum, minimum, and average values of latency. When the number of frames is less than 200, the program prints once after the program finishes running."})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Example"}),"\n"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Single model"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"hrt_model_exec perf --model_file=xxx.bin\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsx)(n.li,{children:"Multiple models"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"hrt_model_exec perf --model_file=xxx.bin,xxx.bin --model_name=xx\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/perf.png",alt:"perf"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Optional parameters"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"core_id"})}),(0,s.jsxs)(n.td,{children:["Specify the core id for model inference. 0: any core, 1: core0, 2: core1; default is ",(0,s.jsx)(n.code,{children:"0"}),"."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"input_file"})}),(0,s.jsx)(n.td,{children:"Model input information, multiple inputs can be separated by commas."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"roi_infer"})}),(0,s.jsxs)(n.td,{children:["Enable resizer model inference; if the model input contains a resizer source, set to ",(0,s.jsx)(n.code,{children:"true"}),", default is ",(0,s.jsx)(n.code,{children:"false"}),"."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"roi"})}),(0,s.jsxs)(n.td,{children:["Effect when ",(0,s.jsx)(n.code,{children:"roi_infer"})," is set to ",(0,s.jsx)(n.code,{children:"true"}),", set the required ",(0,s.jsx)(n.code,{children:"roi"})," area for inference of resizer model, separated by semicolons."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"frame_count"})}),(0,s.jsxs)(n.td,{children:["Set the number of frames for ",(0,s.jsx)(n.code,{children:"perf"})," to run, effective when perf_time is 0, default is ",(0,s.jsx)(n.code,{children:"200"}),"."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"dump_intermediate"})}),(0,s.jsxs)(n.td,{children:["Dump the input and output data of each layer of the model, default is ",(0,s.jsx)(n.code,{children:"0"}),", no dumping. ",(0,s.jsx)(n.code,{children:"1"}),": output file type is ",(0,s.jsx)(n.code,{children:"bin"}),";"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{}),(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.code,{children:"2"}),": output type is ",(0,s.jsx)(n.code,{children:"bin"})," and ",(0,s.jsx)(n.code,{children:"txt"}),", with BPU node output being aligned data; ",(0,s.jsx)(n.code,{children:"3"}),": output type is ",(0,s.jsx)(n.code,{children:"bin"})," and ",(0,s.jsx)(n.code,{children:"txt"}),", with BPU node output being valid data."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"perf_time"})}),(0,s.jsxs)(n.td,{children:["Set the running time of ",(0,s.jsx)(n.code,{children:"perf"}),", unit: minutes, default is ",(0,s.jsx)(n.code,{children:"0"}),"."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"thread_num"})}),(0,s.jsxs)(n.td,{children:["Set the number of threads for program execution, range: [1, 8], default is ",(0,s.jsx)(n.code,{children:"1"}),", set to 8 or greater to process with 8 threads."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"profile_path"})}),(0,s.jsx)(n.td,{children:"Profiling tool log generation path, running generates profiler.log and profiler.csv, analyzing op time and scheduling time."})]})]})]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"multi-thread-latency-data-explanation",children:(0,s.jsx)(n.code,{children:"Multi-thread Latency Data Explanation"})}),"\n",(0,s.jsxs)(n.p,{children:["The purpose of multiple threads is to fully utilize BPU resources. Multiple threads process ",(0,s.jsx)(n.code,{children:"frame_count"})," frames or execute for perf_time until data processing is completed/execution time ends and the program terminates.\nIn the multi-thread ",(0,s.jsx)(n.code,{children:"perf"})," process, you can execute the following command to obtain the real-time occupancy of BPU resources."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"hrut_somstatus -n 10000 -d 1\n"})}),"\n",(0,s.jsx)(n.p,{children:"Output:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"=====================1=====================\ntemperature--\x3e\n        CPU      : 37.5 (C)\ncpu frequency--\x3e\n              min       cur     max\n        cpu0: 240000    1200000 1200000\n        cpu1: 240000    1200000 1200000\n        cpu2: 240000    1200000 1200000\n        cpu3: 240000    1200000 1200000\nbpu status information----\x3e\n          min        cur             max             ratio\n        bpu0: 400000000 1000000000      1000000000      0\n        bpu1: 400000000 1000000000      1000000000      0\n"})}),"\n",(0,s.jsx)(n.admonition,{title:"Remarks",type:"info",children:(0,s.jsxs)(n.p,{children:["The above example shows the output log of the ",(0,s.jsx)(n.strong,{children:"RDK X3"})," development board. If using the ",(0,s.jsx)(n.strong,{children:"RDK Ultra"}),' development board, simply use the above command to obtain the output.\nIn "perf" mode, the latency measurement of a single thread represents the actual on-board performance of the model, while the latency data of multiple threads represents the processing time of each thread per frame, which is longer than that of a single thread. However, the overall processing time of multiple threads is reduced, and the frame rate is improved.']})}),"\n",(0,s.jsx)(n.h4,{id:"supplementary-explanation-of-input-parameters-2",children:"Supplementary explanation of input parameters"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"profile_path"})}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Directory where the profile log file is generated.\nThis parameter can be set by setting the environment variable ",(0,s.jsx)(n.code,{children:"export HB_DNN_PROFILER_LOG_PATH=${path}"})," to view the OP and task scheduling time during model execution.\nGenerally, setting \"--profile_path='.'\" is sufficient, which means that the log file is generated in the current directory and the log file is named profiler.log."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"thread_num"})}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Number of threads (parallelism), indicating the maximum number of tasks processed in parallel.\nWhen testing latency, the value needs to be set to 1, so that there is no resource contention and the latency test is more accurate.\nWhen testing throughput, it is recommended to set it to a value greater than 2 (number of BPU cores), adjust the number of threads to maximize BPU utilization, and make the throughput test more accurate."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"  // Dual-core FPS\n  hrt_model_exec perf --model_file xxx.bin --thread_num 8 --core_id 0\n  // Latency\n  hrt_model_exec perf --model_file xxx.bin --thread_num 1 --core_id 1\n"})}),"\n",(0,s.jsx)(n.h3,{id:"instructions-for-using-the-hrt_bin_dump-toolhrt_bin_dump-is-a-layer-dump-tool-for-the-ptq-debug-model-and-the-output-file-of-the-tool-is-a-binary-file",children:'Instructions for using the hrt_bin_dump tool"hrt_bin_dump" is a layer dump tool for the PTQ debug model, and the output file of the tool is a binary file.'}),"\n",(0,s.jsx)(n.h4,{id:"description-of-input-parameters",children:"Description of Input Parameters"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"No."}),(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Description"}),(0,s.jsx)(n.th,{children:"Explanation"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"1"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"model_file"})}),(0,s.jsx)(n.td,{children:"string"}),(0,s.jsx)(n.td,{children:"Model file path."}),(0,s.jsxs)(n.td,{children:["It must be a debug model. The compilation parameter ",(0,s.jsx)(n.code,{children:"layer_out_dump"})," of the model needs to be set to ",(0,s.jsx)(n.code,{children:"True"}),", which specifies that the intermediate results of each layer should be output during the model conversion process."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"2"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"input_file"})}),(0,s.jsx)(n.td,{children:"string"}),(0,s.jsx)(n.td,{children:"Input file path."}),(0,s.jsxs)(n.td,{children:['The input file of the model supports all types of input of "hbDNNDataType"; The IMG type file must be a binary file (with a suffix of .bin), and the size of the binary file should match the input information of the model. For example, the size of the YUV444 file is :math:',(0,s.jsx)(n.code,{children:"height * width * 3"}),"; The TENSOR type file must be a binary file or a text file (with a suffix of .bin/.txt), The size of the binary file should match the input information of the model, and the number of data read from the text file must be greater than or equal to the number of input data required by the model. The excess data will be discarded; Each input is separated by a comma. For example, if the model has two inputs, it should be: ",(0,s.jsx)(n.code,{children:"--input_file=kite.bin,input.txt"}),"."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"3"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"conv_mapping_file"})}),(0,s.jsx)(n.td,{children:"string"}),(0,s.jsx)(n.td,{children:"Model convolution layer configuration file."}),(0,s.jsxs)(n.td,{children:["The model layer configuration file specifies the information of each layer in the model and is generated during the model compilation process. The file name is generally: ",(0,s.jsx)(n.code,{children:"model_name_quantized_model_conv_output_map.json"}),"."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"4"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"conv_dump_path"})}),(0,s.jsx)(n.td,{children:"string"}),(0,s.jsx)(n.td,{children:"Tool output path."}),(0,s.jsx)(n.td,{children:"The output path of the tool, which should be a valid path."})]})]})]}),"\n",(0,s.jsx)(n.h4,{id:"instructions-1",children:"Instructions"}),"\n",(0,s.jsxs)(n.p,{children:["The tool provides the function of dumping the output of convolutional layers, and the output file is a binary file.\nRun ",(0,s.jsx)(n.code,{children:"hrt_bin_dump"})," directly to get the details of tool usage.\nSee the figure below:"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/hrt_bin_dump_help.png",alt:"hrt_bin_dump_help"})}),"\n",(0,s.jsxs)(n.admonition,{title:"Tips",type:"tip",children:[(0,s.jsxs)(n.p,{children:['The tool can also view the version number of the "dnn" prediction library of the tool using the ',(0,s.jsx)(n.code,{children:"-v"})," or ",(0,s.jsx)(n.code,{children:"--version"})," command."]}),(0,s.jsx)(n.p,{children:"For example: hrt_bin_dump -v or hrt_bin_dump --version"})]}),"\n",(0,s.jsx)(n.h4,{id:"example",children:"Example"}),"\n",(0,s.jsx)(n.p,{children:'Taking the debug model of MobileNetV1 as an example, create the "outputs" folder and run the following command:'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"  ./hrt_bin_dump --model_file=./mobilenetv1_hybrid_horizonrt.bin --conv_mapping_file=./mobilenetv1_quantized_model_conv_output_map.json --conv_dump_path=./outputs --input_file=./zebra_cls.bin\n"})}),"\n",(0,s.jsx)(n.p,{children:"The run log is as follows:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/run_log.png",alt:"run_log"})}),"\n",(0,s.jsx)(n.p,{children:'The output can be viewed in the "outputs/" folder as shown below:'}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/output.png",alt:"output"})})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}}}]);