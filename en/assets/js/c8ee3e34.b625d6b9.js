"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[57533],{15471:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"Algorithm_Application/Python_Sample/Ultralytics_YOLO11_Pose","title":"Pose Estimation - Ultralytics YOLO11","description":"This example demonstrates how to run the Ultralytics YOLO11 pose estimation model on the BPU using hbmruntime, enabling human keypoint detection and visualization. It supports model preprocessing, inference execution, and post-processing (including keypoint decoding, bounding box drawing, and keypoint annotation). The example code is located in the /app/pydevdemo/04posesample/01ultralyticsyolo11_pose/ directory.","source":"@site/i18n/en/docusaurus-plugin-content-docs-docs_s/current/04_Algorithm_Application/02_Python_Sample/08_Ultralytics_YOLO11_Pose.md","sourceDirName":"04_Algorithm_Application/02_Python_Sample","slug":"/Algorithm_Application/Python_Sample/Ultralytics_YOLO11_Pose","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/Python_Sample/Ultralytics_YOLO11_Pose","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1764750792000,"sidebarPosition":8,"frontMatter":{"sidebar_position":8},"sidebar":"tutorialSidebar","previous":{"title":"Instance Segmentation - Ultralytics YOLO11","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/Python_Sample/Ultralytics_YOLO11_Seg"},"next":{"title":"Instance Segmentation - Ultralytics YOLOE11","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/Python_Sample/Ultralytics_YOLOE11_Seg"}}');var t=i(74848),o=i(28453);const r={sidebar_position:8},l="Pose Estimation - Ultralytics YOLO11",d={},c=[{value:"Model Description",id:"model-description",level:2},{value:"Functionality Description",id:"functionality-description",level:2},{value:"Environment Dependencies",id:"environment-dependencies",level:2},{value:"Directory Structure",id:"directory-structure",level:2},{value:"Parameter Description",id:"parameter-description",level:2},{value:"Quick Start",id:"quick-start",level:2},{value:"Notes",id:"notes",level:2},{value:"License",id:"license",level:2}];function a(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"pose-estimation---ultralytics-yolo11",children:"Pose Estimation - Ultralytics YOLO11"})}),"\n",(0,t.jsxs)(n.p,{children:["This example demonstrates how to run the Ultralytics YOLO11 pose estimation model on the BPU using ",(0,t.jsx)(n.code,{children:"hbm_runtime"}),", enabling human keypoint detection and visualization. It supports model preprocessing, inference execution, and post-processing (including keypoint decoding, bounding box drawing, and keypoint annotation). The example code is located in the ",(0,t.jsx)(n.code,{children:"/app/pydev_demo/04_pose_sample/01_ultralytics_yolo11_pose/"})," directory."]}),"\n",(0,t.jsx)(n.h2,{id:"model-description",children:"Model Description"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Introduction:"}),"\n",(0,t.jsx)(n.p,{children:"Ultralytics YOLO11 Pose is an efficient and lightweight human keypoint detection model capable of performing object detection and pose estimation (multi-keypoint prediction) simultaneously. It integrates Distribution Focal Loss (DFL) to enhance the localization accuracy of bounding boxes and keypoints, making it suitable for multi-person pose recognition tasks in real-time scenarios."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"HBM Model Name: yolo11n_pose_nashe_640x640_nv12.hbm"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Input Format: NV12 format image (separated Y and UV planes), resolution 640\xd7640"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Output:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Bounding box coordinates (xyxy) for each person"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Keypoint locations (K\xd72, x/y coordinates)"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Confidence scores for each keypoint"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Supports COCO human keypoint format (typically 17 keypoints)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Model Download URL (automatically downloaded by the program):"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"https://archive.d-robotics.cc/downloads/rdk_model_zoo/rdk_s100/ultralytics_YOLO/yolo11n_pose_nashe_640x640_nv12.hbm\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"functionality-description",children:"Functionality Description"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Model Loading"}),"\n",(0,t.jsxs)(n.p,{children:["Uses ",(0,t.jsx)(n.code,{children:"hbm_runtime"})," to load the specified Ultralytics YOLO11 pose estimation model and automatically parses the model's input/output tensor names, shapes, and quantization parameters."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Input Preprocessing"}),"\n",(0,t.jsx)(n.p,{children:"Resizes the input BGR image to 640\xd7640 and converts it to NV12 format (separated Y and UV planes) for model inference."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Inference Execution"}),"\n",(0,t.jsxs)(n.p,{children:["Calls the ",(0,t.jsx)(n.code,{children:".run()"})," interface to perform inference, supporting configuration of scheduling priority and BPU core binding via ",(0,t.jsx)(n.code,{children:"set_scheduling_params()"}),"."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Result Post-processing"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Decodes bounding boxes from multi-scale outputs (using DFL binning decoding);"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Decodes keypoint locations and confidence scores (K\xd72 + K);"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Applies Non-Maximum Suppression (NMS) to remove redundant detection boxes;"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Maps keypoint coordinates and bounding boxes back to the original image dimensions;"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Provides threshold control to display only high-confidence keypoints;"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Supports image visualization, including drawing detection boxes and keypoints."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"environment-dependencies",children:"Environment Dependencies"}),"\n",(0,t.jsx)(n.p,{children:"This example has no special environment requirements; ensure that the dependencies in pydev are installed:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip install -r ../../requirements.txt\n"})}),"\n",(0,t.jsx)(n.h2,{id:"directory-structure",children:"Directory Structure"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:".\n\u251c\u2500\u2500 ultralytics_yolo11_pose.py    # Main inference script\n\u2514\u2500\u2500 README.md                     # Usage instructions\n"})}),"\n",(0,t.jsx)(n.h2,{id:"parameter-description",children:"Parameter Description"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Parameter"}),(0,t.jsx)(n.th,{children:"Description"}),(0,t.jsx)(n.th,{children:"Default Value"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"--model-path"})}),(0,t.jsxs)(n.td,{children:["Path to the model file (",(0,t.jsx)(n.code,{children:".hbm"})," format)"]}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"/opt/hobot/model/s100/basic/yolo11n_pose_nashe_640x640_nv12.hbm"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"--test-img"})}),(0,t.jsx)(n.td,{children:"Path to the test image"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"/app/res/assets/bus.jpg"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"--label-file"})}),(0,t.jsx)(n.td,{children:"Path to the class label file (one class name per line)"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"/app/res/labels/coco_classes.names"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"--img-save-path"})}),(0,t.jsx)(n.td,{children:"Path to save the detection result"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"result.jpg"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"--priority"})}),(0,t.jsx)(n.td,{children:"Model scheduling priority (0\u2013255; higher value = higher priority)"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"0"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"--bpu-cores"})}),(0,t.jsxs)(n.td,{children:["List of BPU core IDs to use for inference (e.g., ",(0,t.jsx)(n.code,{children:"--bpu-cores 0 1"}),")"]}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"[0]"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"--nms-thres"})}),(0,t.jsx)(n.td,{children:"IoU threshold for Non-Maximum Suppression (NMS)"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"0.7"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"--score-thres"})}),(0,t.jsx)(n.td,{children:"Object confidence threshold (objects below this are filtered out)"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"0.25"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"--kpt-conf-thres"})}),(0,t.jsx)(n.td,{children:"Keypoint visualization confidence threshold (keypoints below this are not displayed)"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"0.5"})})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Run the model"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["With default parameters:","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"python ultralytics_yolo11_pose.py\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["With custom parameters:","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"python ultralytics_yolo11_pose.py \\\n--model-path /opt/hobot/model/s100/basic/yolo11n_pose_nashe_640x640_nv12.hbm \\\n--test-img /app/res/assets/bus.jpg \\\n--label-file /app/res/labels/coco_classes.names \\\n--img-save-path result.jpg \\\n--priority 0 \\\n--bpu-cores 0 \\\n--score-thres 0.25 \\\n--nms-thres 0.7 \\\n--kpt-conf-thres 0.5\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"View results"}),"\n",(0,t.jsxs)(n.p,{children:["Upon successful execution, the results will be overlaid on the original image and saved to the path specified by ",(0,t.jsx)(n.code,{children:"--img-save-path"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"[Saved] Result saved to: result.jpg\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"notes",children:"Notes"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"If the specified model path does not exist, the program will attempt to download the model automatically."}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"license",children:"License"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-license",children:"Copyright (C) 2025, XiangshunZhao D-Robotics.\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Affero General Public License as\npublished by the Free Software Foundation, either version 3 of the\nLicense, or (at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Affero General Public License for more details.\n\nYou should have received a copy of the GNU Affero General Public License\nalong with this program.  If not, see <https://www.gnu.org/licenses/>.\n"})})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(a,{...e})}):a(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>l});var s=i(96540);const t={},o=s.createContext(t);function r(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);