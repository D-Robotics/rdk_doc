"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[44099],{28453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>o});var s=i(96540);const r={},t=s.createContext(r);function l(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),s.createElement(t.Provider,{value:n},e.children)}},94459:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"Algorithm_Application/C++_Sample/ASR","title":"Automatic Speech Recognition - ASR","description":"This example runs a speech recognition model based on the BPU inference engine to automatically transcribe .wav audio files into corresponding text. The example code is located in the /app/cdevdemo/bpu/07speechsample/01asr/ directory.","source":"@site/i18n/en/docusaurus-plugin-content-docs-docs_s/current/04_Algorithm_Application/03_C++_Sample/11_ASR.md","sourceDirName":"04_Algorithm_Application/03_C++_Sample","slug":"/Algorithm_Application/C++_Sample/ASR","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/C++_Sample/ASR","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1764753839000,"sidebarPosition":11,"frontMatter":{"sidebar_position":11},"sidebar":"tutorialSidebar","previous":{"title":"Lane Detection - LaneNet","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/C++_Sample/LaneNet"},"next":{"title":"Text Detection and Recognition - PaddleOCR","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/C++_Sample/PaddleOCR"}}');var r=i(74848),t=i(28453);const l={sidebar_position:11},o="Automatic Speech Recognition - ASR",d={},c=[{value:"Model Description",id:"model-description",level:2},{value:"Functionality Overview",id:"functionality-overview",level:2},{value:"Environment Dependencies",id:"environment-dependencies",level:2},{value:"Directory Structure",id:"directory-structure",level:2},{value:"Build Instructions",id:"build-instructions",level:2},{value:"Model Download",id:"model-download",level:2},{value:"Parameter Description",id:"parameter-description",level:2},{value:"Quick Start",id:"quick-start",level:2},{value:"Notes",id:"notes",level:2}];function a(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"automatic-speech-recognition---asr",children:"Automatic Speech Recognition - ASR"})}),"\n",(0,r.jsxs)(n.p,{children:["This example runs a speech recognition model based on the BPU inference engine to automatically transcribe ",(0,r.jsx)(n.code,{children:".wav"})," audio files into corresponding text. The example code is located in the ",(0,r.jsx)(n.code,{children:"/app/cdev_demo/bpu/07_speech_sample/01_asr/"})," directory."]}),"\n",(0,r.jsx)(n.h2,{id:"model-description",children:"Model Description"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Introduction"}),":"]}),"\n",(0,r.jsxs)(n.p,{children:["The ASR (Automatic Speech Recognition) model converts audio signals into text. The input is a single-channel audio waveform (after sample rate conversion and normalization), and the output is a character-level token sequence. When used together with a vocabulary (",(0,r.jsx)(n.code,{children:"vocab"}),") file, it supports Chinese speech transcription. This example uses a quantized ",(0,r.jsx)(n.code,{children:".hbm"})," model."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"HBM Model Name"}),": asr.hbm"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Input Format"}),": Audio waveform, single-channel, sampled at 16kHz, with a maximum length of 30,000 samples."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Output"}),": Probability distribution (logits) over character tokens; after argmax decoding, mapped to recognized text."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"functionality-overview",children:"Functionality Overview"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Model Loading"})}),"\n",(0,r.jsx)(n.p,{children:"Loads the ASR model and automatically parses its input/output shapes and quantization information."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Input Preprocessing"})}),"\n",(0,r.jsxs)(n.p,{children:["Reads audio using SoundFile (supports ",(0,r.jsx)(n.code,{children:".wav"}),") and performs the following steps:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Converts to single-channel"}),"\n",(0,r.jsx)(n.li,{children:"Resamples to target sample rate (default: 16kHz)"}),"\n",(0,r.jsx)(n.li,{children:"Normalizes to zero-mean and unit-variance (z-score)"}),"\n",(0,r.jsx)(n.li,{children:"Pads or truncates to a fixed length (e.g., 30,000 samples)"}),"\n",(0,r.jsx)(n.li,{children:"Supports generator-based processing for long audio, enabling streaming recognition."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Inference Execution"})}),"\n",(0,r.jsxs)(n.p,{children:["Performs inference using the ",(0,r.jsx)(n.code,{children:".infer()"})," method."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Post-processing"})}),"\n",(0,r.jsxs)(n.p,{children:["Extracts token indices from output logits and maps them to characters using the ",(0,r.jsx)(n.code,{children:"vocab"})," dictionary file (in JSON format), producing the final transcribed text."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"environment-dependencies",children:"Environment Dependencies"}),"\n",(0,r.jsx)(n.p,{children:"Before compiling and running, ensure the following dependencies are installed:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"sudo apt update\nsudo apt install -y libgflags-dev libsndfile1-dev libsamplerate0-dev\n"})}),"\n",(0,r.jsx)(n.h2,{id:"directory-structure",children:"Directory Structure"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-text",children:".\n|-- CMakeLists.txt                  # CMake build script: target/dependency/include/link configuration\n|-- README.md                       # Usage instructions (this file)\n|-- inc\n|   |-- asr.hpp                     # ASR inference wrapper header (interfaces for loading/preprocessing/inference/post-processing)\n|   `-- audio_chunk_reader.hpp      # Audio chunk reader: reads file \u2192 resamples \u2192 outputs chunks\n`-- src\n    |-- asr.cc                      # ASR inference implementation: input writing, forward computation, CTC decoding, etc.\n    |-- audio_chunk_reader.cc       # Chunk reading implementation: libsndfile + libsamplerate for streaming chunking\n    `-- main.cc                     # Program entry point: argument parsing \u2192 loop over chunks \u2192 inference \u2192 concatenate transcribed text\n"})}),"\n",(0,r.jsx)(n.h2,{id:"build-instructions",children:"Build Instructions"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Configuration and Compilation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"mkdir build && cd build\ncmake ..\nmake -j$(nproc)\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"model-download",children:"Model Download"}),"\n",(0,r.jsx)(n.p,{children:"If the model is not found during runtime, download it using the following command:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"wget https://archive.d-robotics.cc/downloads/rdk_model_zoo/rdk_s100/asr/asr.hbm\n"})}),"\n",(0,r.jsx)(n.h2,{id:"parameter-description",children:"Parameter Description"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Parameter"}),(0,r.jsx)(n.th,{children:"Description"}),(0,r.jsx)(n.th,{children:"Default Value"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"--model_path"})}),(0,r.jsxs)(n.td,{children:["Path to the model file (",(0,r.jsx)(n.code,{children:".hbm"}),")"]}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"/opt/hobot/model/s100/basic/asr.hbm"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"--test_sound"})}),(0,r.jsxs)(n.td,{children:["Path to the input audio file (",(0,r.jsx)(n.code,{children:".wav"}),")"]}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"/app/res/assets/chi_sound.wav"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"--vocab_file"})}),(0,r.jsxs)(n.td,{children:["Vocabulary file (JSON), mapping ",(0,r.jsx)(n.strong,{children:"class id \u2192 token"})]}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"/app/res/labels/vocab.json"})})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Run the Model"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Ensure you are in the ",(0,r.jsx)(n.code,{children:"build"})," directory."]}),"\n",(0,r.jsxs)(n.li,{children:["Run with default parameters:","\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"./asr\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Run with specified parameters:","\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"./asr \\\n    --model_path /opt/hobot/model/s100/basic/asr.hbm \\\n    --test_sound /app/res/assets/chi_sound.wav \\\n    --vocab_file /app/res/labels/vocab.json\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"View Results"})}),"\n",(0,r.jsx)(n.p,{children:"Upon successful execution, the result will be printed:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"I am Qwen, a large-scale language model developed by Alibaba Cloud.||\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"notes",children:"Notes"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"For more information about deployment options or model support, please refer to the official documentation or contact platform technical support."}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(a,{...e})}):a(e)}}}]);