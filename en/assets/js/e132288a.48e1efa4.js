"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[10901],{28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>d});var o=t(96540);const i={},s=o.createContext(i);function r(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),o.createElement(s.Provider,{value:n},e.children)}},77774:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>d,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"Robot_development/tros_dev/ai_predict","title":"5.5.2 Model Inference","description":"Model Inference Development","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/05_Robot_development/05_tros_dev/ai_predict.md","sourceDirName":"05_Robot_development/05_tros_dev","slug":"/Robot_development/tros_dev/ai_predict","permalink":"/rdk_doc/en/Robot_development/tros_dev/ai_predict","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1761281915000,"sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"5.5.1 Using \\"zero-copy\\"","permalink":"/rdk_doc/en/Robot_development/tros_dev/zero_copy"},"next":{"title":"5.5.3 Using Breakpad","permalink":"/rdk_doc/en/Robot_development/tros_dev/breakpad"}}');var i=t(74848),s=t(28453);const r={sidebar_position:2},d="5.5.2 Model Inference",a={},c=[{value:"Model Inference Development",id:"model-inference-development",level:2},{value:"Background",id:"background",level:3},{value:"Preparation",id:"preparation",level:3},{value:"Usage",id:"usage",level:3},{value:"1. Create package",id:"1-create-package",level:4},{value:"2. Write exampleThe created project path can be viewed as follows:",id:"2-write-examplethe-created-project-path-can-be-viewed-as-follows",level:4},{value:"2.1 Node Design",id:"21-node-design",level:5},{value:"2.2 Code Explanation",id:"22-code-explanation",level:5},{value:"2.3 Compilation Dependencies",id:"23-compilation-dependencies",level:5},{value:"2.4 Compilation Script",id:"24-compilation-script",level:5},{value:"3 Build and Run",id:"3-build-and-run",level:4},{value:"3.1 Build",id:"31-build",level:5},{value:"3.2 Common Compilation Errors",id:"32-common-compilation-errors",level:5},{value:"3.3 Running",id:"33-running",level:5},{value:"3.4 Common Errors",id:"34-common-errors",level:5},{value:"3.5 Results",id:"35-results",level:5},{value:"Algorithm Workflow Construction",id:"algorithm-workflow-construction",level:2},{value:"Background",id:"background-1",level:3},{value:"Preparation",id:"preparation-1",level:3},{value:"Usage",id:"usage-1",level:3},{value:"1. Start Data Collection Node",id:"1-start-data-collection-node",level:4},{value:"2. Start Human Bounding Box Detection Algorithm Node",id:"2-start-human-bounding-box-detection-algorithm-node",level:4},{value:"3 Start the Node for Hand Keypoint Detection Algorithm",id:"3-start-the-node-for-hand-keypoint-detection-algorithm",level:4},{value:"4 View the Output of the Algorithm Inference",id:"4-view-the-output-of-the-algorithm-inference",level:4},{value:"Summary",id:"summary",level:3}];function l(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"552-model-inference",children:"5.5.2 Model Inference"})}),"\n",(0,i.jsx)(n.h2,{id:"model-inference-development",children:"Model Inference Development"}),"\n",(0,i.jsx)(n.h3,{id:"background",children:"Background"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"hobot_dnn"})," is the edge algorithm inference framework in TogetheROS.Bot software stack. It utilizes the BPU processor on RDK to achieve algorithm inference. Based on the D-Robotics algorithm inference framework and ROS2 Node, it provides a simpler and easier-to-use model integration development interface for robot application development. This includes model management, model description-based input processing and result parsing, and model output memory allocation management."]}),"\n",(0,i.jsxs)(n.p,{children:["By reading this chapter, users can use the models provided by D-Robotics to create and run a human detection algorithm Node based on ",(0,i.jsx)(n.code,{children:"hobot_dnn"})," on RDK. With the help of the components provided by tros.b, it subscribes to the images captured and published by the camera, performs algorithm inference to detect human bounding boxes, and uses the Multi-Object Tracking (MOT) algorithm to track and assign target numbers to the detection boxes. Finally, it achieves real-time rendering and display of images, human bounding box detection, and target tracking results in a web browser on a PC."]}),"\n",(0,i.jsx)(n.h3,{id:"preparation",children:"Preparation"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"RDK development board with relevant software installed, including:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Ubuntu 20.04/22.04 system image provided by D-Robotics."}),"\n",(0,i.jsx)(n.li,{children:"tros.b software package."}),"\n",(0,i.jsxs)(n.li,{children:["Development tools: Compilers and other tools to build ROS packages. ",(0,i.jsx)(n.code,{children:"sudo apt install ros-dev-tools"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"RDK with F37 or GC4663 camera installed."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"PC that can access RDK through the network."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["For detailed instructions on how to use ",(0,i.jsx)(n.code,{children:"hobot_dnn"}),", please refer to the ",(0,i.jsx)(n.a,{href:"https://github.com/D-Robotics/hobot_dnn/blob/develop/README.md",children:"README.md"})," and ",(0,i.jsx)(n.a,{href:"https://github.com/D-Robotics/hobot_dnn/blob/develop/dnn_node/docs/API-Manual/API-Manual.md",children:"API documentation"})," in the ",(0,i.jsx)(n.code,{children:"hobot_dnn"})," code. The workflow of using ",(0,i.jsx)(n.code,{children:"hobot_dnn"})," is as follows:"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/05_tros_dev/image/ai_predict/dnnnode_workflow.jpg",alt:""})}),"\n",(0,i.jsxs)(n.p,{children:["Without understanding the workflow of using ",(0,i.jsx)(n.code,{children:"hobot_dnn"}),", users can also follow the steps in this chapter to develop a model inference example using ",(0,i.jsx)(n.code,{children:"hobot_dnn"}),"."]}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["The following content in this chapter uses the tros.b Foxy version as an example. If you are using the tros.b Humble version, just replace the ",(0,i.jsx)(n.code,{children:"source /opt/tros/setup.bash"})," command with ",(0,i.jsx)(n.code,{children:"source /opt/tros/humble/ setup.bash"}),"."]})}),"\n",(0,i.jsx)(n.h3,{id:"usage",children:"Usage"}),"\n",(0,i.jsx)(n.h4,{id:"1-create-package",children:"1. Create package"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"mkdir -p ~/dev_ws/src\ncd ~/dev_ws/src\nsource /opt/tros/setup.bash\nros2 pkg create --build-type ament_cmake cpp_dnn_demo --dependencies rclcpp sensor_msgs hbm_img_msgs ai_msgs dnn_node hobot_mot\ncd cpp_dnn_demo\ntouch src/body_det_demo.cpp\n"})}),"\n",(0,i.jsx)(n.h4,{id:"2-write-examplethe-created-project-path-can-be-viewed-as-follows",children:"2. Write exampleThe created project path can be viewed as follows:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"root@ubuntu:~# cd ~\nroot@ubuntu:~# tree dev_ws/\ndev_ws/\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 cpp_dnn_demo\n        \u251c\u2500\u2500 CMakeLists.txt\n        \u251c\u2500\u2500 include\n        \u2502   \u2514\u2500\u2500 cpp_dnn_demo\n        \u251c\u2500\u2500 package.xml\n        \u2514\u2500\u2500 src\n            \u2514\u2500\u2500 body_det_demo.cpp\n\n5 directories, 3 files\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Open the created source code file ",(0,i.jsx)(n.code,{children:"body_det_demo.cpp"})," using tools like vi/vim on the RDK: ",(0,i.jsx)(n.code,{children:"vi ~/dev_ws/src/cpp_dnn_demo/src/body_det_demo.cpp"})]}),"\n",(0,i.jsx)(n.p,{children:"Copy the following code into the file:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c++",children:'#include "dnn_node/dnn_node.h"\n#include "dnn_node/util/image_proc.h"\n#include "dnn_node/util/output_parser/detection/fasterrcnn_output_parser.h"\n#include "sensor_msgs/msg/image.hpp"\n#include "ai_msgs/msg/perception_targets.hpp"\n#include "hbm_img_msgs/msg/hbm_msg1080_p.hpp"\n#include "hobot_mot/hobot_mot.h"\n\n// Create an algorithm inference output data structure and add message header information as a member\nstruct FasterRcnnOutput : public hobot::dnn_node::DnnNodeOutput {\n  std::shared_ptr<std_msgs::msg::Header> image_msg_header = nullptr;\n};\n\n// Inherit from the DnnNode base class and create an algorithm inference node\nclass BodyDetNode : public hobot::dnn_node::DnnNode {\n public:\n  BodyDetNode(const std::string &node_name = "body_det",\n  const rclcpp::NodeOptions &options = rclcpp::NodeOptions());\n\n protected:\n  // Implement the pure virtual interface of the base class to configure Node parameters\n  int SetNodePara() override;\n  // Implement the virtual interface of the base class to encapsulate the parsed model output data into ROS Msg and publish it\n  int PostProcess(const std::shared_ptr<hobot::dnn_node::DnnNodeOutput> &node_output)\n    override;\n```private:\n  // Width and height of the input image data for the algorithm model\n  int model_input_width_ = -1;\n  int model_input_height_ = -1;\n  // Model output index corresponding to the body detection box result\n  const int32_t box_output_index_ = 1;\n  // Collection of detection box output indexes\n  const std::vector<int32_t> box_outputs_index_ = {box_output_index_};\n\n  // Image message subscriber\n  rclcpp::Subscription<hbm_img_msgs::msg::HbmMsg1080P>::ConstSharedPtr\n      ros_img_subscription_ = nullptr;\n  // Algorithm inference result message publisher\n  rclcpp::Publisher<ai_msgs::msg::PerceptionTargets>::SharedPtr\n      msg_publisher_ = nullptr;\n  // Multi-target tracking algorithm engine\n  std::shared_ptr<HobotMot> hobot_mot_ = nullptr;\n\n  // Image message subscription callback\n  void FeedImg(const hbm_img_msgs::msg::HbmMsg1080P::ConstSharedPtr msg);\n};\n\nBodyDetNode::BodyDetNode(const std::string & node_name, const rclcpp::NodeOptions & options) :\n  hobot::dnn_node::DnnNode(node_name, options) {\n  // Initialize the algorithm inference using the SetNodePara() method implemented in the BodyDetNode subclass\n  if (Init() != 0 ||\n    GetModelInputSize(0, model_input_width_, model_input_height_) < 0) {\n    RCLCPP_ERROR(rclcpp::get_logger("dnn_demo"), "Node init fail!");\n    rclcpp::shutdown();\n  }\n\n  // Create a message subscriber to subscribe image messages from the camera node\n  ros_img_subscription_ =\n          this->create_subscription<hbm_img_msgs::msg::HbmMsg1080P>(\n          "/hbmem_img", 10, std::bind(&BodyDetNode::FeedImg, this, std::placeholders::_1));\n  // Create a message publisher to publish algorithm inference messages\n  msg_publisher_ = this->create_publisher<ai_msgs::msg::PerceptionTargets>(\n      "/cpp_dnn_demo", 10);\n  // Create a multi-target tracking (MOT) algorithm engine\n  hobot_mot_ = std::make_shared<HobotMot>("config/iou2_method_param.json");\n}\n\nint BodyDetNode::SetNodePara() {\n  if (!dnn_node_para_ptr_) return -1;\n  // Specify the model file path and model name to be used for the algorithm inference\n  dnn_node_para_ptr_->model_file = "config/multitask_body_kps_960x544.hbm";\n  dnn_node_para_ptr_->model_name = "multitask_body_kps_960x544";\n  return 0;\n}void BodyDetNode::FeedImg(const hbm_img_msgs::msg::HbmMsg1080P::ConstSharedPtr img_msg) {\n  if (!rclcpp::ok()) {\n    return;\n  }\n\n  // Validate the subscribed image message, this example only supports processing NV12 format image data\n  if (!img_msg) return;\n  if ("nv12" != std::string(reinterpret_cast<const char*>(img_msg->encoding.data()))) {\n    RCLCPP_ERROR(rclcpp::get_logger("dnn_demo"), "Only support nv12 img encoding!");\n    return;\n  }\n\n  // Create model input data based on the model input resolution using the methods provided by DnnNode\n  auto inputs = std::vector<std::shared_ptr<hobot::dnn_node::DNNInput>>{\n    hobot::dnn_node::ImageProc::GetNV12PyramidFromNV12Img(\n      reinterpret_cast<const char*>(img_msg->data.data()),\n      img_msg->height, img_msg->width, model_input_height_, model_input_width_)};\n  \n  // Create model output data and fill in message header information\n  auto dnn_output = std::make_shared<FasterRcnnOutput>();\n  dnn_output->image_msg_header = std::make_shared<std_msgs::msg::Header>();\n  dnn_output->image_msg_header->set__frame_id(std::to_string(img_msg->index));\n  dnn_output->image_msg_header->set__stamp(img_msg->time_stamp);\n\n  // Run inference asynchronously\n  Run(inputs, dnn_output, nullptr, false);\n}\n\nint BodyDetNode::PostProcess(const std::shared_ptr<hobot::dnn_node::DnnNodeOutput> &node_output) {\n  if (!rclcpp::ok()) {\n    return 0;\n  }\n  \n  // Validate the output data\n  if (node_output->output_tensors.empty() ||\n    static_cast<int32_t>(node_output->output_tensors.size()) < box_output_index_) {\n    RCLCPP_ERROR(rclcpp::get_logger("dnn_demo"), "Invalid outputs");\n    return -1;\n  }\n\n  // Create parsing output data\n  // Dimension of detection box results is equal to the number of detected object categories\n  std::vector<std::shared_ptr<hobot::dnn_node::parser_fasterrcnn::Filter2DResult>>\n      results;\n  // Keypoint data\n  std::shared_ptr<hobot::dnn_node::parser_fasterrcnn::LandmarksResult> output_body_kps = nullptr;\n\n  // Use the built-in Parse method in hobot dnn to parse the algorithm output\n  if (hobot::dnn_node::parser_fasterrcnn::Parse(node_output, nullptr,\n  box_outputs_index_, -1, -1, results, output_body_kps) < 0) {\nRCLCPP_ERROR(rclcpp::get_logger("dnn_node_sample"), "Parse node_output fail!");\n    return -1;\n  }\n\n  auto filter2d_result = results.at(box_output_index_);\n  if (!filter2d_result) return -1;\n\n  // Convert the human detection boxes inferred by the algorithm into the MOT algorithm input data type\n  std::vector<MotBox> in_box_list;\n  for (auto& rect : filter2d_result->boxes) {\n    in_box_list.emplace_back(\n        MotBox(rect.left, rect.top, rect.right, rect.bottom, rect.conf));\n  }\n  \n  // Calculate the timestamp of the current frame based on the message header\n  auto fasterRcnn_output =\n      std::dynamic_pointer_cast<FasterRcnnOutput>(node_output);\n  time_t time_stamp =\n      fasterRcnn_output->image_msg_header->stamp.sec * 1000 +\n      fasterRcnn_output->image_msg_header->stamp.nanosec / 1000 / 1000;\n  \n  // Create the output of the MOT algorithm: human detection boxes with target IDs and disappeared target IDs\n  std::vector<MotBox> out_box_list;\n  std::vector<std::shared_ptr<MotTrackId>> disappeared_ids;\n\n  // Run the multi-object tracking algorithm\n  if (hobot_mot_->DoProcess(in_box_list,\n                            out_box_list,\n                            disappeared_ids,\n                            time_stamp,\n                            model_input_width_,\n                            model_input_height_) < 0) {\n    RCLCPP_ERROR(rclcpp::get_logger("dnn_demo"), "Do mot fail");\n    return -1;\n  }\n\n  // Create an ROS Msg for publishing the inference results\n  ai_msgs::msg::PerceptionTargets::UniquePtr pub_data(\n      new ai_msgs::msg::PerceptionTargets());\n\n  // Fill in the message header for the ROS Msg\n  pub_data->header.set__stamp(fasterRcnn_output->image_msg_header->stamp);\n  pub_data->header.set__frame_id(fasterRcnn_output->image_msg_header->frame_id);\n\n  // Fill in the algorithm inference output frame rate to the ROS Msg\n  if (node_output->rt_stat) {\n    pub_data->set__fps(round(node_output->rt_stat->output_fps));\n    // If the algorithm inference statistics are updated, output the frame rate statistics of the algorithm model input and output\n    if (node_output->rt_stat->fps_updated) {RCLCPP_WARN(rclcpp::get_logger("dnn_demo"),\n                  "input fps: %.2f, out fps: %.2f",\n                  node_output->rt_stat->input_fps,\n                  node_output->rt_stat->output_fps);\n    }\n  }\n\n  for (auto& rect : out_box_list) {\n    // Validate the effectiveness of the target tracking result\n    if (rect.id < 0) {\n      continue;\n    }\n    // Fill the target tracking result and detection box into ROS Msg\n    ai_msgs::msg::Target target;\n    target.set__type("person");\n    target.set__track_id(rect.id);\n    ai_msgs::msg::Roi roi;\n    roi.type = "body";\n    roi.rect.set__x_offset(rect.x1);\n    roi.rect.set__y_offset(rect.y1);\n    roi.rect.set__width(rect.x2 - rect.x1);\n    roi.rect.set__height(rect.y2 - rect.y1);\n    target.rois.emplace_back(roi);\n    pub_data->targets.emplace_back(std::move(target));\n  }\n\n  // Fill the disappeared targets into ROS Msg\n  for (const auto& id_info : disappeared_ids) {\n    if (id_info->value < 0 ||\n        hobot_mot::DataState::INVALID == id_info->state_) {\n      continue;\n    }\n    ai_msgs::msg::Target target;\n    target.set__type("person");\n    target.set__track_id(id_info->value);\n    ai_msgs::msg::Roi roi;\n    roi.type = "body";\n    target.rois.emplace_back(roi);\n    pub_data->disappeared_targets.emplace_back(std::move(target));\n  }\n\n  // Publish ROS Msg\n  msg_publisher_->publish(std::move(pub_data));\n\n  return 0;\n}\n\nint main(int argc, char** argv) {\n  rclcpp::init(argc, argv);\n  rclcpp::spin(std::make_shared<BodyDetNode>());```cpp\n#include "dnn_node/dnn_node.h"\n#include "dnn_node/util/image_proc.h"\n#include "dnn_node/util/output_parser/detection/fasterrcnn_output_parser.h"\n#include "sensor_msgs/msg/image.hpp"\n#include "ai_msgs/msg/perception_targets.hpp"\n#include "hbm_img_msgs/msg/hbm_msg1080_p.hpp"\n#include "hobot_mot/hobot_mot.h"\n\nrclcpp::shutdown();\nreturn 0;\n}\n\n'})}),"\n",(0,i.jsx)(n.h5,{id:"21-node-design",children:"2.1 Node Design"}),"\n",(0,i.jsx)(n.p,{children:"The main body detection algorithm node in the example consists of three logically independent functionalities."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"(1) Node Initialization and Startup"})}),"\n",(0,i.jsx)(n.p,{children:"Configure the model information used by the algorithm, create the publisher for algorithm inference messages and the subscriber for image messages, and start the target tracking algorithm engine."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"(2) Message Subscription and Algorithm Inference"})}),"\n",(0,i.jsxs)(n.p,{children:["When creating the subscriber for image messages, a message callback ",(0,i.jsx)(n.code,{children:"FeedImg"})," is registered to process the image data for algorithm model inference. The callback does not wait for the algorithm inference to complete."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"(3) Processing and Publishing of Inference Results"})}),"\n",(0,i.jsxs)(n.p,{children:["After the algorithm inference is completed, the inference results are output through the registered callback ",(0,i.jsx)(n.code,{children:"PostProcess"}),". In the callback, the detection results are processed using the multi-object tracking algorithm (",(0,i.jsx)(n.code,{children:"HobotMot"}),"), and the algorithm inference result messages are published."]}),"\n",(0,i.jsx)(n.p,{children:"The design and flow logic of the Node are shown in the following figure:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/05_tros_dev/image/ai_predict/node_architecture.jpg",alt:""})}),"\n",(0,i.jsx)(n.h5,{id:"22-code-explanation",children:"2.2 Code Explanation"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Add Header Files"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"dnn_node/dnn_node.h"}),": Header file for the inference framework, used for algorithm model management and inference."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"dnn_node/util/image_proc.h"}),": Header file for algorithm model input image processing."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"dnn_node/util/output_parser/detection/fasterrcnn_output_parser.h"}),": Header file for algorithm model output parsing method, used to parse structured data (in this example, it corresponds to the human detection bounding boxes) from the output address after model inference."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"ROS Msg header files: Used for message subscription and publishing."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"hobot_mot/hobot_mot.h"}),": Header file for the MOT (Multi-Object Tracking) algorithm engine, used for tracking the detected human bounding boxes."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-cpp",children:'#include "dnn_node/dnn_node.h"\n#include "dnn_node/util/image_proc.h"\n#include "dnn_node/util/output_parser/detection/fasterrcnn_output_parser.h"\n#include "sensor_msgs/msg/image.hpp"\n#include "ai_msgs/msg/perception_targets.hpp"\n#include "hbm_img_msgs/msg/hbm_msg1080_p.hpp"\n#include "hobot_mot/hobot_mot.h"\n```**Creating algorithm inference output data structure**\n\nInheriting the `DnnNodeOutput` base class from `hobot_dnn`, adding a message header information member to represent the image information corresponding to the inference output.\n\n```C++\nstruct FasterRcnnOutput : public hobot::dnn_node::DnnNodeOutput {\n  std::shared_ptr<std_msgs::msg::Header> image_msg_header = nullptr;\n};\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Creating algorithm inference Node"})}),"\n",(0,i.jsxs)(n.p,{children:["Inheriting the virtual base class ",(0,i.jsx)(n.code,{children:"DnnNode"})," from ",(0,i.jsx)(n.code,{children:"hobot_dnn"}),", defining the algorithm inference node ",(0,i.jsx)(n.code,{children:"BodyDetNode"})," and implementing the virtual interfaces defined in ",(0,i.jsx)(n.code,{children:"DnnNode"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"int SetNodePara()"}),": Configuring model parameters."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"int PostProcess(const std::shared_ptr<hobot::dnn_node::DnnNodeOutput>& node_output)"}),": Callback for inference results, packaging the parsed and structured model output data into ROS Msg and publishing them."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c++",children:'class BodyDetNode : public hobot::dnn_node::DnnNode {\n public:\n  BodyDetNode(const std::string& node_name = "body_det",\n  const rclcpp::NodeOptions& options = rclcpp::NodeOptions());\n\n protected:\n  int SetNodePara() override;\n  int PostProcess(const std::shared_ptr<hobot::dnn_node::DnnNodeOutput>& node_output) override;\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Implementing the constructor of the BodyDetNode subclass"})}),"\n",(0,i.jsxs)(n.p,{children:["The constructor of the ",(0,i.jsx)(n.code,{children:"BodyDetNode"})," subclass initializes the Node and retrieves the size of the model's input image through the ",(0,i.jsx)(n.code,{children:"GetModelInputSize"})," interface, including the width ",(0,i.jsx)(n.code,{children:"model_input_width_"})," and height ",(0,i.jsx)(n.code,{children:"model_input_height_"})," of the image. This is used for model pre-processing as different models generally have different input image sizes."]}),"\n",(0,i.jsxs)(n.p,{children:["Creating a subscriber for image messages using zero-copy communication method, subscribing to image messages from camera node for algorithm model inference. The subscribed topic is ",(0,i.jsx)(n.code,{children:"/hbmem_img"})," and the message type is the image message type defined in ",(0,i.jsx)(n.code,{children:"tros.b"}),", ",(0,i.jsx)(n.code,{children:"hbm_img_msgs::msg::HbmMsg1080P"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["Creating a publisher for algorithm inference messages. The published topic is ",(0,i.jsx)(n.code,{children:"/cpp_dnn_demo"})," and the message type is the algorithm message type defined in ",(0,i.jsx)(n.code,{children:"tros.b"}),", ",(0,i.jsx)(n.code,{children:"ai_msgs::msg::PerceptionTargets"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"Creating a Multiple Object Tracking (MOT) algorithm engine for tracking each person detection box."}),"\n",(0,i.jsxs)(n.p,{children:["Constructor of ",(0,i.jsx)(n.code,{children:"BodyDetNode"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c++",children:'BodyDetNode::BodyDetNode(const std::string& node_name, const rclcpp::NodeOptions& options) :\n  hobot::dnn_node::DnnNode(node_name, options) {\n  // Initialize algorithm inference using the SetNodePara() method implemented in the BodyDetNode subclass\n  if (Init() != 0 ||\n    GetModelInputSize(0, model_input_width_, model_input_height_) < 0) {\n```RCLCPP_ERROR(rclcpp::get_logger("dnn_demo"), "Node init fail!");\n    rclcpp::shutdown();\n  }\n\n  // Create message subscriber, subscribe to image messages from camera node\n  ros_img_subscription_ =\n          this->create_subscription<hbm_img_msgs::msg::HbmMsg1080P>(\n          "/hbmem_img", 10, std::bind(&BodyDetNode::FeedImg, this, std::placeholders::_1));\n  // Create message publisher, publish algorithm inference messages\n  msg_publisher_ = this->create_publisher<ai_msgs::msg::PerceptionTargets>(\n      "/cpp_dnn_demo", 10);\n  // Create multi-object tracking (MOT) algorithm engine\n  hobot_mot_ = std::make_shared<HobotMot>("config/iou2_method_param.json");\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:["Where ",(0,i.jsx)(n.code,{children:"Init()"})," is an interface defined and implemented in the ",(0,i.jsx)(n.code,{children:"DnnNode"})," base class, which performs algorithm inference initialization, only concatenates the pipeline, and the specific ",(0,i.jsx)(n.code,{children:"SetNodePara()"})," steps are implemented by the user (in the subclass). The initialization process of the concatenation is as follows:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c++",children:'int DnnNode::Init() {\n  RCLCPP_INFO(rclcpp::get_logger("dnn"), "Node init.");\n\n  int ret = 0;\n  // 1. set model info in node para\n  ret = SetNodePara();\n  if (ret != 0) {\n    RCLCPP_ERROR(rclcpp::get_logger("dnn"), "Set node para failed!");\n    return ret;\n  }\n\n  // check node para\n  if (ModelTaskType::InvalidType == dnn_node_para_ptr_->model_task_type) {\n    RCLCPP_ERROR(rclcpp::get_logger("dnn"), "Invalid model task type");\n    return -1;\n  }\n\n  // 2. model init\n  ret = dnn_node_impl_->ModelInit();\n  if (ret != 0) {\n    RCLCPP_ERROR(rclcpp::get_logger("dnn"), "Model init failed!");\n    return ret;\n  }\n\n  // 3. set output parser\n  ret = SetOutputParser();\n  if (ret != 0) {\n    RCLCPP_ERROR(rclcpp::get_logger("dnn"), "Set output parser failed!");\n    return ret;\n  }\n```// 4. task init\n  ret = dnn_node_impl_->TaskInit();\n  if (ret != 0) {\n    RCLCPP_ERROR(rclcpp::get_logger("dnn"), "Task init failed!");\n    return ret;\n  }\n\n  return ret;\n}\n\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Set model parameters"})}),"\n",(0,i.jsx)(n.p,{children:"Set the path and name of the model file used for algorithm inference."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c++",children:'int BodyDetNode::SetNodePara() {\n  if (!dnn_node_para_ptr_) return -1;\n  dnn_node_para_ptr_->model_file = "config/multitask_body_kps_960x544.hbm";\n  dnn_node_para_ptr_->model_name = "multitask_body_kps_960x544";\n  return 0;\n}\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Implement the image subscription callback"})}),"\n",(0,i.jsxs)(n.p,{children:["Create ",(0,i.jsx)(n.code,{children:"DNNInput"})," type model input data. The subscribed message contains image information (such as encoding method, content data, resolution, etc.). Use the image processing interface ",(0,i.jsx)(n.code,{children:"hobot::dnn_node::ImageProc::GetNV12PyramidFromNV12Img"})," in ",(0,i.jsx)(n.code,{children:"hobot_dnn"})," algorithm module to convert the subscribed ",(0,i.jsx)(n.code,{children:"nv12"})," format image to the data type required by the model input, according to the model input resolution (",(0,i.jsx)(n.code,{children:"model_input_width_"})," and ",(0,i.jsx)(n.code,{children:"model_input_height_"}),", obtained from the loaded model by querying with the ",(0,i.jsx)(n.code,{children:"GetModelInputSize"})," interface in the constructor of ",(0,i.jsx)(n.code,{children:"BodyDetNode"}),"). The interface is defined as follows:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c++",children:"//   - [in] in_img_data: image data\n//   - [in] in_img_height: height of the image\n//   - [in] in_img_width: width of the image\n//   - [in] scaled_img_height: height of the model input\n//   - [in] scaled_img_width: width of the model input\nstd::shared_ptr<NV12PyramidInput> GetNV12PyramidFromNV12Img(\n    const char* in_img_data,\n    const int& in_img_height,\n    const int& in_img_width,\n    const int& scaled_img_height,\n    const int& scaled_img_width);\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Create ",(0,i.jsx)(n.code,{children:"FasterRcnnOutput"})," type model output data. The subscribed message contains message headers (",(0,i.jsx)(n.code,{children:"frame_id"})," and timestamp). Use the subscribed message headers to fill in the message headers of the output data, which represent the image information corresponding to the algorithm inference output."]}),"\n",(0,i.jsxs)(n.p,{children:["Start inference. Use the ",(0,i.jsx)(n.code,{children:"Run"})," interface in the base class ",(0,i.jsx)(n.code,{children:"DnnNode"})," to run the inference asynchronously, with the fourth parameter of the interface set as ",(0,i.jsx)(n.code,{children:"false"})," indicating the more efficient asynchronous inference mode. The ",(0,i.jsx)(n.code,{children:"Run"})," interface is defined as follows:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c++",children:"  // - Parameters\n  //   - [in] inputs: shared pointers to input data\n  //   - [in] outputs: shared pointer to output data\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c++",children:"struct Filter2DResult {\n  int track_id;  // Target number\n  hbm_common_msgs::msg::BBox2D bbox;  // bounding box\n};\n\n// Algorithm inference result callback, converting model output data into human detection boxes with target numbers\nvoid BodyDetNode::OnInferenceResult(const std::shared_ptr<hobot::dnn_node::DNNOutput>& output) {\n  if (!output || !output->FrameStamps().size()) {\n    return;\n  }\n\n  // Fill structured inference result data of type 'Filter2DResult' based on algorithm inference results\n  std::vector<Filter2DResult> results;\n  const auto* dnn_output = static_cast<const FasterRcnnOutput*>(output.get());\n  for (size_t i = 0; i < dnn_output->bboxes.size(); ++i) {\n    Filter2DResult result;\n    result.track_id = dnn_output->track_ids[i];\n    result.bbox.x = dnn_output->bboxes[i].tl().x;\n    result.bbox.y = dnn_output->bboxes[i].tl().y;\n    result.bbox.width = dnn_output->bboxes[i].br().x - dnn_output->bboxes[i].tl().x;\n    result.bbox.height = dnn_output->bboxes[i].br().y - dnn_output->bboxes[i].tl().y;\n    results.push_back(result);\n  }\n\n  // Process the inference results using the MOT algorithm to obtain missing target number data\n  std::vector<int> disappear_ids;\n  // TODO: MOT Algorithm processing\n  \n  // Publish the parsed structured reasoning results and disappearing target numbers\n  PublishDetection(results);\n  PublishDisappearingTarget(disappear_ids);\n}\n"})}),"\n",(0,i.jsx)(n.p,{children:"In the inference result callback 'OnInferenceResult', fill in structured inference result data of type 'Filter2DResult' based on the model output data. Then, use the MOT algorithm to process the inference results and obtain the missing target number data. Finally, publish the parsed structured reasoning results and the missing target number.Using the built-in Parse parsing method in hobot dnn, parse the output of the human detection algorithm."}),"\n",(0,i.jsx)(n.p,{children:"Run the multi-target tracking algorithm. Convert the human detection boxes outputted by the algorithm into the data type required by the MOT algorithm. Calculate the timestamp of the current frame based on the message header. After processing with the MOT algorithm, obtain the human detection boxes with target ID and the disappeared target ID."}),"\n",(0,i.jsx)(n.p,{children:"Publish the results of the algorithm inference. Create a ROS Msg, fill in the image message header (frame ID and timestamp) corresponding to the results of the algorithm inference, the human detection boxes with target ID, the frame rate statistics outputted by the algorithm inference, and the disappeared target ID. The published ROS Msg can be subscribed and used by other ROS Nodes."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c++",children:'int BodyDetNode::PostProcess(const std::shared_ptr<hobot::dnn_node::DnnNodeOutput> &node_output) {\n  if (!rclcpp::ok()) {\n    return 0;\n  }\n  \n  // Validate the validity of the output data\n  if (node_output->output_tensors.empty() ||\n    static_cast<int32_t>(node_output->output_tensors.size()) < box_output_index_) {\n    RCLCPP_ERROR(rclcpp::get_logger("dnn_demo"), "Invalid outputs");\n    return -1;\n  }\n\n  // Create the parsing output data\n  // The dimension of detection box results is equal to the number of detected target categories\n  std::vector<std::shared_ptr<hobot::dnn_node::parser_fasterrcnn::Filter2DResult>>\n      results;\n  // Key point data\n  std::shared_ptr<hobot::dnn_node::parser_fasterrcnn::LandmarksResult> output_body_kps = nullptr;\n\n  // Parse the algorithm output using the built-in Parse method in hobot dnn\n  if (hobot::dnn_node::parser_fasterrcnn::Parse(node_output, nullptr,\n  box_outputs_index_, -1, -1, results, output_body_kps) < 0) {\n    RCLCPP_ERROR(rclcpp::get_logger("dnn_node_sample"),\n                "Parse node_output fail!");\n    return -1;\n  }\n\n  auto filter2d_result = results.at(box_output_index_);\n  if (!filter2d_result) return -1;\n\n  // Convert the human detection boxes outputted by the algorithm inference into the data type required by MOT algorithm\n  std::vector<MotBox> in_box_list;\n  for (auto& rect : filter2d_result->boxes) {\n    in_box_list.emplace_back(\n        MotBox(rect.left, rect.top, rect.right, rect.bottom, rect.conf));\n  }\n  \n  // Calculate the timestamp of the current frame based on the message header\n  auto fasterRcnn_output =\n      std::dynamic_pointer_cast<FasterRcnnOutput>(node_output);\n  time_t time_stamp =\n      fasterRcnn_output->image_msg_header->stamp.sec * 1000 +\n```fasterRcnn_output->image_msg_header->stamp.nanosec / 1000 / 1000;\n\n// Create output of MOT algorithm: detection boxes with object IDs and disappeared object IDs\nstd::vector<MotBox> out_box_list;\nstd::vector<std::shared_ptr<MotTrackId>> disappeared_ids;\n\n// Run multi-object tracking algorithm\nif (hobot_mot_->DoProcess(in_box_list,\n                          out_box_list,\n                          disappeared_ids,\n                          time_stamp,\n                          model_input_width_,\n                          model_input_height_) < 0) {\n  RCLCPP_ERROR(rclcpp::get_logger("dnn_demo"), "Do mot fail");\n  return -1;\n}\n\n// Create ROS Msg for publishing inference results\nai_msgs::msg::PerceptionTargets::UniquePtr pub_data(\n  new ai_msgs::msg::PerceptionTargets());\n\n// Fill message header to ROS Msg\npub_data->header.set__stamp(fasterRcnn_output->image_msg_header->stamp);\npub_data->header.set__frame_id(fasterRcnn_output->image_msg_header->frame_id);\n\n// Fill algorithm inference output FPS to ROS Msg\nif (node_output->rt_stat) {\n  pub_data->set__fps(round(node_output->rt_stat->output_fps));\n  // If there is an update in algorithm inference statistics, output the statistics of input and output FPS of the algorithm model\n  if (node_output->rt_stat->fps_updated) {\n    RCLCPP_WARN(rclcpp::get_logger("dnn_demo"),\n                "input fps: %.2f, out fps: %.2f",\n                node_output->rt_stat->input_fps,\n                node_output->rt_stat->output_fps);\n  }\n}\n\nfor (auto& rect : out_box_list) {\n  // Validate the validity of the target tracking result\n  if (rect.id < 0) {\n    continue;\n  }\n  // Fill target tracking result and detection box to ROS Msg\n  ai_msgs::msg::Target target;\n  target.set__type("person");\n  target.set__track_id(rect.id);\n  ai_msgs::msg::Roi roi;\n  roi.type = "body";\n  roi.rect.set__x_offset(rect.x1);\n  roi.rect.set__y_offset(rect.y1);roi.rect.set__width(rect.x2 - rect.x1);\n    roi.rect.set__height(rect.y2 - rect.y1);\n    target.rois.emplace_back(roi);\n    pub_data->targets.emplace_back(std::move(target));\n  }\n\n  // Fill in the disappeared targets to ROS Msg\n  for (const auto& id_info : disappeared_ids) {\n    if (id_info->value < 0 ||\n        hobot_mot::DataState::INVALID == id_info->state_) {\n      continue;\n    }\n    ai_msgs::msg::Target target;\n    target.set__type("person");\n    target.set__track_id(id_info->value);\n    ai_msgs::msg::Roi roi;\n    roi.type = "body";\n    target.rois.emplace_back(roi);\n    pub_data->disappeared_targets.emplace_back(std::move(target));\n  }\n\n  // Publish ROS Msg\n  msg_publisher_->publish(std::move(pub_data));\n\n  return 0;\n}\n'})}),"\n",(0,i.jsx)(n.p,{children:"The dnn node contains built-in methods for parsing the output of various detection, classification, and segmentation algorithms. After installing tros.b on the RDK, the supported parsing methods can be queried as follows:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"root@ubuntu:~# tree /opt/tros/include/dnn_node/util/output_parser\n/opt/tros/include/dnn_node/util/output_parser\n\u251c\u2500\u2500 classification\n\u2502   \u2514\u2500\u2500 ptq_classification_output_parser.h\n\u251c\u2500\u2500 detection\n\u2502   \u251c\u2500\u2500 fasterrcnn_output_parser.h\n\u2502   \u251c\u2500\u2500 fcos_output_parser.h\n\u2502   \u251c\u2500\u2500 nms.h\n\u2502   \u251c\u2500\u2500 ptq_efficientdet_output_parser.h\n\u2502   \u251c\u2500\u2500 ptq_ssd_output_parser.h\n\u2502   \u251c\u2500\u2500 ptq_yolo2_output_parser.h\n\u2502   \u251c\u2500\u2500 ptq_yolo3_darknet_output_parser.h\n\u2502   \u2514\u2500\u2500 ptq_yolo5_output_parser.h\n\u251c\u2500\u2500 perception_common.h\n\u251c\u2500\u2500 segmentation\n\u2502   \u2514\u2500\u2500 ptq_unet_output_parser.h\n\u2514\u2500\u2500 utils.h\n\n3 directories, 12 files\n"})}),"\n",(0,i.jsxs)(n.p,{children:["You can see that there are three paths under the ",(0,i.jsx)(n.code,{children:"/opt/tros/include/dnn_node/util/output_parser"})," directory, which correspond to the output parsing methods for classification, detection, and segmentation algorithms."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"perception_common.h"})," defines the data type of the parsed perception results."]}),"\n",(0,i.jsx)(n.p,{children:"The algorithm models and their corresponding output parsing methods are as follows:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Algorithm Category"}),(0,i.jsx)(n.th,{children:"Algorithm"}),(0,i.jsx)(n.th,{children:"Output Parsing Method"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Object Detection"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"/rdk_doc/en/Robot_development/boxs/detection/fcos",children:"FCOS"})}),(0,i.jsx)(n.td,{children:"fcos_output_parser.h"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Object Detection"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"/rdk_doc/en/Robot_development/boxs/detection/efficientnet",children:"EfficientNet_Det"})}),(0,i.jsx)(n.td,{children:"ptq_efficientdet_output_parser.h"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Object Detection"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"/rdk_doc/en/Robot_development/boxs/detection/mobilenet",children:"MobileNet_SSD"})}),(0,i.jsx)(n.td,{children:"ptq_ssd_output_parser.h"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Object Detection"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"/rdk_doc/en/Robot_development/boxs/detection/yolo",children:"YoloV2"})}),(0,i.jsx)(n.td,{children:"ptq_yolo2_output_parser.h"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Object Detection"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"/rdk_doc/en/Robot_development/boxs/detection/yolo",children:"YoloV3"})}),(0,i.jsx)(n.td,{children:"ptq_yolo3_darknet_output_parser.h"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Object Detection"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"/rdk_doc/en/Robot_development/boxs/detection/yolo",children:"YoloV5"})}),(0,i.jsx)(n.td,{children:"ptq_yolo5_output_parser.h"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Human Detection"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"/i18n/en/docusaurus-plugin-content-docs/current/05_Robot_development/03_boxs/function/mono2d_body_detection.md",children:"FasterRcnn"})}),(0,i.jsx)(n.td,{children:"fasterrcnn_output_parser.h"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Image Classification"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"/rdk_doc/en/Robot_development/boxs/classification/mobilenetv2",children:"mobilenetv2"})}),(0,i.jsx)(n.td,{children:"ptq_classification_output_parser.h"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Semantic Segmentation"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"/rdk_doc/en/Robot_development/boxs/segmentation/mobilenet_unet",children:"mobilenet_unet"})}),(0,i.jsx)(n.td,{children:"ptq_unet_output_parser.h"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Entrance Function"})}),"\n",(0,i.jsxs)(n.p,{children:["Create an instance of ",(0,i.jsx)(n.code,{children:"BodyDetNode"}),", initialize and start the inference task in the constructor of ",(0,i.jsx)(n.code,{children:"BodyDetNode"}),", and stop the inference only when the user inputs the exit signal."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c++",children:"int main(int argc, char** argv) {\n  rclcpp::init(argc, argv);\n  rclcpp::spin(std::make_shared<BodyDetNode>());\n  rclcpp::shutdown();\n  return 0;\n}\n"})}),"\n",(0,i.jsx)(n.h5,{id:"23-compilation-dependencies",children:"2.3 Compilation Dependencies"}),"\n",(0,i.jsxs)(n.p,{children:["In step 1, the ",(0,i.jsx)(n.code,{children:"cpp_dnn_demo"})," package was created using the ",(0,i.jsx)(n.code,{children:"ros2 pkg create"})," command. The ",(0,i.jsx)(n.code,{children:"CMakeLists.txt"})," and ",(0,i.jsx)(n.code,{children:"package.xml"})," files have been automatically created in the ",(0,i.jsx)(n.code,{children:"dev_ws/src/cpp_dnn_demo"})," directory."]}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"package.xml"})," file automatically adds the compiled dependencies, which include ",(0,i.jsx)(n.code,{children:"rclcpp"}),", ",(0,i.jsx)(n.code,{children:"sensor_msgs"}),", ",(0,i.jsx)(n.code,{children:"ai_msgs"}),", ",(0,i.jsx)(n.code,{children:"hbm_img_msgs"}),", ",(0,i.jsx)(n.code,{children:"dnn_node"}),", and ",(0,i.jsx)(n.code,{children:"hobot_mot"}),". ",(0,i.jsx)(n.code,{children:"ai_msgs"})," defines the message format of the algorithm output in TogatherROS, ",(0,i.jsx)(n.code,{children:"hbm_img_msgs"})," defines the image message format used for zero-copy communication, ",(0,i.jsx)(n.code,{children:"dnn_node"})," is the algorithm inference framework, and ",(0,i.jsx)(n.code,{children:"hobot_mot"})," is the multi-object tracking algorithm. These packages are installed when TogatherROS is installed."]}),"\n",(0,i.jsx)(n.h5,{id:"24-compilation-script",children:"2.4 Compilation Script"}),"\n",(0,i.jsxs)(n.p,{children:["Add package dependencies and compilation installation information in ",(0,i.jsx)(n.code,{children:"CMakeLists.txt"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"(1) Add dependencies on the multi-object tracking algorithm and the algorithm inference engine library."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-cmake",children:'link_directories(\n  /opt/tros/lib/\n``````cmake\n# CMakeLists.txt \n\ncmake_minimum_required(VERSION 3.5)\nproject(cpp_dnn_demo)\n\n# Default to C99\nif(NOT CMAKE_C_STANDARD)\n  set(CMAKE_C_STANDARD 99)\nendif()\n\n# Default to C++14\nif(NOT CMAKE_CXX_STANDARD)\n  set(CMAKE_CXX_STANDARD 14)\nendif()\n\nif(CMAKE_COMPILER_IS_GNUCXX OR CMAKE_CXX_COMPILER_ID MATCHES "Clang")\n  add_compile_options(-Wall -Wextra -Wpedantic)\nendif()\n\n# (1) Set the library path to "/usr/lib/hbbpu/"\nset(LIBHB_PATH /usr/lib/hbbpu/)\n\n# (2) Add pkg compilation information\nadd_executable(${PROJECT_NAME}\n  src/body_det_demo.cpp\n)\n\nament_target_dependencies(\n  ${PROJECT_NAME}\n  rclcpp\n  dnn_node\n  sensor_msgs\n  ai_msgs\n  hobot_mot\n  hbm_img_msgs\n)\n\n# (3) Add pkg installation information to enable running the compiled pkg with "ros2 run"\ninstall(\n  TARGETS ${PROJECT_NAME}\n  RUNTIME DESTINATION lib/${PROJECT_NAME}\n)# find dependencies\nfind_package(ament_cmake REQUIRED)\nfind_package(rclcpp REQUIRED)\nfind_package(sensor_msgs REQUIRED)\nfind_package(ai_msgs REQUIRED)\nfind_package(hbm_img_msgs REQUIRED)\nfind_package(dnn_node REQUIRED)\nfind_package(hobot_mot REQUIRED)\n\nlink_directories(\n  /opt/tros/lib/  \n  /usr/lib/hbbpu/\n)\n\nadd_executable(${PROJECT_NAME}\n  src/body_det_demo.cpp\n)\n\nament_target_dependencies(\n  ${PROJECT_NAME}\n  rclcpp\n  dnn_node\n  sensor_msgs\n  ai_msgs\n  hobot_mot\n  hbm_img_msgs\n)\n\n# Install executables\ninstall(\n  TARGETS ${PROJECT_NAME}\n  RUNTIME DESTINATION lib/${PROJECT_NAME}\n)\n\nament_package()\n'})}),"\n",(0,i.jsx)(n.h4,{id:"3-build-and-run",children:"3 Build and Run"}),"\n",(0,i.jsx)(n.h5,{id:"31-build",children:"3.1 Build"}),"\n",(0,i.jsx)(n.p,{children:"In the RDK with tros.b installed, execute the following command to build the package:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"cd ~/dev_ws\n\n# Configure tros.b environment\nsource /opt/tros/setup.bash\n\n```# Compiling pkg\ncolcon build --packages-select cpp_dnn_demo\n"})}),"\n",(0,i.jsx)(n.p,{children:'If the compilation is successful, an installation package called "install" will be generated in the compilation path for the cpp_dnn_demo pkg. The following information will be displayed in the compilation terminal:'}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"Starting >>> cpp_dnn_demo\n[Processing: cpp_dnn_demo]\nFinished <<< cpp_dnn_demo [32.7s]\n\nSummary: 1 package finished [33.4s]\n"})}),"\n",(0,i.jsx)(n.h5,{id:"32-common-compilation-errors",children:"3.2 Common Compilation Errors"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"ModuleNotFoundError: No module named 'ament_package'"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Specific error message:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'# colcon build --packages-select cpp_dnn_demo\nStarting >>> cpp_dnn_demo\n--- stderr: cpp_dnn_demo\nCMake Error at CMakeLists.txt:19 (find_package):\n  By not providing "Findament_cmake.cmake" in CMAKE_MODULE_PATH this project\n  has asked CMake to find a package configuration file provided by\n  "ament_cmake", but CMake did not find one.\n\n  Could not find a package configuration file provided by "ament_cmake" with\n  any of the following names:\n\n    ament_cmakeConfig.cmake\n    ament_cmake-config.cmake\n\n  Add the installation prefix of "ament_cmake" to CMAKE_PREFIX_PATH or set\n  "ament_cmake_DIR" to a directory containing one of the above files.  If\n  "ament_cmake" provides a separate development package or SDK, be sure it\n  has been installed.\n\n\n---\nFailed <<< cpp_dnn_demo [2.83s, exited with code 1]\n\nSummary: 0 packages finished [3.44s]\n  1 package failed: cpp_dnn_demo\n  1 package had stderr output: cpp_dnn_demo\n'})}),"\n",(0,i.jsx)(n.p,{children:'This indicates that the ROS2 environment is not configured successfully. Enter the "ros2" command in the terminal to check the environment:'}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"# ros2\n\n-bash: ros2: command not found\n"})}),"\n",(0,i.jsx)(n.p,{children:'If you receive the "command not found" prompt, it means that the ROS2 environment has not been configured successfully. Please check if the command source /opt/tros/setup.bash has been executed successfully. The successful output information is as follows:'}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"# ros2\n\nusage: ros2 [-h] Call `ros2 <command> -h` for more detailed usage. ...\n\nros2 is an extensible command-line tool for ROS 2.\n\noptional arguments:\n  -h, --help            show this help message and exit\n"})}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsx)(n.li,{children:"Cannot find the dnn_node package"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The specific error message is as follows:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:'colcon build --packages-select cpp_dnn_demo\nStarting >>> cpp_dnn_demo\n[Processing: cpp_dnn_demo]\n--- stderr: cpp_dnn_demo\nCMake Error at CMakeLists.txt:22 (find_package):\n By not providing "Finddnn_node.cmake" in CMAKE_MODULE_PATH this project has\n asked CMake to find a package configuration file provided by "dnn_node",\n but CMake did not find one.\nCould not find a package configuration file provided by "dnn_node" with any\n of the following names:\ndnn_nodeConfig.cmake\ndnn_node-config.cmake\nAdd the installation prefix of "dnn_node" to CMAKE_PREFIX_PATH or set\n "dnn_node_DIR" to a directory containing one of the above files. If\n "dnn_node" provides a separate development package or SDK, be sure it has\n been installed.\nFailed <<< cpp_dnn_demo [59.7s, exited with code 1]\nSummary: 0 packages finished [1min 1s]\n 1 package failed: cpp_dnn_demo\n 1 package had stderr output: cpp_dnn_demo\n'})}),"\n",(0,i.jsx)(n.p,{children:"This indicates that the hobot_dnn environment has not been configured successfully. Please check if /opt/tros/share/dnn_node exists."}),"\n",(0,i.jsx)(n.h5,{id:"33-running",children:"3.3 Running"}),"\n",(0,i.jsx)(n.p,{children:"In order to better display the effect of algorithm reasoning and experience perception ability, the MIPI camera image capture, image encoding, and WEB data display Node in tros.b are used to provide the ability of data sensing and display. The system can publish the captured images from the camera on RDK, perform algorithm reasoning to detect human body frames, and render and display the images and human body frame detection results in real time on the WEB browser on the PC side."}),"\n",(0,i.jsx)(n.p,{children:"The runtime system process diagram is as follows:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/05_tros_dev/image/ai_predict/pipeline.jpg",alt:""})}),"\n",(0,i.jsx)(n.p,{children:"There are 4 nodes running on RDK, and algorithm reasoning is one of them in this example."}),"\n",(0,i.jsx)(n.p,{children:"The system startup process is as follows:"}),"\n",(0,i.jsx)(n.p,{children:"(1) Open terminal one on RDK and start the algorithm reasoning node:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"cd ~/dev_ws\n\n# Configure the tros.b environment\nsource /opt/tros/setup.bash\n\n# Configure the cpp_dnn_demo environment\nsource ./install/setup.bash\n\n# Copy the necessary configuration files for running the example from the tros.b installation path\n# Model file\nmkdir -p config && cp /opt/tros/lib/dnn_benchmark_example/config/X3/multitask_body_kps_960x544.hbm config/\n# Multi-object tracking configuration file\ncp -r /opt/tros/lib/hobot_mot/config/iou2_method_param.json config/\n\n\n# Run the cpp_dnn_demo pkg\nros2 run cpp_dnn_demo cpp_dnn_demo --ros-args --log-level warn\n"})}),"\n",(0,i.jsx)(n.p,{children:"(2) Open terminal 2 on RDK and start the image publishing, encoding, and display Node in tros.b:"}),"\n",(0,i.jsxs)(n.p,{children:["Since multiple Nodes need to be started, a launch script is used to start the Nodes in batches. Create a launch script ",(0,i.jsx)(n.code,{children:"cpp_dnn_demo.launch.py"})," in any path on RDK, with the following content:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import os\n\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\nfrom launch.actions import IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom ament_index_python import get_package_share_directory\nfrom ament_index_python.packages import get_package_prefix\n\ndef generate_launch_description():\n    web_service_launch_include = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource(\n```os.path.join(\n                get_package_share_directory(\'websocket\'),\n                \'launch/websocket_service.launch.py\'))\n    )\n\n    return LaunchDescription([\n        web_service_launch_include,\n        # Start image publishing package\n        Node(\n            package=\'mipi_cam\',\n            executable=\'mipi_cam\',\n            output=\'screen\',\n            parameters=[\n                {"out_format": "nv12"},\n                {"image_width": 960},\n                {"image_height": 544},\n                {"io_method": "shared_mem"},\n                {"video_device": "F37"}\n            ],\n            arguments=[\'--ros-args\', \'--log-level\', \'error\']\n        ),\n        # Start jpeg image encoding & publishing package\n        Node(\n            package=\'hobot_codec\',\n            executable=\'hobot_codec_republish\',\n            output=\'screen\',\n            parameters=[\n                {"channel": 1},\n                {"in_mode": "shared_mem"},\n                {"in_format": "nv12"},\n                {"out_mode": "ros"},\n                {"out_format": "jpeg"},\n                {"sub_topic": "/hbmem_img"},\n                {"pub_topic": "/image_jpeg"}\n            ],\n            arguments=[\'--ros-args\', \'--log-level\', \'error\']\n        ),\n        # Start web display package\n        Node(\n            package=\'websocket\',\n            executable=\'websocket\',\n            output=\'screen\',\n            parameters=[\n                {"image_topic": "/image_jpeg"},\n                {"image_type": "mjpeg"},\n                {"smart_topic": "/cpp_dnn_demo"}\n            ],\n            arguments=[\'--ros-args\', \'--log-level\', \'error\']\n        )\n    ])Using startup script:\n\n```shell\n# Configure tros.b environment\nsource /opt/tros/setup.bash\n\n# Launch the nodes for image publishing, encoding, and display\nros2 launch cpp_dnn_demo.launch.py\n'})}),"\n",(0,i.jsx)(n.h5,{id:"34-common-errors",children:"3.4 Common Errors"}),"\n",(0,i.jsx)(n.p,{children:"If the following error message is displayed during startup:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"error while loading shared libraries: libdnn_node.so: cannot open shared object file: No such file or directory\n"})}),"\n",(0,i.jsx)(n.p,{children:"It indicates that the configuration of the hobot_dnn environment has failed. Please check if /opt/tros/share/dnn_node exists."}),"\n",(0,i.jsx)(n.h5,{id:"35-results",children:"3.5 Results"}),"\n",(0,i.jsx)(n.p,{children:"After successful execution, the terminal outputs the following information:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"root@ubuntu:~/dev_ws# ros2 run cpp_dnn_demo cpp_dnn_demo\n[BPU_PLAT]BPU Platform Version(1.3.1)!\n[C][154775][10-25][00:33:53:266][configuration.cpp:49][EasyDNN]EasyDNN version: 0.4.11\n[HBRT] set log level as 0. version = 3.14.5\n[DNN] Runtime version = 1.9.7_(3.14.5 HBRT)\n[WARN] [1666629233.325690884] [dnn]: Run default SetOutputParser.\n[WARN] [1666629233.326263403] [dnn]: Set output parser with default dnn node parser, you will get all output tensors and should parse output_tensors in PostProcess.\n(MOTMethod.cpp:34): MOTMethod::Init config/iou2_method_param.json\n\n(IOU2.cpp:29): IOU2 Mot::Init config/iou2_method_param.json\n\n[WARN] [1666629234.410291616] [dnn_demo]: input fps: 31.22, out fps: 31.28\n[WARN] [1666629235.410357068] [dnn_demo]: input fps: 30.00, out fps: 30.00\n[WARN] [1666629236.444863458] [dnn_demo]: input fps: 30.01, out fps: 29.98\n[WARN] [1666629237.476656118] [dnn_demo]: input fps: 30.00, out fps: 30.07\n[WARN] [1666629238.478156431] [dnn_demo]: input fps: 30.01, out fps: 29.97\n[WARN] [1666629239.510039629] [dnn_demo]: input fps: 30.01, out fps: 30.07\n[WARN] [1666629240.511561150] [dnn_demo]: input fps: 30.00, out fps: 29.97\n[WARN] [1666629241.543333811] [dnn_demo]: input fps: 30.01, out fps: 30.07\n[WARN] [1666629242.544654089] [dnn_demo]: input fps: 30.01, out fps: 29.97\n[WARN] [1666629243.576435625] [dnn_demo]: input fps: 30.01, out fps: 30.07\n"})}),"\n",(0,i.jsxs)(n.p,{children:["The log output shows that the model used for algorithm inference during initialization has an input image resolution of 960x544. The ",(0,i.jsx)(n.code,{children:"MOT"})," algorithm engine is using the configuration file ",(0,i.jsx)(n.code,{children:"config/iou2_method_param.json"}),". During inference, the input and output frame rate of the algorithm is 30fps, and the statistics are refreshed once per second."]}),"\n",(0,i.jsxs)(n.p,{children:["Use the ",(0,i.jsx)(n.code,{children:"ros2"})," command on RDK to query and output the contents of the ",(0,i.jsx)(n.code,{children:"/cpp_dnn_demo"})," topic messages published by the inference node."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"root@ubuntu:~# source /opt/tros/setup.bash\nroot@ubuntu:~# ros2 topic list\n/cpp_dnn_demo\n/hbmem_img08172824022201080202012021072315\n/image_jpeg\n/parameter_events\n/rosout\nroot@ubuntu:~# ros2 topic echo /cpp_dnn_demo\nheader:\n  stamp:\n    sec: 1659938514\n    nanosec: 550421888\n  frame_id: '7623'\nfps: 30\nperfs: []\ntargets:\n- type: person\n  track_id: 1\n  rois:\n  - type: body\n    rect:\n      x_offset: 306\n      y_offset: 106\n      height: 416\n      width: 151\n      do_rectify: false\n  attributes: []\n  points: []\n  captures: []\n- type: person\n  track_id: 2\n  rois:\n  - type: body\n    rect:\n      x_offset: 135\n      y_offset: 89\n      height: 423\n      width: 155\n      do_rectify: false\n  attributes: []\n  points: []\n  captures: []\n- type: person\n  track_id: 3\n  rois:\n  - type: body\n    rect:\n      x_offset: 569y_offset: 161\n  height: 340\n  width: 123\n  do_rectify: false\nattributes: []\npoints: []\ncaptures: []\n- type: person\n  track_id: 4\n  rois:\n  - type: body\n    rect:\n      x_offset: 677\n      y_offset: 121\n      height: 398\n      width: 123\n      do_rectify: false\n  attributes: []\n  points: []\n  captures: []\n- type: person\n  track_id: 5\n  rois:\n  - type: body\n    rect:\n      x_offset: 478\n      y_offset: 163\n      height: 348\n      width: 103\n      do_rectify: false\n  attributes: []\n  points: []\n  captures: []\ndisappeared_targets: []\n---\n\n"})}),"\n",(0,i.jsxs)(n.p,{children:["The output ",(0,i.jsx)(n.code,{children:"/cpp_dnn_demo"})," topic message indicates that the algorithm detected 5 human body boxes , and output the coordinates and corresponding target tracking results (track_id) for each detection box."]}),"\n",(0,i.jsxs)(n.p,{children:["Enter ",(0,i.jsx)(n.a,{href:"http://IP:8000",children:"http://IP:8000"})," (IP is the IP address of RDK, for example, the IP address used in this example is 10.64.28.88) on the PC's web browser to view real-time images and algorithm inference rendering effects:"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/05_tros_dev/image/ai_predict/render.jpg",alt:""})}),"\n",(0,i.jsxs)(n.p,{children:["Each detection box is rendered with the detection box type (such as ",(0,i.jsx)(n.code,{children:"body"})," indicating human body detection box) and the target tracking result. The ",(0,i.jsx)(n.code,{children:"fps"})," field in the lower left corner of the browser indicates the real-time algorithm inference output frame rate."]}),"\n",(0,i.jsxs)(n.p,{children:["Enter the ",(0,i.jsx)(n.code,{children:"Ctrl+C"})," command to exit the program."]}),"\n",(0,i.jsxs)(n.p,{children:["Summary of this sectionThis chapter introduces how to use the models provided by D-Robotics to create and run an algorithm inference example for human detection based on ",(0,i.jsx)(n.code,{children:"hobot_dnn"}),". It uses images published from the camera, obtains the algorithm output, and renders and displays the image and algorithm inference results in real-time on the PC browser."]}),"\n",(0,i.jsxs)(n.p,{children:["Users can refer to the ",(0,i.jsx)(n.a,{href:"https://github.com/D-Robotics/hobot_dnn/blob/develop/README.md",children:"README.md"})," and the ",(0,i.jsx)(n.a,{href:"https://github.com/D-Robotics/hobot_dnn/blob/develop/docs/API-Manual/API-Manual.md",children:"API Manual"})," in ",(0,i.jsx)(n.code,{children:"hobot_dnn"})," to learn about the richer algorithm inference capabilities."]}),"\n",(0,i.jsx)(n.h2,{id:"algorithm-workflow-construction",children:"Algorithm Workflow Construction"}),"\n",(0,i.jsx)(n.h3,{id:"background-1",children:"Background"}),"\n",(0,i.jsx)(n.p,{children:'ROS2 nodes decompose complex robot software systems into multiple functional and logically independent modules. For example, a robot application may include multiple nodes for sensing and algorithm perception functions. Nodes exchange data through "topics". Nodes with different functions in the robot software system are connected through topics to form a directed acyclic graph (DAG).'}),"\n",(0,i.jsx)(n.p,{children:"The D-Robotics TogetheROS.Bot software stack includes rich robot development components and algorithm nodes. The sensing nodes support capturing image data from the camera and publishing it for use in perception algorithm inference. The human bounding box detection algorithm node in the perception algorithm library performs inference on image data and outputs human bounding box detection results. The human keypoint detection algorithm node performs inference on image data and the human bounding box detection results to output human keypoint detection results. Therefore, the human keypoint detection algorithm node needs to communicate with the human bounding box detection algorithm node by subscribing to the human bounding box messages published by that node."}),"\n",(0,i.jsx)(n.p,{children:"By reading this chapter, users can use the sensing nodes, human bounding box detection algorithm nodes, and human keypoint detection algorithm nodes in RDK, connect the sensing nodes and perception nodes through ROS2 topics, and achieve the goal of developing complex robot algorithm applications."}),"\n",(0,i.jsx)(n.h3,{id:"preparation-1",children:"Preparation"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"RDK development board with the following software installed:"}),"\n"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"D-Robotics-provided  Ubuntu 20.04/22.04 system image"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"tros.b"})," software package"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"RDK with F37 or GC4663 camera installed"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"A PC in the same network segment (wired or connected to the same wireless network, with the first three segments of the IP address consistent with RDK). The PC needs to have the following environment installed:"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Ubuntu 20.04/22.04 system"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"https://docs.ros.org/en/foxy/Installation/Ubuntu-Install-Debians.html",children:"ROS2 Foxy Desktop"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"http://docs.ros.org/en/foxy/Concepts/About-RQt.html",children:"rqt graphical tool"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"usage-1",children:"Usage"}),"\n",(0,i.jsx)(n.h4,{id:"1-start-data-collection-node",children:"1. Start Data Collection Node"}),"\n",(0,i.jsx)(n.p,{children:"On the RDK, open a terminal and start the image publishing node, which captures image data from the F37 camera and publishes it for algorithm inference:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"# Configure the tros.b environment\nsource /opt/tros/setup.bash\n# Start the node\nros2 run mipi_cam mipi_cam --ros-args -p out_format:=nv12 -p image_width:=960 -p image_height:=544 -p video_device:=F37 -p io_method:=shared_mem --log-level warn\n"})}),"\n",(0,i.jsx)(n.h4,{id:"2-start-human-bounding-box-detection-algorithm-node",children:"2. Start Human Bounding Box Detection Algorithm Node"}),"\n",(0,i.jsxs)(n.p,{children:["On the RDK, open a terminal and start the human bounding box detection algorithm node, which subscribes to the image messages published by the data collection node, detects and publishes human bounding box messages.The publishing topic specified in the launch command is ",(0,i.jsx)(n.code,{children:"hobot_hand_detection"}),"."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"# Set up tros.b environment\nsource /opt/tros/setup.bash\n# Copy the required configuration files from the installation path of tros.b.\ncp -r /opt/tros/lib/mono2d_body_detection/config/ .\n# Start Node\nros2 run mono2d_body_detection mono2d_body_detection --ros-args --log-level warn --ros-args -p ai_msg_pub_topic_name:=hobot_hand_detection\n"})}),"\n",(0,i.jsx)(n.h4,{id:"3-start-the-node-for-hand-keypoint-detection-algorithm",children:"3 Start the Node for Hand Keypoint Detection Algorithm"}),"\n",(0,i.jsx)(n.p,{children:"Open a terminal on the RDK and start the Node for hand keypoint detection algorithm. It subscribes to image messages published by the data collection Node and hand bounding box messages published by the hand bounding box detection algorithm Node."}),"\n",(0,i.jsxs)(n.p,{children:["The publishing topic is specified as ",(0,i.jsx)(n.code,{children:"hobot_hand_lmk_detection"}),", and the subscribing topic is specified as ",(0,i.jsx)(n.code,{children:"hobot_hand_detection"})," in the launch command."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"# Set up tros.b environment\nsource /opt/tros/setup.bash\n# Copy the required configuration files from the installation path of tros.b.\ncp -r /opt/tros/lib/hand_lmk_detection/config/ .\n# Start Node\nros2 run hand_lmk_detection hand_lmk_detection --ros-args --log-level warn --ros-args -p ai_msg_pub_topic_name:=hobot_hand_lmk_detection -p ai_msg_sub_topic_name:=hobot_hand_detection\n"})}),"\n",(0,i.jsx)(n.h4,{id:"4-view-the-output-of-the-algorithm-inference",children:"4 View the Output of the Algorithm Inference"}),"\n",(0,i.jsx)(n.p,{children:"Open a terminal on the RDK and use the ROS2 command to view the Topic messages published by the algorithm inference Node."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"View the hand bounding box detection messages published by the hand bounding box detection algorithm Node"})}),"\n",(0,i.jsx)(n.p,{children:"Query command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"# Set up tros.b environment\nsource /opt/tros/setup.bash\n# Start Node\nros2 topic echo /hobot_hand_detection\n"})}),"\n",(0,i.jsx)(n.p,{children:"Output:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"header:\n  stamp:\n    sec: 1660034025\n    nanosec: 429969208\n  frame_id: '8049'\nfps: 30\ntargets:\n- type: person\n  track_id: 10\n  rois:\n  - type: hand\n    rect:\n      x_offset: 619\n      y_offset: 128\n      height: 229\n      width: 168\n      do_rectify: false\n  attributes: []\n  points: []\n  captures: []\ndisappeared_targets: []\n"})}),"\n",(0,i.jsx)(n.p,{children:"The message contains the results of a person's hand detection algorithm (roi type: hand) published by the fps: 30 target detection algorithm Node."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"View the hand keypoint detection message published by the hand keypoint detection algorithm Node"})}),"\n",(0,i.jsx)(n.p,{children:"Query command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"# Configure the tros.b environment\nsource /opt/tros/setup.bash\n# Start the Node\nros2 topic echo /hobot_hand_lmk_detection\n"})}),"\n",(0,i.jsx)(n.p,{children:"Output result:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"header:\n  stamp:\n    sec: 1660034025\n    nanosec: 429969208\n  frame_id: '8049'\nfps: 30\ntargets:\n- type: person\n  track_id: 10\n  rois:\n  - type: hand\n    rect:\n      x_offset: 619\n      y_offset: 128\n      height: 229\n      width: 168do_rectify: false\nattributes: []\npoints:\n- type: hand_kps\n  point:\n  - x: 715.2421875\n    y: 348.0546875\n    z: 0.0\n  - x: 673.4921875\n    y: 315.8515625\n    z: 0.0\n  - x: 655.2265625\n    y: 294.3828125\n    z: 0.0\n  - x: 639.5703125\n    y: 262.1796875\n    z: 0.0\n  - x: 621.3046875\n    y: 229.9765625\n    z: 0.0\n  - x: 686.5390625\n    y: 247.8671875\n    z: 0.0\n  - x: 683.9296875\n    y: 201.3515625\n    z: 0.0\n  - x: 683.9296875\n    y: 176.3046875\n    z: 0.0\n  - x: 681.3203125\n    y: 147.6796875\n    z: 0.0\n  - x: 712.6328125\n    y: 240.7109375\n    z: 0.0\n  - x: 717.8515625\n    y: 194.1953125\n    z: 0.0\n  - x: 720.4609375\n    y: 161.9921875\n    z: 0.0\n  - x: 723.0703125\n    y: 129.7890625\n    z: 0.0\n  - x: 736.1171875\n    y: 247.8671875\n    z: 0.0\n  - x: 743.9453125\n    y: 201.3515625\n    z: 0.0- x: 749.1640625\n  y: 172.7265625\n  z: 0.0\n- x: 749.1640625\n  y: 140.5234375\n  z: 0.0\n- x: 759.6015625\n  y: 262.1796875\n  z: 0.0\n- x: 770.0390625\n  y: 226.3984375\n  z: 0.0\n- x: 775.2578125\n  y: 204.9296875\n  z: 0.0\n- x: 775.2578125\n  y: 179.8828125\n  z: 0.0\nconfidence: []\ncaptures: []\ndisappeared_targets: []\n"})}),"\n",(0,i.jsxs)(n.p,{children:["We can see that the published message contains the bounding box and keypoint detection results of the hand. The bounding box information is consistent with the subscribed message.The Rqt's Node Graph function on the PC side (",(0,i.jsx)(n.strong,{children:"the PC needs to be in the same network segment as the RDK"}),") can visualize the Nodes running on the RDK, the topics published and subscribed by the Nodes, and the graph composed of these topics, as shown in the figure below:"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/05_tros_dev/image/ai_predict/rosgraph_handlmk.jpg",alt:""})}),"\n",(0,i.jsx)(n.p,{children:"In the figure, the ellipses represent the Node names, and the rectangles represent the Topic names. It can be seen that the entire graph consists of 3 Nodes and 2 Topics."}),"\n",(0,i.jsx)(n.p,{children:'The "mipi_cam" (sensing Node) is the starting point, responsible for capturing and publishing images from the camera.'}),"\n",(0,i.jsx)(n.p,{children:'The "mono2d_body_det" (algorithm perception Node) is the middle node, subscribing to the image data published by the "mipi_cam" Node and performing human body box detection.'}),"\n",(0,i.jsx)(n.p,{children:'The "hand_lmk_det" (algorithm perception Node) is the end node, subscribing to the image data published by the "mipi_cam" Node and the human body box detection data published by the "mono2d_body_det" Node, and performing hand keypoint detection.'}),"\n",(0,i.jsx)(n.h3,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"This chapter introduces the use of sensing Nodes, human body box detection Nodes, and hand keypoint detection algorithm Nodes in the RDK, based on ROS2 topic communication. By connecting two perception algorithm Nodes, images captured from the camera are used for algorithm inference and the detected hand keypoints are published as messages."}),"\n",(0,i.jsx)(n.p,{children:"Based on the algorithm chaining principle introduced in this chapter, users can connect more algorithm Nodes on the RDK and develop feature-rich robot algorithm applications."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}}}]);