"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[1410],{8986:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"Basic_Application/pydev_demo_sample/basic_sample","title":"3.3.1 Basic Image Classification Example Introduction","description":"Example Overview","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/03_Basic_Application/03_pydev_demo_sample/01_basic_sample.md","sourceDirName":"03_Basic_Application/03_pydev_demo_sample","slug":"/Basic_Application/pydev_demo_sample/basic_sample","permalink":"/rdk_doc/en/Basic_Application/pydev_demo_sample/basic_sample","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1765534611000,"sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"3.2.6 vio2encoder Sample Introduction","permalink":"/rdk_doc/en/Basic_Application/cdev_demo_sample/vio2encoder"},"next":{"title":"3.3.2 Segment Model Example Introduction","permalink":"/rdk_doc/en/Basic_Application/pydev_demo_sample/segment_sample"}}');var o=i(74848),s=i(28453);const r={sidebar_position:1},a="3.3.1 Basic Image Classification Example Introduction",l={},c=[{value:"Example Overview",id:"example-overview",level:2},{value:"Effect Demonstration",id:"effect-demonstration",level:2},{value:"Hardware Preparation",id:"hardware-preparation",level:2},{value:"Hardware Connection",id:"hardware-connection",level:3},{value:"Quick Start",id:"quick-start",level:2},{value:"Code and Board Location",id:"code-and-board-location",level:3},{value:"Compilation and Execution",id:"compilation-and-execution",level:3},{value:"Execution Results",id:"execution-results",level:3},{value:"Detailed Introduction",id:"detailed-introduction",level:2},{value:"Example Program Parameter Options Description",id:"example-program-parameter-options-description",level:3},{value:"Software Architecture Description",id:"software-architecture-description",level:3},{value:"API Flow Description",id:"api-flow-description",level:3},{value:"FAQ",id:"faq",level:3}];function d(e){const n={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"331-basic-image-classification-example-introduction",children:"3.3.1 Basic Image Classification Example Introduction"})}),"\n",(0,o.jsx)(n.h2,{id:"example-overview",children:"Example Overview"}),"\n",(0,o.jsxs)(n.p,{children:["The basic image classification example is a set of ",(0,o.jsx)(n.strong,{children:"Python interface"})," development code samples located in ",(0,o.jsx)(n.code,{children:"/app/pydev_demo/01_basic_sample/"}),", demonstrating how to use the ",(0,o.jsx)(n.code,{children:"hobot_dnn"})," module for image classification tasks. These examples implement the same image classification functionality based on different neural network models."]}),"\n",(0,o.jsx)(n.p,{children:"Included model examples:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"test_resnet18.py - Image classification using ResNet18 model"}),"\n",(0,o.jsx)(n.li,{children:"test_efficientnasnet_m.py - Image classification using EfficientNasNet model"}),"\n",(0,o.jsx)(n.li,{children:"test_googlenet.py - Image classification using GoogleNet model"}),"\n",(0,o.jsx)(n.li,{children:"test_mobilenetv1.py - Image classification using MobileNetV1 model"}),"\n",(0,o.jsx)(n.li,{children:"test_vargconvnet.py - Image classification using VargConvNet model"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"effect-demonstration",children:"Effect Demonstration"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/03_pydev_demo_sample/image/pydev_01_running.png",alt:"output-img"})}),"\n",(0,o.jsx)(n.h2,{id:"hardware-preparation",children:"Hardware Preparation"}),"\n",(0,o.jsx)(n.h3,{id:"hardware-connection",children:"Hardware Connection"}),"\n",(0,o.jsx)(n.p,{children:"This example only requires the RDK development board itself, without additional peripheral connections. Ensure the development board is properly powered and the system is booted."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/03_pydev_demo_sample/image/pydev_01_hw_connect.png",alt:"connect-img"})}),"\n",(0,o.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,o.jsx)(n.h3,{id:"code-and-board-location",children:"Code and Board Location"}),"\n",(0,o.jsxs)(n.p,{children:["Navigate to ",(0,o.jsx)(n.code,{children:"/app/pydev_demo/01_basic_sample/"})," location, where you can see the basic example contains multiple model test files:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"root@ubuntu:/app/pydev_demo/01_basic_sample# tree\n.\n\u251c\u2500\u2500 imagenet1000_clsidx_to_labels.txt\n\u251c\u2500\u2500 test_efficientnasnet_m.py\n\u251c\u2500\u2500 test_googlenet.py\n\u251c\u2500\u2500 test_mobilenetv1.py\n\u251c\u2500\u2500 test_resnet18.py\n\u251c\u2500\u2500 test_vargconvnet.py\n\u2514\u2500\u2500 zebra_cls.jpg\n\n"})}),"\n",(0,o.jsx)(n.h3,{id:"compilation-and-execution",children:"Compilation and Execution"}),"\n",(0,o.jsx)(n.p,{children:"Python examples do not require compilation and can be run directly."}),"\n",(0,o.jsx)(n.h3,{id:"execution-results",children:"Execution Results"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"root@ubuntu:/app/pydev_demo/01_basic_sample# ./test_resnet18.py \n[BPU_PLAT]BPU Platform Version(1.3.6)!\n[HBRT] set log level as 0. version = 3.15.55.0\n[DNN] Runtime version = 1.24.5_(3.15.55 HBRT)\n[A][DNN][packed_model.cpp:247][Model](2025-09-09,19:46:43.279.404) [HorizonRT] The model builder version = 1.23.5\n[W][DNN]bpu_model_info.cpp:491][Version](2025-09-09,19:46:43.415.165) Model: resnet18_224x224_nv12. Inconsistency between the hbrt library version 3.15.55.0 and the model build version 3.15.47.0 detected, in order to ensure correct model results, it is recommended to use compilation tools and the BPU SDK from the same OpenExplorer package.\n========== inputs[0] properties ==========\ntensor type: NV12\ndata type: uint8\nlayout: NCHW\nshape: (1, 3, 224, 224)\ninputs[0] name is: data\n========== outputs[0] properties ==========\ntensor type: float32\ndata type: float32\nlayout: NCHW\nshape: (1, 1000, 1, 1)\noutputs[0] name is: prob\npostprocess time is : 0.0007224082946777344\ncls id: 340, Confidence: 0.98893, class_name: zebra\nroot@ubuntu:/app/pydev_demo/01_basic_sample# \n"})}),"\n",(0,o.jsx)(n.h2,{id:"detailed-introduction",children:"Detailed Introduction"}),"\n",(0,o.jsx)(n.h3,{id:"example-program-parameter-options-description",children:"Example Program Parameter Options Description"}),"\n",(0,o.jsx)(n.p,{children:"The basic classification example does not require command line parameters and can be run directly. The program will automatically load the zebra_cls.jpg image in the same directory for inference."}),"\n",(0,o.jsx)(n.h3,{id:"software-architecture-description",children:"Software Architecture Description"}),"\n",(0,o.jsx)(n.p,{children:"This example demonstrates the execution effects of different models through different Python code samples, but the software architecture is basically consistent. Therefore, it is explained uniformly here. The software architecture of the example program includes the following core parts:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Model Loading: Using hobot_dnn.pyeasy_dnn module to load precompiled model files"}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Image Preprocessing: Converting input images to NV12 format and specified dimensions required by the model"}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Model Inference: Calling the model for forward computation"}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Post-processing: Using libpostprocess library to parse inference results"}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Result Output: Outputting classification results and confidence scores"}),"\n"]}),"\n"]}),"\n",(0,o.jsx)("center",{children:(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/03_pydev_demo_sample/image/pydev_01_basic_software_arch.png",alt:"software_arch"})})}),"\n",(0,o.jsx)(n.h3,{id:"api-flow-description",children:"API Flow Description"}),"\n",(0,o.jsx)("center",{children:(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/03_pydev_demo_sample/image/pydev_01_basic_api_flow.png",alt:"API_Flow"})})}),"\n",(0,o.jsx)(n.h3,{id:"faq",children:"FAQ"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Q:"})," What should I do when encountering \"ModuleNotFoundError: No module named 'hobot_dnn'\" error while running the example?",(0,o.jsx)(n.br,{}),"\n",(0,o.jsx)(n.strong,{children:"A:"})," Please ensure that the RDK Python environment and official dedicated inference libraries such as the hobot_dnn module are properly installed."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Q:"})," How to change the test image?",(0,o.jsx)(n.br,{}),"\n",(0,o.jsx)(n.strong,{children:"A:"})," Place the new image file in the example directory and modify img_file = cv2.imread('image path') in the code."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Q:"})," What are the differences between different model files?",(0,o.jsx)(n.br,{}),"\n",(0,o.jsx)(n.strong,{children:"A:"})," Different models have differences in accuracy, inference speed, and model size. ResNet18 has higher accuracy but slightly slower speed, while MobileNetV1 is faster but with slightly lower accuracy. Choose the appropriate model according to actual requirements."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Q:"})," How to obtain other pre-trained models?",(0,o.jsx)(n.br,{}),"\n",(0,o.jsx)(n.strong,{children:"A:"})," You can refer to ",(0,o.jsx)(n.a,{href:"https://github.com/D-Robotics/rdk_model_zoo",children:"model_zoo repository"})," or ",(0,o.jsx)(n.a,{href:"https://github.com/D-Robotics/hobot_model",children:"toolchain's basic model repository"})]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var t=i(96540);const o={},s=t.createContext(o);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);