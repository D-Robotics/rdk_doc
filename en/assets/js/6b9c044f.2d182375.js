"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[20397],{28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var i=t(96540);const o={},r=i.createContext(o);function s(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(r.Provider,{value:n},e.children)}},72290:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"Basic_Application/pydev_demo_sample/centernet_sample","title":"3.3.6 CenterNet Example Introduction","description":"Example Overview","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/03_Basic_Application/03_pydev_demo_sample/11_centernet_sample.md","sourceDirName":"03_Basic_Application/03_pydev_demo_sample","slug":"/Basic_Application/pydev_demo_sample/centernet_sample","permalink":"/rdk_doc/en/Basic_Application/pydev_demo_sample/centernet_sample","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1768581630000,"sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"3.3.5 YOLOv5x Model Example Introduction","permalink":"/rdk_doc/en/Basic_Application/pydev_demo_sample/yolov5x_sample"},"next":{"title":"3.3.7 YOLOv5s v6/v7 Sample Introduction","permalink":"/rdk_doc/en/Basic_Application/pydev_demo_sample/yolov5s_v6_v7_sample"}}');var o=t(74848),r=t(28453);const s={sidebar_position:6},a="3.3.6 CenterNet Example Introduction",d={},c=[{value:"Example Overview",id:"example-overview",level:2},{value:"Result Display",id:"result-display",level:2},{value:"Hardware Preparation",id:"hardware-preparation",level:2},{value:"Hardware Connection",id:"hardware-connection",level:3},{value:"Quick Start",id:"quick-start",level:2},{value:"Code and Board Location",id:"code-and-board-location",level:3},{value:"Compilation and Execution",id:"compilation-and-execution",level:3},{value:"Execution Result",id:"execution-result",level:3},{value:"Detailed Introduction",id:"detailed-introduction",level:2},{value:"Example Program Parameter Options",id:"example-program-parameter-options",level:3},{value:"Software Architecture Description",id:"software-architecture-description",level:3},{value:"API Flow Description",id:"api-flow-description",level:3},{value:"FAQ",id:"faq",level:3}];function l(e){const n={br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"336-centernet-example-introduction",children:"3.3.6 CenterNet Example Introduction"})}),"\n",(0,o.jsx)(n.h2,{id:"example-overview",children:"Example Overview"}),"\n",(0,o.jsxs)(n.p,{children:["The CenterNet object detection example is a ",(0,o.jsx)(n.strong,{children:"Python interface"})," development code sample located in ",(0,o.jsx)(n.code,{children:"/app/pydev_demo/11_centernet_sample/"}),", demonstrating how to use the CenterNet model for efficient object detection tasks. CenterNet is an object detection algorithm based on center point prediction. Compared to traditional anchor-based methods, it has a simpler architecture and higher detection accuracy, making it particularly suitable for scenarios requiring precise localization and recognition of small objects."]}),"\n",(0,o.jsx)(n.h2,{id:"result-display",children:"Result Display"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/03_pydev_demo_sample/image/pydev_11_runing.png",alt:"output-img"})}),"\n",(0,o.jsx)(n.h2,{id:"hardware-preparation",children:"Hardware Preparation"}),"\n",(0,o.jsx)(n.h3,{id:"hardware-connection",children:"Hardware Connection"}),"\n",(0,o.jsxs)(n.p,{children:["This example only requires the RDK development board itself, without any additional peripheral connections. Ensure the development board is properly powered and the system is booted.\n",(0,o.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/03_pydev_demo_sample/image/pydev_11_hw_connect.png",alt:"connect-img"})]}),"\n",(0,o.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,o.jsx)(n.h3,{id:"code-and-board-location",children:"Code and Board Location"}),"\n",(0,o.jsxs)(n.p,{children:["Navigate to the ",(0,o.jsx)(n.code,{children:"/app/pydev_demo/11_centernet_sample/"})," directory. The CenterNet example contains the following files:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"root@ubuntu:/app/pydev_demo/11_centernet_sample# tree\n.\n\u251c\u2500\u2500 kite.jpg\n\u2514\u2500\u2500 test_centernet.py\n"})}),"\n",(0,o.jsx)(n.h3,{id:"compilation-and-execution",children:"Compilation and Execution"}),"\n",(0,o.jsx)(n.p,{children:"The Python example does not require compilation and can be run directly:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"python3 test_centernet.py\n"})}),"\n",(0,o.jsx)(n.h3,{id:"execution-result",children:"Execution Result"}),"\n",(0,o.jsx)(n.p,{children:"After running, the program will load the pre-trained CenterNet model, perform object detection on the kite.jpg image, and generate a result image output_image.jpg with detection boxes."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"root@ubuntu:/app/pydev_demo/11_centernet_sample# ./test_centernet.py \n[BPU_PLAT]BPU Platform Version(1.3.6)!\n[HBRT] set log level as 0. version = 3.15.55.0\n[DNN] Runtime version = 1.24.5_(3.15.55 HBRT)\n[A][DNN][packed_model.cpp:247][Model](2000-01-01,09:04:41.531.16) [HorizonRT] The model builder version = 1.23.5\n[W][DNN]bpu_model_info.cpp:491][Version](2000-01-01,09:04:41.900.505) Model: centernet_resnet101_512x512_nv12. Inconsistency between the hbrt library version 3.15.55.0 and the model build version 3.15.47.0 detected, in order to ensure correct model results, it is recommended to use compilation tools and the BPU SDK from the same OpenExplorer package.\ntensor type: NV12\ndata type: uint8\nlayout: NCHW\nshape: (1, 3, 512, 512)\n3\ntensor type: int16\ndata type: int16\nlayout: NCHW\nshape: (1, 80, 128, 128)\ntensor type: int32\ndata type: int32\nlayout: NCHW\nshape: (1, 2, 128, 128)\ntensor type: int32\ndata type: int32\nlayout: NCHW\nshape: (1, 2, 128, 128)\ninferece time is : 0.038387179374694824\npostprocess time is : 0.008000016212463379\nbbox: [535.099487, 518.289795, 552.85321, 533.168884], score: 0.411767, id: 0, name: person\nbbox: [1205.362183, 452.914368, 1213.579956, 462.972992], score: 0.416783, id: 0, name: person\nbbox: [37.22504, 512.7771, 55.708, 551.758057], score: 0.479478, id: 0, name: person\nbbox: [302.082428, 373.24588, 326.903992, 406.137909], score: 0.481639, id: 33, name: kite\nbbox: [79.558655, 511.698425, 104.828987, 561.18573], score: 0.483801, id: 0, name: person\nbbox: [763.340332, 381.275391, 773.633484, 388.304504], score: 0.49954, id: 33, name: kite\nbbox: [512.4505, 506.076019, 535.645386, 527.521606], score: 0.50862, id: 0, name: person\nbbox: [1083.63208, 398.408325, 1101.694458, 420.525391], score: 0.560764, id: 33, name: kite\nbbox: [578.292786, 346.042908, 599.692627, 366.190063], score: 0.561831, id: 33, name: kite\nbbox: [470.628357, 341.963165, 484.707916, 356.737732], score: 0.599044, id: 33, name: kite\nbbox: [176.473038, 539.143616, 190.889175, 567.11084], score: 0.602763, id: 0, name: person\nbbox: [116.152634, 617.276489, 164.758057, 756.843872], score: 0.655859, id: 0, name: person\nbbox: [345.088379, 485.373199, 357.569305, 505.430756], score: 0.656233, id: 0, name: person\nbbox: [593.67334, 80.689156, 670.185425, 148.085022], score: 0.668426, id: 33, name: kite\nbbox: [214.575424, 696.642883, 276.78363, 853.193604], score: 0.709791, id: 0, name: person\nbbox: [278.955109, 234.4608, 304.618103, 279.500824], score: 0.716334, id: 33, name: kite\ndraw result time is : 0.036167144775390625\ndet.size(): 16root@ubuntu:/app/pydev_demo/11_centernet_sample# \n"})}),"\n",(0,o.jsx)(n.h2,{id:"detailed-introduction",children:"Detailed Introduction"}),"\n",(0,o.jsx)(n.h3,{id:"example-program-parameter-options",children:"Example Program Parameter Options"}),"\n",(0,o.jsx)(n.p,{children:"The CenterNet object detection example does not require command-line parameters and can be run directly. The program will automatically load the kite.jpg image in the same directory for detection processing."}),"\n",(0,o.jsx)(n.h3,{id:"software-architecture-description",children:"Software Architecture Description"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"The software architecture of the CenterNet object detection example includes the following core parts:"}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Model Loading: Use the pyeasy_dnn module to load the pre-trained CenterNet-ResNet101 model"}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Image Preprocessing: Convert the input image to the NV12 format and specified size (512x512) required by the model"}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Model Inference: Call the model for forward computation to generate heatmaps, size, and offset predictions"}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Post-processing: Use the libpostprocess library to parse the model outputs and generate detection results"}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Result Visualization: Draw detection boxes and category information on the original image"}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Result Saving: Save the visualized result as an image file"}),"\n"]}),"\n"]}),"\n",(0,o.jsx)("center",{children:(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/03_pydev_demo_sample/image/pydev_11_centernet_sample_software_arch.png",alt:"software_arch"})})}),"\n",(0,o.jsx)(n.h3,{id:"api-flow-description",children:"API Flow Description"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Model Loading: models = dnn.load('../models/centernet_resnet101_512x512_nv12.bin')"}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Image Preprocessing: Resize the image and convert it to NV12 format"}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Model Inference: outputs = models[0].forward(nv12_data)"}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Post-processing Configuration: Set post-processing parameters (size, threshold, etc.)"}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Result Parsing: Call the post-processing library to parse the output tensors"}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Result Visualization: Draw detection boxes and label information on the original image"}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Result Saving: Save the result as an image file"}),"\n"]}),"\n"]}),"\n",(0,o.jsx)("center",{children:(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/03_pydev_demo_sample/image/pydev_11_centernet_sample_api_flow.png",alt:"API_Flow"})})}),"\n",(0,o.jsx)(n.h3,{id:"faq",children:"FAQ"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Q:"})," What is the difference between CenterNet and YOLO?",(0,o.jsx)(n.br,{}),"\n",(0,o.jsx)(n.strong,{children:"A:"})," CenterNet uses center point prediction instead of anchor boxes, offering a simpler architecture and more precise localization capabilities, especially suitable for small object detection."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Q:"})," What should I do if I encounter a \"No module named 'hobot_dnn'\" error when running the example?",(0,o.jsx)(n.br,{}),"\n",(0,o.jsx)(n.strong,{children:"A:"})," Please ensure that the RDK Python environment is correctly installed, including the hobot_dnn module and other official dedicated inference libraries."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Q:"})," How can I change the test image?",(0,o.jsx)(n.br,{}),"\n",(0,o.jsx)(n.strong,{children:"A:"})," Place the new image file in the example directory and modify the code: img_file = cv2.imread('your_image_path')."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Q:"})," What should I do if the detection results are inaccurate?",(0,o.jsx)(n.br,{}),"\n",(0,o.jsx)(n.strong,{children:"A:"})," The CenterNet model is trained on the COCO dataset. For specific scenarios, fine-tuning or using a more suitable model may be necessary."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Q:"})," How can I adjust the detection threshold?",(0,o.jsx)(n.br,{}),"\n",(0,o.jsx)(n.strong,{children:"A:"})," Modify the value of centernet_postprocess_info.score_threshold in the code. For example, changing it to 0.5 can increase detection sensitivity."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Q:"})," Can real-time video stream processing be implemented?",(0,o.jsx)(n.br,{}),"\n",(0,o.jsx)(n.strong,{children:"A:"})," The current example is designed for single images, but the code can be modified to achieve real-time object detection for video streams."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Q:"})," What advantages does CenterNet have in handling small objects?",(0,o.jsx)(n.br,{}),"\n",(0,o.jsx)(n.strong,{children:"A:"})," Through center point prediction and fine offset adjustments, CenterNet can more accurately locate small objects, avoiding the mismatch issue between anchor boxes and small objects in anchor-based methods."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Q:"})," How can detection accuracy be further improved?",(0,o.jsx)(n.br,{}),"\n",(0,o.jsx)(n.strong,{children:"A:"})," You can try using larger input sizes (if supported by the model), fine-tuning the model for specific scenarios, or adjusting post-processing parameters to optimize detection results."]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}}}]);