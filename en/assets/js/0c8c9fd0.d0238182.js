"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[99894],{28453:(n,e,t)=>{t.d(e,{R:()=>r,x:()=>c});var o=t(96540);const a={},i=o.createContext(a);function r(n){const e=o.useContext(i);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function c(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:r(n.components),o.createElement(i.Provider,{value:e},n.children)}},34007:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>s,contentTitle:()=>c,default:()=>_,frontMatter:()=>r,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"Advanced_development/toolchain_development/expert/quick_start","title":"Quick Start","description":"quick_start}","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/07_Advanced_development/04_toolchain_development/expert/quick_start.md","sourceDirName":"07_Advanced_development/04_toolchain_development/expert","slug":"/Advanced_development/toolchain_development/expert/quick_start","permalink":"/rdk_doc/en/Advanced_development/toolchain_development/expert/quick_start","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1756289213000,"sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Environment Dependency","permalink":"/rdk_doc/en/Advanced_development/toolchain_development/expert/environment_config"},"next":{"title":"Development Guide","permalink":"/rdk_doc/en/Advanced_development/toolchain_development/expert/user_guide"}}');var a=t(74848),i=t(28453);const r={sidebar_position:2},c="Quick Start {#quick_start}",s={},l=[{value:"Basic Process",id:"basic-process",level:2},{value:"Get Float Model",id:"get-float-model",level:2},{value:"Calibration",id:"Calibration",level:2},{value:"Quantization-Aware Training",id:"quantization-aware-training",level:2},{value:"Convert to Fixed-Point Model",id:"convert-to-fixed-point-model",level:2},{value:"Model Deployment",id:"model-deployment",level:2}];function d(n){const e={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"quick_start",children:"Quick Start"})}),"\n",(0,a.jsxs)(e.p,{children:["The D-Robotics Plugin Pytorch (referred to as Plugin) refers to the PyTorch official quantization interface and ideas. The Plugin adopts the Quantization Aware Training (QAT) scheme. Therefore, it is recommended that users first read the relevant parts of the ",(0,a.jsx)(e.a,{href:"https://pytorch.org/docs/stable/quantization.html#quantization",children:(0,a.jsx)(e.strong,{children:"PyTorch official documentation"})})," and be familiar with the usage of quantization training and deployment tools provided by PyTorch."]}),"\n",(0,a.jsx)(e.h2,{id:"basic-process",children:"Basic Process"}),"\n",(0,a.jsx)(e.p,{children:"The basic process of using the quantization training tool is as follows:"}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/expert/quick_start.svg",alt:"quick_start"})}),"\n",(0,a.jsxs)(e.p,{children:["Below, we take the ",(0,a.jsx)(e.code,{children:"MobileNetV2"})," model in ",(0,a.jsx)(e.code,{children:"torchvision"})," as an example to introduce the specific operations of each stage in the process."]}),"\n",(0,a.jsxs)(e.p,{children:["For the sake of execution speed in the process demonstration, we use the ",(0,a.jsx)(e.code,{children:"cifar-10"})," dataset instead of the ImageNet-1K dataset."]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'\n    import os\n    import copy\n    import numpy as np\n    import torch\n    import torch.nn as nn\n    import torchvision.transforms as transforms\n    from torch import Tensor\n    from torch.quantization import DeQuantStub\n    from torchvision.datasets import CIFAR10\n    from torchvision.models.mobilenetv2 import MobileNetV2\n    from torch.utils import data\n    from typing import Optional, Callable, List, Tuple\n\n    from horizon_plugin_pytorch.functional import rgb2centered_yuv\n\n    import torch.quantization\n    from horizon_plugin_pytorch.march import March, set_march\n    from horizon_plugin_pytorch.quantization import (\n        QuantStub,\n        convert_fx,\n        prepare_qat_fx,\n        set_fake_quantize,\n        FakeQuantState,\n        check_model,\n        compile_model,\n        perf_model,\n        visualize_model,\n    )\n    from horizon_plugin_pytorch.quantization.qconfig import (\n        default_calib_8bit_fake_quant_qconfig,\n        default_qat_8bit_fake_quant_qconfig,\n        default_qat_8bit_weight_32bit_out_fake_quant_qconfig,\n        default_calib_8bit_weight_32bit_out_fake_quant_qconfig,\n    )\n\n    import logging\n    logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")\n'})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-shell",children:"\n    2023-06-29 14:46:09,502] WARNING: `fx_force_duplicate_shared_convbn` will be set False by default after plugin 1.9.0. If you are not loading old checkpoint, please set `fx_force_duplicate_shared_convbn` False to train your new model.\n    2023-06-29 14:46:09,575] WARNING: due to bug of torch.quantization.fuse_modules, it is replaced by horizon.quantization.fuse_modules\n\n"})}),"\n",(0,a.jsx)(e.h2,{id:"get-float-model",children:"Get Float Model"}),"\n",(0,a.jsx)(e.p,{children:"First, make necessary modifications to the float model to support quantization-related operations. The necessary operations for modifying the model are:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:["Insert a ",(0,a.jsx)(e.code,{children:"QuantStub"})," before the model input."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:["Insert a ",(0,a.jsx)(e.code,{children:"DequantStub"})," after the model output."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"When modifying the model, please note:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:["The inserted ",(0,a.jsx)(e.code,{children:"QuantStub"})," and ",(0,a.jsx)(e.code,{children:"DequantStub"})," must be registered as sub-modules of the model, otherwise their quantization states will not be handled correctly."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:["Multiple inputs can share the same ",(0,a.jsx)(e.code,{children:"QuantStub"})," only when their scales are the same. Otherwise, please define a separate ",(0,a.jsx)(e.code,{children:"QuantStub"})," for each input."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:['If you need to specify the data source of the input data when on board as "pyramid", please manually set the ',(0,a.jsx)(e.code,{children:"scale"})," parameter of the corresponding ",(0,a.jsx)(e.code,{children:"QuantStub"})," to 1/128."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:["You can also use ",(0,a.jsx)(e.code,{children:"torch.quantization.QuantStub"}),", but only ",(0,a.jsx)(e.code,{children:"horizon_plugin_pytorch.quantization.QuantStub"})," supports manually fixing the scale through parameters."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"The modified model can seamlessly load the parameters of the original model, so if there is an already trained float model, you can load it directly. Otherwise, you need to train it normally in float."}),"\n",(0,a.jsx)(e.admonition,{title:"Note",type:"caution",children:(0,a.jsxs)(e.p,{children:["The input image data when the model is on board is generally in the format of centered_yuv444. Therefore, when training the model, the image needs to be converted to the centered_yuv444 format (pay attention to the use of ",(0,a.jsx)(e.code,{children:"rgb2centered_yuv"})," in the code below).\nIf it is not possible to convert to centered_yuv444 format for model training, please refer to the ",(0,a.jsx)(e.strong,{children:"RGB888 Data Deployment"})," section for instructions on how to modify the model accordingly. (Note that this method may lead to a decrease in model accuracy)\nIn this example, the number of epochs for float and QAT training is small, just to illustrate the usage of the training tool, and the accuracy does not represent the best performance of the model."]})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'\n    ######################################################################\n    # Users can modify the following parameters as needed\n    # 1. Paths for saving model checkpoints and compilation artifacts\n    model_path = "model/mobilenetv2"# 2. Data set download and save path\n    data_path = "data"\n    # 3. Batch size used during training\n    train_batch_size = 256\n    # 4. Batch size used during prediction\n    eval_batch_size = 256\n    # 5. Number of epochs for training\n    epoch_num = 30\n    # 6. Device used for model saving and computation\n    device = (\n        torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")\n    )\n    ######################################################################\n\n\n    # Prepare data loaders, please note the use of rgb2centered_yuv in collate_fn\n    def prepare_data_loaders(\n        data_path: str, train_batch_size: int, eval_batch_size: int\n    ) -> Tuple[data.DataLoader, data.DataLoader]:\n        normalize = transforms.Normalize(mean=0.0, std=128.0)\n\n        def collate_fn(batch):\n            batched_img = torch.stack(\n                [\n                    torch.from_numpy(np.array(example[0], np.uint8, copy=True))\n                    for example in batch\n                ]\n            ).permute(0, 3, 1, 2)\n            batched_target = torch.tensor([example[1] for example in batch])\n\n            batched_img = rgb2centered_yuv(batched_img)\n            batched_img = normalize(batched_img.float())\n\n            return batched_img, batched_target\n\n        train_dataset = CIFAR10(\n            data_path,\n            True,\n            transforms.Compose(\n                [\n                    transforms.RandomHorizontalFlip(),\n                    transforms.RandAugment(),\n                ]\n            ),\n            download=True,\n        )\n\n        eval_dataset = CIFAR10(\n            data_path,\n            False,        download=True,\n    )\n\n    train_data_loader = data.DataLoader(\n        train_dataset,\n        batch_size=train_batch_size,\n        sampler=data.RandomSampler(train_dataset),\n        num_workers=8,\n        collate_fn=collate_fn,\n        pin_memory=True,\n    )\n\n    eval_data_loader = data.DataLoader(\n        eval_dataset,\n        batch_size=eval_batch_size,\n        sampler=data.SequentialSampler(eval_dataset),\n        num_workers=8,\n        collate_fn=collate_fn,\n        pin_memory=True,\n    )\n\n    return train_data_loader, eval_data_loader\n\n\n# Make necessary modifications to the floating-point model\nclass FxQATReadyMobileNetV2(MobileNetV2):\n    def __init__(\n        self,\n        num_classes: int = 10,\n        width_mult: float = 1.0,\n        inverted_residual_setting: Optional[List[List[int]]] = None,\n        round_nearest: int = 8,\n    ):\n        super().__init__(\n            num_classes, width_mult, inverted_residual_setting, round_nearest\n        )\n        self.quant = QuantStub(scale=1 / 128)\n        self.dequant = DeQuantStub()\n\n    def forward(self, x: Tensor) -> Tensor:\n        x = self.quant(x)\n        x = super().forward(x)\n        x = self.dequant(x)\n\n        return x\n\n\nif not os.path.exists(model_path):\n    os.makedirs(model_path, exist_ok=True)# Float model initialization\n    float_model = FxQATReadyMobileNetV2()\n\n    # Prepare dataset\n    train_data_loader, eval_data_loader = prepare_data_loaders(\n        data_path, train_batch_size, eval_batch_size\n    )\n\n    # Since the last layer of the model is inconsistent with the pretrained model, float finetuning is needed\n    optimizer = torch.optim.Adam(\n        float_model.parameters(), lr=0.001, weight_decay=1e-3\n    )\n    best_acc = 0\n\n    for nepoch in range(epoch_num):\n        float_model.train()\n        train_one_epoch(\n            float_model,\n            nn.CrossEntropyLoss(),\n            optimizer,\n            None,\n            train_data_loader,\n            device,\n        )\n\n        # Float precision testing\n        float_model.eval()\n        top1, top5 = evaluate(float_model, eval_data_loader, device)\n\n        print(\n            "Float Epoch {}: evaluation Acc@1 {:.3f} Acc@5 {:.3f}".format(\n                nepoch, top1.avg, top5.avg\n            )\n        )\n\n        if top1.avg > best_acc:\n            best_acc = top1.avg\n            # Save the best parameters of the float model\n            torch.save(\n                float_model.state_dict(),\n                os.path.join(model_path, "float-checkpoint.ckpt"),\n            )\n\n'})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-shell",children:"\n    Files already downloaded and verified\n    Files already downloaded and verified\n    ....................................................................................................................................................................................................Full cifar-10 train set: Loss 2.116 Acc@1 20.744 Acc@5 70.668\n    ........................................\n    Float Epoch 0: evaluation Acc@1 34.140 Acc@5 87.330\n    ....................................................................................................................................................................................................\n    Full cifar-10 train set: Loss 1.815 Acc@1 32.464 Acc@5 84.110\n    ........................................\n    Float Epoch 1: evaluation Acc@1 42.770 Acc@5 90.560\n    ....................................................................................................................................................................................................\n    Full cifar-10 train set: Loss 1.682 Acc@1 38.276 Acc@5 87.374\n    ........................................\n    Float Epoch 2: evaluation Acc@1 45.810 Acc@5 91.240\n    ....................................................................................................................................................................................................\n    Full cifar-10 train set: Loss 1.581 Acc@1 42.676 Acc@5 89.224\n    ........................................\n    Float Epoch 3: evaluation Acc@1 50.070 Acc@5 92.620\n    ....................................................................................................................................................................................................\n    Full cifar-10 train set: Loss 1.495 Acc@1 45.882 Acc@5 90.668\n    ........................................\n    Float Epoch 4: evaluation Acc@1 53.860 Acc@5 93.690\n    ....................................................................................................................................................................................................\n    Full cifar-10 train set: Loss 1.413 Acc@1 49.274 Acc@5 91.892\n    ........................................\n    Float Epoch 5: evaluation Acc@1 51.230 Acc@5 94.370\n    ....................................................................................................................................................................................................\n    Full cifar-10 train set: Loss 1.339 Acc@1 52.488 Acc@5 92.760\n    ........................................\n    Float Epoch 6: evaluation Acc@1 58.460 Acc@5 95.450\n    ....................................................................................................................................................................................................\n    Full cifar-10 train set: Loss 1.269 Acc@1 54.710 Acc@5 93.702\n    ........................................\n    Float Epoch 7: evaluation Acc@1 59.870 Acc@5 95.260\n    ....................................................................................................................................................................................................\n    Full cifar-10 train set: Loss 1.208 Acc@1 57.170 Acc@5 94.258\n    ........................................\n    Float Epoch 8: evaluation Acc@1 60.040 Acc@5 95.870\n    ....................................................................................................................................................................................................\n    Full cifar-10 train set: Loss 1.147 Acc@1 59.420 Acc@5 95.150\n    ........................................\n    Float Epoch 9: evaluation Acc@1 61.370 Acc@5 96.830\n    ....................................................................................................................................................................................................\n    Full cifar-10 train set: Loss 1.098 Acc@1 61.652 Acc@5 95.292\n    ........................................\n    Float Epoch 10: evaluation Acc@1 66.410 Acc@5 96.910\n    ....................................................................................................................................................................................................\n    Full cifar-10 train set: Loss 1.060 Acc@1 62.902 Acc@5 95.758\n    ........................................\n    Float Epoch 11: evaluation Acc@1 67.900 Acc@5 96.660\n    ....................................................................................................................................................................................................\n    Full cifar-10 train set: Loss 1.013 Acc@1 64.606 Acc@5 96.250\n    ........................................Float Epoch 12: evaluation Acc@1 69.120 Acc@5 97.180\n    ....................................................................................................................................................................................................\n    Full cifar-10 train set: Loss 0.980 Acc@1 65.954 Acc@5 96.486\n    ........................................\n    Float Epoch 13: evaluation Acc@1 70.410 Acc@5 97.420\n    ....................................................................................................................................................................................................\n    Full cifar-10 train set: Loss 0.944 Acc@1 67.002 Acc@5 96.792\n    ........................................\n    Float Epoch 14: evaluation Acc@1 71.200 Acc@5 97.410\n    ....................................................................................................................................................................................................\n    Full cifar-10 train set: Loss 0.915 Acc@1 68.024 Acc@5 96.896\n    ........................................\n    Float Epoch 15: evaluation Acc@1 72.570 Acc@5 97.780\n    ....................................................................................................................................................................................................\n    Full cifar-10 train set: Loss 0.892 Acc@1 69.072 Acc@5 97.062\n    ........................................\n    Float Epoch 16: evaluation Acc@1 72.950 Acc@5 98.020\n    ....................................................................................................................................................................................................\n    Full cifar-10 train set: Loss 0.868 Acc@1 70.072 Acc@5 97.234\n    ........................................\n    Float Epoch 17: evaluation Acc@1 75.020 Acc@5 98.230\n    ....................................................................................................................................................................................................\n    Full cifar-10 train set: Loss 0.850 Acc@1 70.544 Acc@5 97.384\n    ........................................\n    Float Epoch 18: evaluation Acc@1 74.870 Acc@5 98.140\n    ....................................................................................................................................................................................................\n    Full cifar-10 train set: Loss 0.826 Acc@1 71.334 Acc@5 97.476\n    ........................................\n    Float Epoch 19: evaluation Acc@1 74.700 Acc@5 98.090\n    ....................................................................................................................................................................................................\n    Full cifar-10 train set: Loss 0.817 Acc@1 71.988 Acc@5 97.548\n    ........................................\n    Float Epoch 20: evaluation Acc@1 75.690 Acc@5 98.140\n    ....................................................................................................................................................................................................\n    Full cifar-10 train set: Loss 0.796 Acc@1 72.530 Acc@5 97.734\n    ........................................\n    Float Epoch 21: evaluation Acc@1 76.500 Acc@5 98.470\n..............................................................\n    Full cifar-10 train set: Loss 0.786 Acc@1 72.754 Acc@5 97.770\n........................................\nFloat Epoch 22: evaluation Acc@1 76.200 Acc@5 98.290\n    ....................................................................................................................................................................................................\n    Full cifar-10 train set: Loss 0.772 Acc@1 73.392 Acc@5 97.802\n........................................\nFloat Epoch 23: evaluation Acc@1 74.800 Acc@5 98.640\n    ....................................................................................................................................................................................................\n    Full cifar-10 train set: Loss 0.753 Acc@1 73.982 Acc@5 97.914\n........................................\nFloat Epoch 24: evaluation Acc@1 77.150 Acc@5 98.490\n    ....................................................................................................................................................................................................Full cifar-10 train set: Loss 0.741 Acc@1 74.278 Acc@5 98.038\n........................................\nFloat Epoch 25: evaluation Acc@1 77.270 Acc@5 98.690\n....................................................................................................................................................................................................\nFull cifar-10 train set: Loss 0.737 Acc@1 74.582 Acc@5 97.916\n........................................\nFloat Epoch 26: evaluation Acc@1 77.050 Acc@5 98.580\n....................................................................................................................................................................................................\nFull cifar-10 train set: Loss 0.725 Acc@1 75.254 Acc@5 98.038\n........................................\nFloat Epoch 27: evaluation Acc@1 79.120 Acc@5 98.620\n....................................................................................................................................................................................................\nFull cifar-10 train set: Loss 0.714 Acc@1 75.290 Acc@5 98.230\n........................................\nFloat Epoch 28: evaluation Acc@1 78.060 Acc@5 98.550\n....................................................................................................................................................................................................\nFull cifar-10 train set: Loss 0.711 Acc@1 75.662 Acc@5 98.218\n........................................\nFloat Epoch 29: evaluation Acc@1 77.580 Acc@5 98.610\n"})}),"\n",(0,a.jsx)(e.h2,{id:"Calibration",children:"Calibration"}),"\n",(0,a.jsx)(e.p,{children:"After the model transformation is complete and the floating-point training is completed, calibration can be performed. In this process, the distribution of data is calculated by inserting observers into the model during the forward process, in order to calculate the appropriate quantization parameters:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:"For some models, only calibration is necessary to achieve the required accuracy, and there is no need to perform time-consuming quantization-aware training."}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:"Even if the calibrated model does not meet the precision requirements, this process can reduce the difficulty of subsequent quantization-aware training, shorten the training time, and improve the final training accuracy."}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'\n    ######################################################################\n    # Users can modify the following parameters as needed\n    # 1. Batch size used during calibration\n    calib_batch_size = 256\n    # 2. Batch size used during validation\n    eval_batch_size = 256\n    # 3. Number of examples used during calibration, set to inf to use all data\n    num_examples = float("inf")\n    # 4. The code for the target hardware platform\n    march = March.BAYES\n    ######################################################################\n\n    # Set the hardware platform that the model will run on before the model transformation\n    set_march(march)\n\n\n    # Convert the model to calibration state to calculate the numerical distribution characteristics of data at each location\n    calib_model = prepare_qat_fx(# The output model will share the attributes of the input model to avoid affecting the subsequent use of float_model. A \'deepcopy\' is performed here.\n\n    copy.deepcopy(float_model),\n    {\n        "": default_calib_8bit_fake_quant_qconfig,\n        "module_name": {\n            # When the output layer of the model is Conv or Linear, \'out_qconfig\' can be used to configure high-precision output.\n            "classifier": default_calib_8bit_weight_32bit_out_fake_quant_qconfig,\n        },\n    },\n    ).to(\n        device\n    )  # The \'prepare_qat_fx\' interface does not guarantee that the output model\'s device is completely consistent with the input model.\n\n    # Prepare the dataset\n    calib_data_loader, eval_data_loader = prepare_data_loaders(\n        data_path, calib_batch_size, eval_batch_size\n    )\n\n    # Perform the Calibration process (no need for backward)\n    # Note the control over the model\'s state, the model needs to be in \'eval\' state to ensure that the behavior of BatchNorm matches the requirements.\n    calib_model.eval()\n    set_fake_quantize(calib_model, FakeQuantState.CALIBRATION)\n    with torch.no_grad():\n        cnt = 0\n        for image, target in calib_data_loader:\n            image, target = image.to(device), target.to(device)\n            calib_model(image)\n            print(".", end="", flush=True)\n            cnt += image.size(0)\n            if cnt >= num_examples:\n                break\n        print()\n\n    # Test the precision of fake quantization\n    # Note the control over the model\'s state\n    calib_model.eval()\n    set_fake_quantize(calib_model, FakeQuantState.VALIDATION)\n\n    top1, top5 = evaluate(\n        calib_model,\n        eval_data_loader,\n        device,\n    )\n    print(\n        "Calibration: evaluation Acc@1 {:.3f} Acc@5 {:.3f}".format(\n            top1.avg, top5.avg\n        )\n    )# Save Calibration Model Parameters\n    torch.save(\n        calib_model.state_dict(),\n        os.path.join(model_path, "calib-checkpoint.ckpt"),\n    )\n\n'})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-shell",children:"    Files already downloaded and verified\n    Files already downloaded and verified\n\n    /home/users/yushu.gao/horizon/qat_logger/horizon_plugin_pytorch/quantization/observer_v2.py:405: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:568.)\n    min_val_cur, max_val_cur = torch._aminmax(\n    /home/users/yushu.gao/horizon/qat_logger/horizon_plugin_pytorch/quantization/observer_v2.py:672: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at ../aten/src/ATen/native/ReduceAllOps.cpp:45.)\n    min_val_cur, max_val_cur = torch._aminmax(x)\n\n    ....................................................................................................................................................................................................\n    ........................................\n    Calibration: evaluation Acc@1 77.890 Acc@5 98.640\n\n"})}),"\n",(0,a.jsxs)(e.p,{children:["If the quantization accuracy of the model after calibration already meets the requirements, you can directly proceed to the next step of ",(0,a.jsx)(e.strong,{children:"converting the model to fixed point"}),", otherwise, you need to perform ",(0,a.jsx)(e.strong,{children:"quantization-aware training"})," to further improve the accuracy."]}),"\n",(0,a.jsx)(e.h2,{id:"quantization-aware-training",children:"Quantization-Aware Training"}),"\n",(0,a.jsx)(e.p,{children:"Quantization-aware training inserts fake quantization nodes into the model to make the model aware of the impact of quantization during the training process. In this case, the model parameters are fine-tuned to improve the accuracy after quantization."}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'\n    ######################################################################\n    # The following parameters can be modified according to your needs\n    # 1. Batch size used during training\n    train_batch_size = 256\n    # 2. Batch size used during validation\n    eval_batch_size = 256\n    # 3. Number of epochs for training\n    epoch_num = 3\n    ######################################################################\n\n    # Prepare the data loaders\n    train_data_loader, eval_data_loader = prepare_data_loaders(\n        data_path, train_batch_size, eval_batch_size\n    )\n\n    # Convert the model to QAT state\n    qat_model = prepare_qat_fx(\n    copy.deepcopy(float_model),\n    {\n        "": default_qat_8bit_fake_quant_qconfig,\n        "module_name": {\n            "classifier": default_qat_8bit_weight_32bit_out_fake_quant_qconfig,\n        },\n    },\n    ).to(device)\n\n    # Load quantization parameters from calibration model\n    qat_model.load_state_dict(calib_model.state_dict())\n\n    # Perform Quantization-Aware Training (QAT)\n    # As a fine-tuning process, QAT generally requires a smaller learning rate\n    optimizer = torch.optim.Adam(\n        qat_model.parameters(), lr=1e-3, weight_decay=1e-4\n    )\n\n    best_acc = 0\n\n    for nepoch in range(epoch_num):\n        # Control QAT model\'s training mode\n        qat_model.train()\n        set_fake_quantize(qat_model, FakeQuantState.QAT)\n\n        train_one_epoch(\n            qat_model,\n            nn.CrossEntropyLoss(),\n            optimizer,\n            None,\n            train_data_loader,\n            device,\n        )\n\n        # Control QAT model\'s evaluation mode\n        qat_model.eval()\n        set_fake_quantize(qat_model, FakeQuantState.VALIDATION)\n\n        top1, top5 = evaluate(\n            qat_model,\n            eval_data_loader,\n            device,\n        )\n        print(\n            "QAT Epoch {}: evaluation Acc@1 {:.3f} Acc@5 {:.3f}".format(\n                nepoch, top1.avg, top5.avg\n            )\n        )\n\n        if top1.avg > best_acc:best_acc = top1.avg\n\n    torch.save(\n        qat_model.state_dict(),\n        os.path.join(model_path, "qat-checkpoint.ckpt"),\n    )\n\n'})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-shell",children:"\n    Files already downloaded and verified\n    Files already downloaded and verified\n    ...............................................................\n    Full cifar-10 train set: Loss 0.759 Acc@1 73.692 Acc@5 97.940\n    ........................................\n    QAT Epoch 0: evaluation Acc@1 79.170 Acc@5 98.490\n    ...............................................................\n    Full cifar-10 train set: Loss 0.718 Acc@1 75.414 Acc@5 97.998\n    ........................................\n    QAT Epoch 1: evaluation Acc@1 78.540 Acc@5 98.580\n    ...............................................................\n    Full cifar-10 train set: Loss 0.719 Acc@1 75.180 Acc@5 98.126\n    ........................................\n    QAT Epoch 2: evaluation Acc@1 78.200 Acc@5 98.540\n\n"})}),"\n",(0,a.jsx)(e.h2,{id:"convert-to-fixed-point-model",children:"Convert to Fixed-Point Model"}),"\n",(0,a.jsx)(e.p,{children:"After the accuracy of the quantization model meets the requirement, the model can be converted to a fixed-point model. It is generally believed that the results of the fixed-point model and the compiled model are exactly the same."}),"\n",(0,a.jsx)(e.admonition,{title:"Note",type:"caution",children:(0,a.jsx)(e.p,{children:"Fixed-point models and quantized models cannot achieve exact numerical consistency, so please refer to the accuracy of the fixed-point model. If the fixed-point accuracy does not meet the requirements, further quantization training is needed."})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'\n    ######################################################################\n    # Users can modify the following parameters as needed\n    # 1. Use which model as the input of the process, you can choose calib_model or qat_model\n    base_model = qat_model\n    ######################################################################\n\n    # Convert the model to fixed-point mode\n    quantized_model = convert_fx(base_model).to(device)\n\n    # Test the accuracy of the fixed-point model\n    top1, top5 = evaluate(\n    quantized_model, \n    eval_data_loader, \n    device, \n    ) \n    print(\n        "Quantized model: evaluation Acc@1 {:.3f} Acc@5 {:.3f}".format(\n            top1.avg, top5.avg\n        )\n    )\n'})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-shell",children:"[2023-06-29 14:55:05,825] WARNING: AdaptiveAvgPool2d has not collected any statistics of activations and its scale is 1, please check whether this is intended!\n\n........................................\nQuantized model: evaluation Accuracy@1 78.390, Accuracy@5 98.640\n"})}),"\n",(0,a.jsx)(e.h2,{id:"model-deployment",children:"Model Deployment"}),"\n",(0,a.jsx)(e.p,{children:"After verifying the quantized model's accuracy and ensuring it meets requirements, proceed with deployment steps, including model inspection, compilation, performance testing, and visualization."}),"\n",(0,a.jsx)(e.admonition,{title:"Note",type:"caution",children:(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"If desired, you can skip the actual calibration and quantization-aware training processes and directly proceed with model inspection to ensure there are no issues preventing successful compilation."}),"\n",(0,a.jsx)(e.li,{children:"Since the compiler only supports CPUs, both the model and data must be on the CPU."}),"\n"]})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'######################################################################\n# Users can modify these parameters as needed\n# 1. Optimization level for compilation, can be 0-3; higher levels result in\n#    faster execution on the board but slower compilation times.\ncompile_opt = "O1"\n######################################################################\n\n# `example_input` can also be randomly generated data, but using real data is recommended\n# for more accurate performance testing\nexample_input = next(iter(eval_data_loader))[0]\n\n# Trace the model, serialize it, and generate the computation graph. Ensure the model and data are on CPU\nscript_model = torch.jit.trace(quantized_model.cpu(), example_input)\ntorch.jit.save(script_model, os.path.join(model_path, "int_model.pt"))\n\n# Model inspection\ncheck_model(script_model, [example_input])\n'})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-shell",children:"/home/users/yushu.gao/horizon/qat_logger/horizon_plugin_pytorch/qtensor.py:992: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\nif scale is not None and scale.numel() > 1:\n/home/users/yushu.gao/horizon/qat_logger/horizon_plugin_pytorch/nn/quantized/conv2d.py:290: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\nper_channel_axis=-1 if self.out_scale.numel() == 1 else 1,\n/home/users/yushu.gao/horizon/qat_logger/horizon_plugin_pytorch/nn/quantized/adaptive_avg_pool2d.py:30: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\nif (\n/home/users/yushu.gao/horizon/qat_logger/horizon_plugin_pytorch/utils/script_quantized_fn.py:224: UserWarning: operator() profile_node %59 : int[] = prim::profile_ivalue(%57)\ndoes not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)\nreturn compiled_fn(*args, **kwargs)\n"})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-shell",children:"This model is supported!\nHBDK model check PASS\n"})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'\n# Compile the model, the generated hbm file is the deployable model\ncompile_model(\n    script_model,\n    [example_input],\n    hbm=os.path.join(model_path, "model.hbm"),\n    input_source="pyramid",\n    opt=compile_opt,\n)\n'})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-shell",children:"\nINFO: launch 16 threads for optimization\n[==================================================] 100%\nWARNING: arg0 can not be assigned to NCHW_NATIVE layout because it's input source is pyramid/resizer.\nconsumed time 10.4302\nHBDK model compilation SUCCESS\n"})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'\n# Model performance testing\nperf_model(\n    script_model,\n[example_input],\nout_dir=os.path.join(model_path, "perf_out"),\ninput_source="pyramid",\nopt=compile_opt,\nlayer_details=True,\n)\n\n'})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-shell",children:"\nINFO: launch 16 threads for optimization\n[==================================================] 100%\nWARNING: arg0 can not be assigned to NCHW_NATIVE layout because it's input source is pyramid/resizer.\n"})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-shell",children:"\nconsumed time 10.3666\nHBDK model compilation SUCCESS\nFPS=5722.98, latency = 44731.9 us   (see model/mobilenetv2/perf_out/FxQATReadyMobileNetV2.html)\nHBDK model compilation SUCCESS\nHBDK performance estimation SUCCESS\n\n{'summary': {'BPU OPs per frame (effective)': 12249856,\n    'BPU OPs per run (effective)': 3135963136,\n    'BPU PE number': 1,\n    'BPU core number': 1,\n    'BPU march': 'BAYES',\n    'DDR bytes per frame': 1403592.0,\n    'DDR bytes per run': 359319552,\n    'DDR bytes per second': 8032734694,\n    'DDR megabytes per frame': 1.339,\n    'DDR megabytes per run': 342.674,\n    'DDR megabytes per second': 7660.6,\n    'FPS': 5722.98,\n    'HBDK version': '3.46.4',\n    'compiling options': '--march bayes -m /tmp/hbdktmp_ocro1_9_.hbir -f hbir --O1 -o /tmp/hbdktmp_ocro1_9_.hbm --jobs 16 -n FxQATReadyMobileNetV2 -i pyramid --input-name arg0 --output-layout NCHW --progressbar --debug',\n    'frame per run': 256,\n    'frame per second': 5722.98,\n    'input features': [['input name', 'input size'], ['arg0', '256x32x32x3']],\n    'interval computing unit utilization': [0.081,\n    0.113,\n    0.021,\n    0.001,\n    0.063,\n    0.004,\n    0.092,\n    0.019,\n    0.001,\n    0.065,\n    0.078,\n    0.11,\n    0.108,\n    0.235,\n    0.078,\n    0.179,\n    0.246,\n    0.219,\n    0.154,\n    0.046,\n    0.16,\n    0.108,\n    0.064,\n    0.099,\n    0.113,\n    0.153,\n    0.046,\n    0.052,\n    0.075,\n    0.041,\n    0.077,\n    0.081,\n    0.081,\n    0.06,\n    0.1,\n    0.304,\n    0.603,\n    0.521,\n    0.104,\n    0.11],\n    'interval computing units utilization': [0.081,\n    0.113,\n    0.021,\n    0.001,\n    0.063,\n    0.004,\n    0.092,\n    0.019,\n    0.001,\n    0.016,\n    0.053,\n    0.021,\n    0.001,\n    0.0930.065,\n    0.078,\n    0.11,\n    0.108,\n    0.235,\n    0.078,\n    0.179,\n    0.246,\n    0.219,\n    0.154,\n    0.046,\n    0.16,\n    0.108,\n    0.064,\n    0.099,\n    0.113,\n    0.153,\n    0.046,\n    0.052,\n    0.075,\n    0.041,\n    0.077,\n    0.081,\n    0.081,\n    0.06,\n    0.1,\n    0.304,\n    0.603,\n    0.521,\n    0.104,\n    0.11],\n    'interval loading bandwidth (megabytes/s)': [798,\n    2190,\n    3291,\n    3001,\n    4527,\n    6356,\n    5985,\n    4096,\n    3098,\n    5315,\n    5907,\n    3763,\n    2887,\n    4891,\n    6121,\n    4107,\n    2900,\n    1686,\n    3146,\n    3855,4372,\n    2714,\n    2180,\n    2074,\n    2516,\n    3674,\n    4533,\n    3849,\n    4317,\n    3738,\n    3378,\n    3901,\n    3068,\n    4697,\n    6180,\n    3583,\n    3760,\n    6467,\n    3897,\n    3404,\n    5554,\n    4941,\n    2143,\n    0,\n    2572,\n    5019],\n    'interval number': 45,\n    'interval storing bandwidth (megabytes/s)': [4000,\n    3368,\n    4334,\n    6936,\n    5824,\n    3695,\n    2524,\n    3720,\n    6066,\n    4863,\n    2938,\n    3924,\n    6061,\n    4752,\n    2250,\n    2238,\n    3000,\n    4500,\n    3000,\n    1500,\n    3000,\n    3000,\n    3000,\n    3041,\n    4458,\n    3617,\n    3295,\n    3841,\n    3495,\n    4500,\n    3927,\n    4839,\n    5822,\n    3302,\n    3749,\n    6609,\n    3749,\n    3177,\n    5876,\n    4570,\n    2255,\n    2187,\n    3430,\n    2812,\n    942],\n    'interval time (ms)': 1.0,\n    'latency (ms)': 44.73,\n    'latency (ms) by segments': [44.732],\n    'latency (us)': 44731.9,\n    'layer details': [['layer',\n        'ops',\n        'computing cost (no DDR)',\n        'load/store cost'],\n    ['_features_0_0_hz_conv2d',\n        '113,246,208',\n        '29 us (0% of model)',\n        '267 us (0.5% of model)'],\n    ['_features_1_conv_0_0_hz_conv2d',\n        '37,748,736',\n        '42 us (0% of model)',\n        '1156 us (2.5% of model)'],\n    ['_features_1_conv_1_hz_conv2d',\n        '67,108,864',\n        '20 us (0% of model)',\n        '1 us (0% of model)'],\n    ['_features_2_conv_0_0_hz_conv2d',\n        '201,326,592',\n        '44 us (0% of model)',\n        '3132 us (7.0% of model)'],\n    ['_features_2_conv_1_0_hz_conv2d',\n        '28,311,552',\n        '54 us (0.1% of model)',\n        '3132 us (7.0% of model)'],['_features_2_conv_2_hz_conv2d',\n    '75,497,472',\n    '23 us (0% of model)',\n    '1 us (0% of model)'],\n    ['_features_3_conv_0_0_hz_conv2d',\n    '113,246,208',\n    '24 us (0% of model)',\n    '638 us (1.4% of model)'],\n    ['_features_3_conv_1_0_hz_conv2d',\n    '42,467,328',\n    '33 us (0% of model)',\n    '3592 us (8.0% of model)'],\n    ['_features_3_generated_add_0_hz_conv2d',\n    '113,246,208',\n    '14 us (0% of model)',\n    '637 us (1.4% of model)'],\n    ['_features_4_conv_0_0_hz_conv2d',\n    '113,246,208',\n    '45 us (0.1% of model)',\n    '2433 us (5.4% of model)'],\n    ['_features_4_conv_1_0_hz_conv2d',\n    '10,616,832',\n    '63 us (0.1% of model)',\n    '2432 us (5.4% of model)'],\n    ['_features_4_conv_2_hz_conv2d',\n    '37,748,736',\n    '21 us (0% of model)',\n    '1 us (0% of model)'],\n    ['_features_5_conv_0_0_hz_conv2d',\n    '50,331,648',\n    '40 us (0% of model)',\n    '3 us (0% of model)'],\n    ['_features_5_conv_1_0_hz_conv2d',\n    '14,155,776',\n    '45 us (0% of model)',\n    '463 us (1.0% of model)'],\n    ['_features_5_generated_add_0_hz_conv2d',\n    '50,331,648',\n    '23 us (0% of model)',\n    '462 us (1.0% of model)'],\n    ['_features_6_conv_0_0_hz_conv2d',\n    '50,331,648',\n    '40 us (0% of model)',\n    '3 us (0% of model)'],\n    ['_features_6_conv_1_0_hz_conv2d',\n    '14,155,776',\n    '23 us (0% of model)',\n    '463 us (1.0% of model)'],\n    ['_features_6_generated_add_0_hz_conv2d',\n    '50,331,648',['23 us (0% of model)',\n    '462 us (1.0% of model)'],\n    ['_features_7_conv_0_0_hz_conv2d',\n    '50,331,648',\n    '61 us (0.1% of model)',\n    '813 us (1.8% of model)'],\n    ['_features_7_conv_1_0_hz_conv2d',\n    '3,538,944',\n    '76 us (0.1% of model)',\n    '812 us (1.8% of model)'],\n    ['_features_7_conv_2_hz_conv2d',\n    '25,165,824',\n    '47 us (0.1% of model)',\n    '3 us (0% of model)'],\n    ['_features_8_conv_0_0_hz_conv2d',\n    '50,331,648',\n    '76 us (0.1% of model)',\n    '5 us (0% of model)'],\n    ['_features_8_conv_1_0_hz_conv2d',\n    '7,077,888',\n    '75 us (0.1% of model)',\n    '463 us (1.0% of model)'],\n    ['_features_8_generated_add_0_hz_conv2d',\n    '50,331,648',\n    '67 us (0.1% of model)',\n    '465 us (1.0% of model)'],\n    ['_features_9_conv_0_0_hz_conv2d',\n    '50,331,648',\n    '76 us (0.1% of model)',\n    '5 us (0% of model)'],\n    ['_features_9_conv_1_0_hz_conv2d',\n    '7,077,888',\n    '75 us (0.1% of model)',\n    '463 us (1.0% of model)'],\n    ['_features_9_generated_add_0_hz_conv2d',\n    '50,331,648',\n    '67 us (0.1% of model)',\n    '465 us (1.0% of model)'],\n    ['_features_10_conv_0_0_hz_conv2d',\n    '50,331,648',\n    '76 us (0.1% of model)',\n    '5 us (0% of model)'],\n    ['_features_10_conv_1_0_hz_conv2d',\n    '7,077,888',\n    '51 us (0.1% of model)',\n    '463 us (1.0% of model)'],\n    ['_features_10_generated_add_0_hz_conv2d',\n    '50,331,648',\n    '67 us (0.1% of model)',\n    '465 us (1.0% of model)']['_features_11_conv_0_0_hz_conv2d',\n    '50,331,648',\n    '76 us (0.1% of model)',\n    '5 us (0% of model)'],\n\n    ['_features_11_conv_1_0_hz_conv2d',\n    '7,077,888',\n    '75 us (0.1% of model)',\n    '463 us (1.0% of model)'],\n\n    ['_features_11_conv_2_hz_conv2d',\n    '75,497,472',\n    '110 us (0.2% of model)',\n    '467 us (1.0% of model)'],\n\n    ['_features_12_conv_0_0_hz_conv2d',\n    '113,246,208',\n    '43 us (0% of model)',\n    '644 us (1.4% of model)'],\n\n    ['_features_12_conv_1_0_hz_conv2d',\n    '10,616,832',\n    '46 us (0.1% of model)',\n    '1274 us (2.8% of model)'],\n\n    ['_features_12_generated_add_0_hz_conv2d',\n    '113,246,208',\n    '37 us (0% of model)',\n    '643 us (1.4% of model)'],\n\n    ['_features_13_conv_0_0_hz_conv2d',\n    '113,246,208',\n    '43 us (0% of model)',\n    '644 us (1.4% of model)'],\n\n    ['_features_13_conv_1_0_hz_conv2d',\n    '10,616,832',\n    '55 us (0.1% of model)',\n    '1274 us (2.8% of model)'],\n\n    ['_features_13_generated_add_0_hz_conv2d',\n    '113,246,208',\n    '31 us (0% of model)',\n    '642 us (1.4% of model)'],\n\n    ['_features_14_conv_0_0_hz_conv2d',\n    '113,246,208',\n    '67 us (0.1% of model)',\n    '644 us (1.4% of model)'],\n\n    ['_features_14_conv_1_0_hz_conv2d',\n    '2,654,208',\n    '72 us (0.1% of model)',\n    '1274 us (2.8% of model)'],\n\n    ['_features_14_conv_2_hz_conv2d',\n    '47,185,920',\n    '108 us (0.2% of model)',\n    '647 us (1.4% of model)'],\n\n    ['_features_15_conv_0_0_hz_conv2d',\n    '78,643,200'    \n    ['_features_15_conv_1_0_hz_conv2d',\n        '4,423,680',\n        '51 us (0.1% of model)',\n        '1973 us (4.4% of model)'],\n    ['_features_15_generated_add_0_hz_conv2d',\n        '78,643,200',\n        '48 us (0.1% of model)',\n        '1004 us (2.2% of model)'],\n    ['_features_16_conv_0_0_hz_conv2d',\n        '78,643,200',\n        '39 us (0% of model)',\n        '1004 us (2.2% of model)'],\n    ['_features_16_conv_1_0_hz_conv2d',\n        '4,423,680',\n        '56 us (0.1% of model)',\n        '1973 us (4.4% of model)'],\n    ['_features_16_generated_add_0_hz_conv2d',\n        '78,643,200',\n        '41 us (0% of model)',\n        '1004 us (2.2% of model)'],\n    ['_features_17_conv_0_0_hz_conv2d',\n        '78,643,200',\n        '80 us (0.1% of model)',\n        '1004 us (2.2% of model)'],\n    ['_features_17_conv_1_0_hz_conv2d',\n        '4,423,680',\n        '63 us (0.1% of model)',\n        '1973 us (4.4% of model)'],\n    ['_features_17_conv_2_hz_conv2d',\n        '157,286,400',\n        '99 us (0.2% of model)',\n        '1021 us (2.2% of model)'],\n    ['_features_18_0_hz_conv2d',\n        '209,715,200',\n        '878 us (1.9% of model)',\n        '2601 us (5.8% of model)'],\n    ['_features_18_0_hz_conv2d_torch_native', '0', '11 us (0% of model)', '0'],\n    ['_classifier_1_hz_linear_torch_native',\n        '6,553,600',\n        '5 us (0% of model)',\n        '7 us (0% of model)']],\n    'loaded bytes per frame': 707232,\n    'loaded bytes per run': 181051392,\n    'model json CRC': '51b16a11',\n    'model json file': '/tmp/hbdktmp_ocro1_9_.hbir',\n    'model name': 'FxQATReadyMobileNetV2',\n    'model param CRC': '00000000',\n    'multicore sync time (ms)': 0.0,'run per second': 22.36,\n'runtime version': '3.15.29.0',\n'stored bytes per frame': 696360,\n'stored bytes per run': 178268160,\n'worst FPS': 5722.98\n\n"})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'\n    # Model visualization\n    visualize_model(\n        script_model,\n        [example_input],\n        save_path=os.path.join(model_path, "model.svg"),\n        show=False,\n    )\n\n'})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-shell",children:"\n    INFO: launch 1 threads for optimization\n\n    consumed time 1.6947\n    HBDK model compilation SUCCESS\n\n"})})]})}function _(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}}}]);