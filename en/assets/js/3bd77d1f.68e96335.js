"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[28487],{18726:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"Basic_Application/pydev_demo_sample/yolov5x_sample","title":"3.3.5 YOLOv5x Model Example Introduction","description":"Example Overview","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/03_Basic_Application/03_pydev_demo_sample/09_yolov5x_sample.md","sourceDirName":"03_Basic_Application/03_pydev_demo_sample","slug":"/Basic_Application/pydev_demo_sample/yolov5x_sample","permalink":"/rdk_doc/en/Basic_Application/pydev_demo_sample/yolov5x_sample","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1770786802000,"sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"3.3.4 YOLOv5 Model Example Introduction","permalink":"/rdk_doc/en/Basic_Application/pydev_demo_sample/yolov5_sample"},"next":{"title":"3.3.6 CenterNet Example Introduction","permalink":"/rdk_doc/en/Basic_Application/pydev_demo_sample/centernet_sample"}}');var t=i(74848),r=i(28453);const s={sidebar_position:5},a="3.3.5 YOLOv5x Model Example Introduction",d={},l=[{value:"Example Overview",id:"example-overview",level:2},{value:"Result Demonstration",id:"result-demonstration",level:2},{value:"Hardware Preparation",id:"hardware-preparation",level:2},{value:"Hardware Connection",id:"hardware-connection",level:3},{value:"Quick Start",id:"quick-start",level:2},{value:"Code and Board Location",id:"code-and-board-location",level:3},{value:"Compilation and Execution",id:"compilation-and-execution",level:3},{value:"Execution Result",id:"execution-result",level:3},{value:"Detailed Introduction",id:"detailed-introduction",level:2},{value:"Example Program Parameter Options",id:"example-program-parameter-options",level:3},{value:"Software Architecture Description",id:"software-architecture-description",level:3},{value:"API Process Description",id:"api-process-description",level:3},{value:"FAQ",id:"faq",level:3}];function c(e){const n={br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"335-yolov5x-model-example-introduction",children:"3.3.5 YOLOv5x Model Example Introduction"})}),"\n",(0,t.jsx)(n.h2,{id:"example-overview",children:"Example Overview"}),"\n",(0,t.jsxs)(n.p,{children:["The YOLOv5X object detection example is a ",(0,t.jsx)(n.strong,{children:"Python interface"})," development code sample located in ",(0,t.jsx)(n.code,{children:"/app/pydev_demo/09_yolov5x_sample/"}),", demonstrating how to use the YOLOv5X model for high-precision object detection tasks. YOLOv5X is the largest and most accurate model variant in the YOLOv5 series, offering higher detection accuracy compared to YOLOv5s, making it suitable for applications requiring higher detection precision."]}),"\n",(0,t.jsx)(n.h2,{id:"result-demonstration",children:"Result Demonstration"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/03_pydev_demo_sample/image/pydev_09_runing.png",alt:"output-img"})}),"\n",(0,t.jsx)(n.h2,{id:"hardware-preparation",children:"Hardware Preparation"}),"\n",(0,t.jsx)(n.h3,{id:"hardware-connection",children:"Hardware Connection"}),"\n",(0,t.jsxs)(n.p,{children:["This example only requires the RDK development board itself, with no additional peripherals needed. Ensure the development board is properly powered and the system is booted.\n",(0,t.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/03_pydev_demo_sample/image/pydev_09_hw_connect.png",alt:"connect-img"})]}),"\n",(0,t.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,t.jsx)(n.h3,{id:"code-and-board-location",children:"Code and Board Location"}),"\n",(0,t.jsxs)(n.p,{children:["Navigate to ",(0,t.jsx)(n.code,{children:"/app/pydev_demo/09_yolov5x_sample/"})," to see the YOLOv5X example containing the following files:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"root@ubuntu:/app/pydev_demo/09_yolov5x_sample# tree\n.\n\u251c\u2500\u2500 coco_classes.names\n\u251c\u2500\u2500 kite.jpg\n\u2514\u2500\u2500 test_yolov5x.py\n"})}),"\n",(0,t.jsx)(n.h3,{id:"compilation-and-execution",children:"Compilation and Execution"}),"\n",(0,t.jsx)(n.p,{children:"The Python example does not require compilation and can be run directly:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"python3 test_yolov5x.py\n"})}),"\n",(0,t.jsx)(n.h3,{id:"execution-result",children:"Execution Result"}),"\n",(0,t.jsx)(n.p,{children:"After running, the program will load the pre-trained YOLOv5X model, perform object detection on the kite.jpg image, and generate a result image with detection boxes named output_image.jpg."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"root@ubuntu:/app/pydev_demo/09_yolov5x_sample# ./test_yolov5x.py \n[BPU_PLAT]BPU Platform Version(1.3.6)!\n[HBRT] set log level as 0. version = 3.15.55.0\n[DNN] Runtime version = 1.24.5_(3.15.55 HBRT)\n[A][DNN][packed_model.cpp:247][Model](2000-01-01,08:32:56.277.320) [HorizonRT] The model builder version = 1.23.5\n[W][DNN]bpu_model_info.cpp:491][Version](2000-01-01,08:32:56.956.224) Model: yolov5x_672x672_nv12. Inconsistency between the hbrt library version 3.15.55.0 and the model build version 3.15.47.0 detected, in order to ensure correct model results, it is recommended to use compilation tools and the BPU SDK from the same OpenExplorer package.\ntensor type: NV12\ndata type: uint8\nlayout: NCHW\nshape: (1, 3, 672, 672)\n3\ntensor type: int32\ndata type: int32\nlayout: NHWC\nshape: (1, 84, 84, 255)\ntensor type: int32\ndata type: int32\nlayout: NHWC\nshape: (1, 42, 42, 255)\ntensor type: int32\ndata type: int32\nlayout: NHWC\nshape: (1, 21, 21, 255)\ninferece time is : 0.10300564765930176\npostprocess time is : 0.060691237449645996\ndraw result time is : 0.048194289207458496\n"})}),"\n",(0,t.jsx)(n.h2,{id:"detailed-introduction",children:"Detailed Introduction"}),"\n",(0,t.jsx)(n.h3,{id:"example-program-parameter-options",children:"Example Program Parameter Options"}),"\n",(0,t.jsx)(n.p,{children:"The YOLOv5X object detection example does not require command-line parameters and can be run directly. The program will automatically load the kite.jpg image in the same directory for detection processing."}),"\n",(0,t.jsx)(n.h3,{id:"software-architecture-description",children:"Software Architecture Description"}),"\n",(0,t.jsx)(n.p,{children:"The software architecture of the YOLOv5X object detection example includes the following core components:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Model Loading: Use the pyeasy_dnn module to load the pre-trained YOLOv5X model"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Image Preprocessing: Convert the input image to the NV12 format and specified size (672x672) required by the model"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Model Inference: Call the model for forward computation to generate feature maps"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Post-processing: Use the libpostprocess library to parse the model output and generate detection results"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Result Visualization: Draw detection boxes and category information on the original image"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Result Saving: Save the visualized result as an image file"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)("center",{children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/03_pydev_demo_sample/image/pydev_09_yolov5x_sample_software_arch.png",alt:"software_arch"})})}),"\n",(0,t.jsx)(n.h3,{id:"api-process-description",children:"API Process Description"}),"\n",(0,t.jsx)(n.p,{children:"Model Loading: models = dnn.load('../models/yolov5x_672x672_nv12.bin')"}),"\n",(0,t.jsx)(n.p,{children:"Image Preprocessing: Adjust image size and convert to NV12 format"}),"\n",(0,t.jsx)(n.p,{children:"Model Inference: outputs = models[0].forward(nv12_data)"}),"\n",(0,t.jsx)(n.p,{children:"Post-processing Configuration: Set post-processing parameters (size, threshold, etc.)"}),"\n",(0,t.jsx)(n.p,{children:"Result Parsing: Call the post-processing library to parse output tensors"}),"\n",(0,t.jsx)(n.p,{children:"Result Visualization: Draw detection boxes and label information on the original image"}),"\n",(0,t.jsx)(n.p,{children:"Result Saving: Save the result as an image file"}),"\n",(0,t.jsx)("center",{children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/03_pydev_demo_sample/image/pydev_09_yolov5x_sample_api_flow.png",alt:"API_Flow"})})}),"\n",(0,t.jsx)(n.h3,{id:"faq",children:"FAQ"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Q:"})," What is the difference between YOLOv5X and YOLOv5s?",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"A:"})," YOLOv5X is the largest and most accurate model variant in the YOLOv5 series, with approximately 4 times the parameters of YOLOv5s, offering higher detection accuracy but slower inference speed."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Q:"})," What should I do if I encounter a \"No module named 'hobot_dnn'\" error when running the example?",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"A:"})," Please ensure that the RDK Python environment is correctly installed, including the hobot_dnn module and other official dedicated inference libraries."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Q:"})," How can I change the test image?",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"A:"})," Place the new image file in the example directory and modify img_file = cv2.imread('your_image_path') in the code."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Q:"})," What should I do if the detection results are inaccurate?",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"A:"})," The YOLOv5X model is trained on the COCO dataset. For specific scenarios, fine-tuning or using a more suitable model may be necessary."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Q:"})," How can I adjust the detection threshold?",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"A:"})," Modify the value of yolov5_postprocess_info.score_threshold in the code. For example, changing it to 0.5 can increase detection sensitivity."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Q:"})," Can real-time video stream processing be implemented?",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"A:"})," The current example is designed for single images, but the code can be modified to achieve real-time object detection for video streams. Due to the large size of the YOLOv5X model, real-time processing may require reducing the frame rate or resolution."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Q:"})," How to choose the appropriate YOLOv5 model variant?",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"A:"})," Select based on application requirements: choose YOLOv5X for the highest accuracy; YOLOv5s or YOLOv5m for balanced accuracy and speed; YOLOv5n for resource-constrained environments."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Q:"})," How to handle input images of different sizes?",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"A:"})," The YOLOv5X model requires fixed-size input (672x672). The program will automatically resize the input image to this dimension."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Q:"})," How to further improve detection accuracy?",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"A:"})," Try using larger input sizes (if the model supports it) or fine-tuning the model for specific scenarios."]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>a});var o=i(96540);const t={},r=o.createContext(t);function s(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);