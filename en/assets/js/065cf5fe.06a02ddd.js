"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[84061],{28453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>o});var t=i(96540);const s={},r=t.createContext(s);function l(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),t.createElement(r.Provider,{value:n},e.children)}},73458:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"Algorithm_Application/C++_Sample/UNetMobileNet","title":"Semantic Segmentation - UNetMobileNet","description":"This example demonstrates how to run the UNet-MobileNet semantic segmentation model on the BPU, supporting functionalities such as image preprocessing, inference, and post-processing (parsing outputs and overlaying color segmentation masks). The example code is located in the /app/cdevdemo/bpu/03instancesegmentationsample/01_unetmobilenet/ directory.","source":"@site/i18n/en/docusaurus-plugin-content-docs-docs_s/current/04_Algorithm_Application/03_C++_Sample/06_UNetMobileNet.md","sourceDirName":"04_Algorithm_Application/03_C++_Sample","slug":"/Algorithm_Application/C++_Sample/UNetMobileNet","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/C++_Sample/UNetMobileNet","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1770185252000,"sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Object Detection - Ultralytics YOLO11","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/C++_Sample/Ultralytics_YOLO11"},"next":{"title":"Instance Segmentation - Ultralytics YOLO11","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/C++_Sample/Ultralytics_YOLO11_Seg"}}');var s=i(74848),r=i(28453);const l={sidebar_position:6},o="Semantic Segmentation - UNetMobileNet",a={},c=[{value:"Model Description",id:"model-description",level:2},{value:"Functionality Description",id:"functionality-description",level:2},{value:"Environment Dependencies",id:"environment-dependencies",level:2},{value:"Directory Structure",id:"directory-structure",level:2},{value:"Building the Project",id:"building-the-project",level:2},{value:"Parameter Description",id:"parameter-description",level:2},{value:"Quick Start",id:"quick-start",level:2},{value:"Notes",id:"notes",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"semantic-segmentation---unetmobilenet",children:"Semantic Segmentation - UNetMobileNet"})}),"\n",(0,s.jsxs)(n.p,{children:["This example demonstrates how to run the UNet-MobileNet semantic segmentation model on the BPU, supporting functionalities such as image preprocessing, inference, and post-processing (parsing outputs and overlaying color segmentation masks). The example code is located in the ",(0,s.jsx)(n.code,{children:"/app/cdev_demo/bpu/03_instance_segmentation_sample/01_unetmobilenet/"})," directory."]}),"\n",(0,s.jsx)(n.h2,{id:"model-description",children:"Model Description"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Overview"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"UNet is a classic semantic segmentation network architecture that adopts an encoder-decoder structure and excels in fields such as medical image analysis. In this example, MobileNet is used as the encoder backbone to reduce model complexity and accelerate inference speed, making it suitable for real-time segmentation tasks on edge devices. The model outputs a class label for each pixel, enabling applications like urban street scene segmentation."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"HBM Model Name"}),": unet_mobilenet_1024x2048_nv12.hbm"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Input Format"}),": NV12, with resolution 1024x2048 (separate Y and UV planes)"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Output"}),": A segmentation map with the same dimensions as the input, where each pixel corresponds to a class label ranging from 0 to 18 (19 classes in total)"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"functionality-description",children:"Functionality Description"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Model Loading"})}),"\n",(0,s.jsx)(n.p,{children:"Loads the quantized semantic segmentation model and extracts model metadata."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Input Preprocessing"})}),"\n",(0,s.jsx)(n.p,{children:"The original image is loaded in BGR format, resized to 1024\xd72048, converted to NV12 format (with separate Y/UV planes), and packaged into the input dictionary structure required by the inference interface."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Inference Execution"})}),"\n",(0,s.jsxs)(n.p,{children:["Executes forward inference using the ",(0,s.jsx)(n.code,{children:".infer()"})," method, producing a logits tensor for each class."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Result Post-processing"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Applies argmax to the output tensor to obtain the predicted class for each pixel;"}),"\n",(0,s.jsx)(n.li,{children:"Resizes the prediction map to match the original input image size;"}),"\n",(0,s.jsx)(n.li,{children:"Restores it to the original image dimensions and maps classes to a specified color palette;"}),"\n",(0,s.jsx)(n.li,{children:"Blends the result with the original image using a configurable alpha blending coefficient to generate a visual segmentation overlay;"}),"\n",(0,s.jsx)(n.li,{children:"The final image contains an intuitive overlay of the segmentation result and can be saved or displayed."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"environment-dependencies",children:"Environment Dependencies"}),"\n",(0,s.jsx)(n.p,{children:"Before compiling and running, ensure the following dependencies are installed:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sudo apt update\nsudo apt install libgflags-dev\n"})}),"\n",(0,s.jsx)(n.h2,{id:"directory-structure",children:"Directory Structure"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:".\n\u251c\u2500\u2500 CMakeLists.txt             # CMake build configuration file\n\u251c\u2500\u2500 README.md                  # Usage documentation (this file)\n\u251c\u2500\u2500 inc\n\u2502   \u2514\u2500\u2500 unet_mobilenet.hpp      # Header file for the UnetMobileNet class (declares interfaces for model loading, preprocessing, inference, and post-processing)\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 main.cc                 # Main program entry point, orchestrating the inference pipeline\n    \u2514\u2500\u2500 unet_mobilenet.cc       # Implementation of the UnetMobileNet class\n"})}),"\n",(0,s.jsx)(n.h2,{id:"building-the-project",children:"Building the Project"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Configuration and Compilation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"mkdir build && cd build\ncmake ..\nmake -j$(nproc)\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"parameter-description",children:"Parameter Description"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Description"}),(0,s.jsx)(n.th,{children:"Default Value"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--model_path"})}),(0,s.jsxs)(n.td,{children:["Path to the model file (",(0,s.jsx)(n.code,{children:".hbm"})," format)"]}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"/opt/hobot/model/s100/basic/unet_mobilenet_1024x2048_nv12.hbm"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--test_img"})}),(0,s.jsx)(n.td,{children:"Path to the input test image"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"/app/res/assets/segmentation.png"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--alpha_f"})}),(0,s.jsxs)(n.td,{children:["Visualization blending factor: ",(0,s.jsx)(n.code,{children:"0.0=mask only"}),", ",(0,s.jsx)(n.code,{children:"1.0=original image only"})]}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"0.75"})})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Running the Model"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Ensure you are in the ",(0,s.jsx)(n.code,{children:"build"})," directory."]}),"\n",(0,s.jsxs)(n.li,{children:["Run with default parameters:","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"./unet_mobilenet\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Run with custom parameters:","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"./unet_mobilenet \\\n--model_path /opt/hobot/model/s100/basic/unet_mobilenet_1024x2048_nv12.hbm \\\n--test_img /app/res/assets/segmentation.png \\\n--alpha_f 0.75\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Viewing Results"})}),"\n",(0,s.jsxs)(n.p,{children:["Upon successful execution, the result will be overlaid on the original image and saved as ",(0,s.jsx)(n.code,{children:"build/result.jpg"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"[Saved] Result saved to: result.jpg\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"notes",children:"Notes"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["The output result is saved as ",(0,s.jsx)(n.code,{children:"result.jpg"}),"; users can inspect it directly."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"For more information about deployment options or model support, please refer to the official documentation or contact platform technical support."}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);