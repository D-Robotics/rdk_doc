"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[89274],{15454:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"Algorithm_Application/Python_Sample/MobileNetV2","title":"Image Classification - MobileNetV2","description":"This example demonstrates how to perform image classification using a BPU-deployed MobileNetV2 model with inference executed via hbmruntime. The example code is located in the /app/pydevdemo/01classificationsample/02_mobilenetv2/ directory.","source":"@site/i18n/en/docusaurus-plugin-content-docs-docs_s/current/04_Algorithm_Application/02_Python_Sample/03_MobileNetV2.md","sourceDirName":"04_Algorithm_Application/02_Python_Sample","slug":"/Algorithm_Application/Python_Sample/MobileNetV2","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/Python_Sample/MobileNetV2","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1768581652000,"sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Image Classification - ResNet18","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/Python_Sample/ResNet18"},"next":{"title":"Object Detection - Ultralytics YOLOv5x","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/Python_Sample/Ultralytics_YOLOv5x"}}');var s=i(74848),r=i(28453);const l={sidebar_position:3},o="Image Classification - MobileNetV2",d={},c=[{value:"Model Description",id:"model-description",level:2},{value:"Functionality Description",id:"functionality-description",level:2},{value:"Environment Dependencies",id:"environment-dependencies",level:2},{value:"Directory Structure",id:"directory-structure",level:2},{value:"Parameter Description",id:"parameter-description",level:2},{value:"Quick Start",id:"quick-start",level:2},{value:"Notes",id:"notes",level:2}];function a(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"image-classification---mobilenetv2",children:"Image Classification - MobileNetV2"})}),"\n",(0,s.jsxs)(n.p,{children:["This example demonstrates how to perform image classification using a BPU-deployed ",(0,s.jsx)(n.code,{children:"MobileNetV2"})," model with inference executed via ",(0,s.jsx)(n.code,{children:"hbm_runtime"}),". The example code is located in the ",(0,s.jsx)(n.code,{children:"/app/pydev_demo/01_classification_sample/02_mobilenetv2/"})," directory."]}),"\n",(0,s.jsx)(n.h2,{id:"model-description",children:"Model Description"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Overview"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"MobileNetV2 is a lightweight convolutional neural network proposed by Google in 2018, designed for efficient image recognition on mobile devices. It introduces the Inverted Residual and Linear Bottleneck structures to reduce computational cost while improving performance. MobileNetV2 is highly suitable for deployment on edge devices and in resource-constrained scenarios for tasks such as image classification and detection. The MobileNetV2 model used in this example accepts 224\xd7224 input and is a BPU quantized model supporting the NV12 format."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"HBM Model Name"}),": ",(0,s.jsx)(n.code,{children:"mobilenetv2_224x224_nv12.hbm"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Input Format"}),": NV12, with resolution 224x224 (separated Y and UV planes)"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Output"}),": Softmax probability distribution over 1000 classes (conforming to the ImageNet 1000-class standard)"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Model Download URL"})," (automatically downloaded by the program):"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"https://archive.d-robotics.cc/downloads/rdk_model_zoo/rdk_s100/MobileNet/mobilenetv2_224x224_nv12.hbm\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"functionality-description",children:"Functionality Description"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Model Loading"})}),"\n",(0,s.jsxs)(n.p,{children:["Load the model file using ",(0,s.jsx)(n.code,{children:"hbm_runtime"}),", and extract the model name, input/output names, and their corresponding shapes."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Input Preprocessing"})}),"\n",(0,s.jsx)(n.p,{children:"Resize the input image from BGR format to 224x224, then convert it to the hardware-required NV12 format (with separated Y and UV planes). The processed data is structured as a dictionary to match the inference interface."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Inference Execution"})}),"\n",(0,s.jsxs)(n.p,{children:["Call the ",(0,s.jsx)(n.code,{children:".run()"})," method to perform inference, supporting configuration of BPU core(s) (e.g., ",(0,s.jsx)(n.code,{children:"core0"}),"/",(0,s.jsx)(n.code,{children:"core1"}),") and inference priority (0\u2013255)."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Result Post-processing"})}),"\n",(0,s.jsx)(n.p,{children:"Retrieve the model output tensor, parse the softmax probabilities, and display the top-K predictions (default: top-5), including class names and corresponding probabilities."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"environment-dependencies",children:"Environment Dependencies"}),"\n",(0,s.jsxs)(n.p,{children:["This example has no special environment requirements\u2014only the dependencies from ",(0,s.jsx)(n.code,{children:"pydev"})," need to be installed:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install -r ../../requirements.txt\n"})}),"\n",(0,s.jsx)(n.h2,{id:"directory-structure",children:"Directory Structure"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:".\n\u251c\u2500\u2500 mobilenetv2.py              # Main inference script\n\u2514\u2500\u2500 README.md                   # Usage instructions\n"})}),"\n",(0,s.jsx)(n.h2,{id:"parameter-description",children:"Parameter Description"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Description"}),(0,s.jsx)(n.th,{children:"Default Value"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--model-path"})}),(0,s.jsxs)(n.td,{children:["Path to the model file (",(0,s.jsx)(n.code,{children:".hbm"})," format)"]}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"/opt/hobot/model/s100/basic/mobilenetv2_224x224_nv12.hbm"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--test-img"})}),(0,s.jsx)(n.td,{children:"Path to the test image"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"/app/res/assets/zebra_cls.jpg"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--label-file"})}),(0,s.jsx)(n.td,{children:"Path to the class label mapping file"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"/app/res/labels/imagenet1000_clsidx_to_labels.txt"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--priority"})}),(0,s.jsx)(n.td,{children:"Model inference priority (0\u2013255; higher value = higher priority)"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"0"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--bpu-cores"})}),(0,s.jsxs)(n.td,{children:["List of BPU core IDs to use for inference (e.g., ",(0,s.jsx)(n.code,{children:"--bpu-cores 0 1"}),")"]}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"[0]"})})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Run the Model"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["With default parameters:","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python mobilenetv2.py\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["With custom parameters:","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python mobilenetv2.py \\\n--model-path /opt/hobot/model/s100/basic/mobilenetv2_224x224_nv12.hbm \\\n--test-img /app/res/assets/zebra_cls.jpg \\\n--label-file /app/res/labels/imagenet1000_clsidx_to_labels.txt\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"View Results"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"Top-5 Predictions:\nzebra: 0.8916\ntiger, Panthera tigris: 0.0028\nhartebeest: 0.0018\njaguar, panther, Panthera onca, Felis onca: 0.0016\ntiger cat: 0.0016\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"notes",children:"Notes"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"If the specified model path does not exist, the program will attempt to download the model automatically."}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(a,{...e})}):a(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>o});var t=i(96540);const s={},r=t.createContext(s);function l(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);