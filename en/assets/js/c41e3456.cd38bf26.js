"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[18852],{28453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>c});var o=n(96540);const i={},r=o.createContext(i);function s(e){const t=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function c(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),o.createElement(r.Provider,{value:t},e.children)}},69315:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>c,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>a});const o=JSON.parse('{"id":"Robot_development/boxs/driver/hobot_centerpoint","title":"LiDAR Object Detection Algorithm","description":"Feature Description","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/05_Robot_development/03_boxs/driver/hobot_centerpoint.md","sourceDirName":"05_Robot_development/03_boxs/driver","slug":"/Robot_development/boxs/driver/hobot_centerpoint","permalink":"/rdk_doc/en/Robot_development/boxs/driver/hobot_centerpoint","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1760066808000,"sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Road Structuring","permalink":"/rdk_doc/en/Robot_development/boxs/driver/parking_perception"},"next":{"title":"BEV Perception Algorithm","permalink":"/rdk_doc/en/Robot_development/boxs/driver/hobot_bev"}}');var i=n(74848),r=n(28453);const s={sidebar_position:2},c="LiDAR Object Detection Algorithm",l={},a=[{value:"Feature Description",id:"feature-description",level:2},{value:"Supported Platforms",id:"supported-platforms",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"RDK Platform",id:"rdk-platform",level:3},{value:"Usage Introduction",id:"usage-introduction",level:2},{value:"RDK Platform",id:"rdk-platform-1",level:3},{value:"Using Local Point Cloud File for Inference",id:"using-local-point-cloud-file-for-inference",level:3},{value:"Result Analysis",id:"result-analysis",level:2}];function d(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"lidar-object-detection-algorithm",children:"LiDAR Object Detection Algorithm"})}),"\n",(0,i.jsx)(t.h2,{id:"feature-description",children:"Feature Description"}),"\n",(0,i.jsxs)(t.p,{children:["The LiDAR object detection algorithm is based on the ",(0,i.jsx)(t.code,{children:"CenterPoint"})," model, which is trained on the ",(0,i.jsx)(t.a,{href:"https://www.nuscenes.org/nuscenes",children:"nuscenes"})," dataset using ",(0,i.jsx)(t.a,{href:"https://developer.d-robotics.cc/api/v1/fileData/horizon_j5_open_explorer_cn_doc/hat/source/examples/centerpoint.html",children:"OpenExplorer"}),"."]}),"\n",(0,i.jsx)(t.p,{children:"The algorithm takes in 32-line LiDAR point cloud data as input and outputs information including the 3D bounding boxes of detected objects, confidence scores, and object categories. The supported object detection categories include car, truck, bus, barrier, motorcycle, and pedestrian, making up a total of six categories."}),"\n",(0,i.jsx)(t.p,{children:"In this example, local LiDAR point cloud files are used as input. The algorithm runs on a BPU (Brain Processing Unit) for inference and publishes the rendered images containing point cloud data, detection boxes, and orientation to a PC browser via WebSocket. The algorithm results are rendered and displayed in the browser."}),"\n",(0,i.jsxs)(t.p,{children:["Code repository: ",(0,i.jsx)(t.a,{href:"https://github.com/D-Robotics/hobot_centerpoint",children:"GitHub - hobot_centerpoint"})]}),"\n",(0,i.jsx)(t.h2,{id:"supported-platforms",children:"Supported Platforms"}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Platform"}),(0,i.jsx)(t.th,{children:"Operating Mode"}),(0,i.jsx)(t.th,{children:"Example Feature"})]})}),(0,i.jsx)(t.tbody,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"RDK Ultra"}),(0,i.jsx)(t.td,{children:"Ubuntu 20.04 (Foxy)"}),(0,i.jsx)(t.td,{children:"Use local point cloud data and render inference results via web"})]})})]}),"\n",(0,i.jsx)(t.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(t.h3,{id:"rdk-platform",children:"RDK Platform"}),"\n",(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"The RDK platform has Ubuntu 20.04 system image pre-installed."}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"TogetheROS.Bot is successfully installed on the RDK platform."}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"Ensure that the PC can access the RDK platform via the network."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"usage-introduction",children:"Usage Introduction"}),"\n",(0,i.jsx)(t.h3,{id:"rdk-platform-1",children:"RDK Platform"}),"\n",(0,i.jsx)(t.h3,{id:"using-local-point-cloud-file-for-inference",children:"Using Local Point Cloud File for Inference"}),"\n",(0,i.jsx)(t.p,{children:"The LiDAR object detection algorithm example uses a local LiDAR point cloud file for inference. After the inference, the results are rendered and published as an image message containing the algorithm's output (detection boxes, confidence scores, and orientations). These results are displayed on a PC browser using a WebSocket package."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"Preparing the LiDAR Point Cloud File:"})}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-shell",children:"# Download LiDAR Point Cloud File for Local Inference\nwget http://archive.d-robotics.cc/TogetheROS/data/hobot_centerpoint_data.tar.gz\n\n# Extract the Data\nmkdir config\ntar -zxvf hobot_centerpoint_data.tar.gz -C config\n# After extraction, the data will be located at config/hobot_centerpoint_data\n"})}),"\n",(0,i.jsx)(t.p,{children:"Start Algorithm Example:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-shell",children:"# Configure tros.b environment\nsource /opt/tros/setup.bash\n\n# Start the websocket service\nros2 launch websocket websocket_service.launch.py\n\n# Launch the CenterPoint algorithm\nros2 launch hobot_centerpoint hobot_centerpoint_websocket.launch.py lidar_pre_path:=config/hobot_centerpoint_data\n"})}),"\n",(0,i.jsx)(t.h2,{id:"result-analysis",children:"Result Analysis"}),"\n",(0,i.jsx)(t.p,{children:"After starting the algorithm example, the following information will be output in the terminal:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-text",children:"[INFO] [launch]: Default logging verbosity is set to INFO\n[INFO] [hobot_centerpoint-1]: process started with pid [22470]\n[INFO] [websocket-2]: process started with pid [22472]\n[hobot_centerpoint-1] [WARN] [0948485758.916907430] [centerpoint_node]:\n[hobot_centerpoint-1]  preprocess_config: config/centerpoint_preprocess_5dim.json\n[hobot_centerpoint-1]  model_file: config/model/model.hbm\n[hobot_centerpoint-1]  lidar_list_file: ./config/nuscenes_lidar_val.lst\n[hobot_centerpoint-1]  is_show: 1\n[hobot_centerpoint-1]  is_loop: 1\n[hobot_centerpoint-1]  pub_topic_name: /hobot_centerpoint\n[hobot_centerpoint-1]  lidar_pre_path: ./config/hobot_centerpoint_data\n[hobot_centerpoint-1] [BPU_PLAT]BPU Platform Version(1.3.3)!\n[hobot_centerpoint-1] [HBRT] set log level as 0. version = 3.14.25.0\n[hobot_centerpoint-1] [DNN] Runtime version = 1.12.3_(3.14.25 HBRT)\n[hobot_centerpoint-1] [WARN] [0948485759.205674972] [dnn]: Run default SetOutputParser.\n[hobot_centerpoint-1] [WARN] [0948485759.205820889] [dnn]: Set output parser with default dnn node parser, you will get all output tensors and should parse output_tensors in PostProcess.\n[hobot_centerpoint-1] [WARN] [0948485759.208895472] [hobot_centerpoint]: A total of 81 files were fetched!\n[hobot_centerpoint-1] [WARN] [0948485759.400904472] [CenterPoint_Node]: input fps: -1.00, out fps: -1.00, infer time ms: 61, post process time ms: 57\n[hobot_centerpoint-1] [WARN] [0948485759.839328014] [CenterPoint_Node]: input fps: -1.00, out fps: -1.00, infer time ms: 27, post process time ms: 53\n[hobot_centerpoint-1] [WARN] [0948485760.281992264] [CenterPoint_Node]: input fps: -1.00, out fps: -1.00, infer time ms: 28, post process time ms: 53\n[hobot_centerpoint-1] [WARN] [0948485760.731948223] [CenterPoint_Node]: input fps: 2.93, out fps: 3.01, infer time ms: 27, post process time ms: 56\n[hobot_centerpoint-1] [WARN] [0948485761.155906223] [CenterPoint_Node]: input fps: 2.93, out fps: 3.01, infer time ms: 28, post process time ms: 56\n[hobot_centerpoint-1] [WARN] [0948485761.572980640] [CenterPoint_Node]: input fps: 2.93, out fps: 3.01, infer time ms: 27, post process time ms: 53\n[hobot_centerpoint-1] [WARN] [0948485761.983718973] [CenterPoint_Node]: input fps: 2.40, out fps: 2.40, infer time ms: 28, post process time ms: 55\n[hobot_centerpoint-1] [WARN] [0948485762.396930973] [CenterPoint_Node]: input fps: 2.40, out fps: 2.40, infer time ms: 28, post process time ms: 55\n[hobot_centerpoint-1] [WARN] [0948485762.816782057] [CenterPoint_Node]: input fps: 2.40, out fps: 2.40, infer time ms: 27, post process time ms: 56\n[hobot_centerpoint-1] [WARN] [0948485763.239294099] [CenterPoint_Node]: input fps: 2.39, out fps: 2.39, infer time ms: 27, post process time ms: 57\n[hobot_centerpoint-1] [WARN] [0948485763.661555807] [CenterPoint_Node]: input fps: 2.39, out fps: 2.39, infer time ms: 27, post process time ms: 57\n[hobot_centerpoint-1] [WARN] [0948485764.084410183] [CenterPoint_Node]: input fps: 2.39, out fps: 2.39, infer time ms: 27, post process time ms: 57\n[hobot_centerpoint-1] [WARN] [0948485764.502788849] [CenterPoint_Node]: input fps: 2.37, out fps: 2.37, infer time ms: 27, post process time ms: 55\n"})}),"\n",(0,i.jsxs)(t.p,{children:["The output log shows that the topic for publishing the algorithm inference results is ",(0,i.jsx)(t.code,{children:"/hobot_centerpoint"}),", and the point cloud file used for inference contains 81 frames. After inference and post-processing (including rendering and publishing the results), the frame rate is approximately 2.4 fps."]}),"\n",(0,i.jsxs)(t.p,{children:["To view the images and algorithm rendering results, open a web browser on the PC and enter the following URL (replace ",(0,i.jsx)(t.code,{children:"IP"})," with the IP address of the RDK):",(0,i.jsx)(t.a,{href:"http://IP:8000",children:"http://IP:8000"})]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/03_boxs/function/image/box_adv/render_centerpoint_det.jpg",alt:""})})]})}function p(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);