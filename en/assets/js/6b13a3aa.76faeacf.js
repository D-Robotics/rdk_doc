"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[1001],{11470:(e,n,r)=>{r.d(n,{A:()=>v});var t=r(96540),i=r(34164),a=r(23104),o=r(56347),s=r(205),l=r(57485),c=r(31682),p=r(70679);function d(e){return t.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,t.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function u(e){const{values:n,children:r}=e;return(0,t.useMemo)(()=>{const e=n??function(e){return d(e).map(({props:{value:e,label:n,attributes:r,default:t}})=>({value:e,label:n,attributes:r,default:t}))}(r);return function(e){const n=(0,c.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,r])}function h({value:e,tabValues:n}){return n.some(n=>n.value===e)}function g({queryString:e=!1,groupId:n}){const r=(0,o.W6)(),i=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,l.aZ)(i),(0,t.useCallback)(e=>{if(!i)return;const n=new URLSearchParams(r.location.search);n.set(i,e),r.replace({...r.location,search:n.toString()})},[i,r])]}function m(e){const{defaultValue:n,queryString:r=!1,groupId:i}=e,a=u(e),[o,l]=(0,t.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!h({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const r=n.find(e=>e.default)??n[0];if(!r)throw new Error("Unexpected error: 0 tabValues");return r.value}({defaultValue:n,tabValues:a})),[c,d]=g({queryString:r,groupId:i}),[m,b]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[r,i]=(0,p.Dv)(n);return[r,(0,t.useCallback)(e=>{n&&i.set(e)},[n,i])]}({groupId:i}),f=(()=>{const e=c??m;return h({value:e,tabValues:a})?e:null})();(0,s.A)(()=>{f&&l(f)},[f]);return{selectedValue:o,selectValue:(0,t.useCallback)(e=>{if(!h({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);l(e),d(e),b(e)},[d,b,a]),tabValues:a}}var b=r(92303);const f={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var x=r(74848);function j({className:e,block:n,selectedValue:r,selectValue:t,tabValues:o}){const s=[],{blockElementScrollPositionUntilNextRender:l}=(0,a.a_)(),c=e=>{const n=e.currentTarget,i=s.indexOf(n),a=o[i].value;a!==r&&(l(n),t(a))},p=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const r=s.indexOf(e.currentTarget)+1;n=s[r]??s[0];break}case"ArrowLeft":{const r=s.indexOf(e.currentTarget)-1;n=s[r]??s[s.length-1];break}}n?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":n},e),children:o.map(({value:e,label:n,attributes:t})=>(0,x.jsx)("li",{role:"tab",tabIndex:r===e?0:-1,"aria-selected":r===e,ref:e=>{s.push(e)},onKeyDown:p,onClick:c,...t,className:(0,i.A)("tabs__item",f.tabItem,t?.className,{"tabs__item--active":r===e}),children:n??e},e))})}function _({lazy:e,children:n,selectedValue:r}){const a=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=a.find(e=>e.props.value===r);return e?(0,t.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:a.map((e,n)=>(0,t.cloneElement)(e,{key:n,hidden:e.props.value!==r}))})}function k(e){const n=m(e);return(0,x.jsxs)("div",{className:(0,i.A)("tabs-container",f.tabList),children:[(0,x.jsx)(j,{...n,...e}),(0,x.jsx)(_,{...n,...e})]})}function v(e){const n=(0,b.A)();return(0,x.jsx)(k,{...e,children:d(e.children)},String(n))}},19365:(e,n,r)=>{r.d(n,{A:()=>o});r(96540);var t=r(34164);const i={tabItem:"tabItem_Ymn6"};var a=r(74848);function o({children:e,hidden:n,className:r}){return(0,a.jsx)("div",{role:"tabpanel",className:(0,t.A)(i.tabItem,r),hidden:n,children:e})}},28453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>s});var t=r(96540);const i={},a=t.createContext(i);function o(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),t.createElement(a.Provider,{value:n},e.children)}},45948:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>p,contentTitle:()=>c,default:()=>h,frontMatter:()=>l,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"Robot_development/boxs/driver/parking_perception","title":"Road Structuring","description":"Introduction","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/05_Robot_development/03_boxs/driver/parking_perception.md","sourceDirName":"05_Robot_development/03_boxs/driver","slug":"/Robot_development/boxs/driver/parking_perception","permalink":"/rdk_doc/en/Robot_development/boxs/driver/parking_perception","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1761553174000,"sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Human Instance Tracking Reid","permalink":"/rdk_doc/en/Robot_development/boxs/body/reid"},"next":{"title":"LiDAR Object Detection Algorithm","permalink":"/rdk_doc/en/Robot_development/boxs/driver/hobot_centerpoint"}}');var i=r(74848),a=r(28453),o=r(11470),s=r(19365);const l={sidebar_position:1},c="Road Structuring",p={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Supported Platforms",id:"supported-platforms",level:2},{value:"Preparation",id:"preparation",level:2},{value:"RDK",id:"rdk",level:3},{value:"Usage",id:"usage",level:2},{value:"RDK",id:"rdk-1",level:3},{value:"Result Analysis",id:"result-analysis",level:2}];function u(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"road-structuring",children:"Road Structuring"})}),"\n","\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(n.p,{children:"The parking_perception package is a road structuring algorithm based on hobot_dnn package, which uses BPU for model inference to obtain algorithm results.\nThis package supports subscribing to topics of type sensors/msg/image directly, and supports inferring from local images offline. The algorithm information will be published through topics and the results will be rendered and visualized on the web page. It also supports saving the rendered images in the result directory during program execution."}),"\n",(0,i.jsx)(n.p,{children:"The supported object detection categories are as follows:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Category"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"cyclist"}),(0,i.jsx)(n.td,{children:"Cyclist"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"person"}),(0,i.jsx)(n.td,{children:"Pedestrian"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"rear"}),(0,i.jsx)(n.td,{children:"Rear"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"vehicle"}),(0,i.jsx)(n.td,{children:"Vehicle"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"parking_lock"}),(0,i.jsx)(n.td,{children:"Parking lock"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:"The supported semantic segmentation categories are as follows:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Category"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"road"}),(0,i.jsx)(n.td,{children:"Road"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"background"}),(0,i.jsx)(n.td,{children:"Background"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"lane_marking"}),(0,i.jsx)(n.td,{children:"Lane marking"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"sign_line"}),(0,i.jsx)(n.td,{children:"Sign line"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"parking_lane"}),(0,i.jsx)(n.td,{children:"Parking lane"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"parking_space"}),(0,i.jsx)(n.td,{children:"Parking space"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"parking_rod"}),(0,i.jsx)(n.td,{children:"Parking rod"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"parking_lock"}),(0,i.jsx)(n.td,{children:"Parking lock"})]})]})]}),"\n",(0,i.jsxs)(n.p,{children:["Code repository:  (",(0,i.jsx)(n.a,{href:"https://github.com/D-Robotics/parking_perception.git",children:"https://github.com/D-Robotics/parking_perception.git"}),")"]}),"\n",(0,i.jsx)(n.p,{children:"Application scenario: The outdoor parking area detection algorithm is based on semantic segmentation, which identifies parking areas in the images and can achieve automatic parking. It is mainly used in the field of autonomous driving."}),"\n",(0,i.jsxs)(n.p,{children:["Car parking space search case: ",(0,i.jsx)(n.a,{href:"../../apps/parking_search",children:"Car Parking Space Search"})]}),"\n",(0,i.jsx)(n.h2,{id:"supported-platforms",children:"Supported Platforms"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Platform"}),(0,i.jsx)(n.th,{children:"System"}),(0,i.jsx)(n.th,{children:"Function"})]})}),(0,i.jsx)(n.tbody,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"RDK X3, RDK X3 Module"}),(0,i.jsx)(n.td,{children:"Ubuntu 20.04 (Foxy), Ubuntu 22.04 (Humble)"}),(0,i.jsx)(n.td,{children:"Start MIPI/USB camera/local image offline, inference rendering results displayed/saved locally on the Web"})]})})]}),"\n",(0,i.jsx)(n.h2,{id:"preparation",children:"Preparation"}),"\n",(0,i.jsx)(n.h3,{id:"rdk",children:"RDK"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"The RDK has burned the  Ubuntu 20.04/22.04 system image provided by D-Robotics."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"The RDK has successfully installed TogetheROS.Bot."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,i.jsx)(n.p,{children:"The package publishes algorithm messages that include semantic segmentation and object detection information, and users can subscribe to these messages for application development."}),"\n",(0,i.jsx)(n.h3,{id:"rdk-1",children:"RDK"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Publishing images from MIPI camera"})}),"\n",(0,i.jsxs)(o.A,{groupId:"tros-distro",children:[(0,i.jsx)(s.A,{value:"foxy",label:"Foxy",children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"# Configure the tros.b environment\nsource /opt/tros/setup.bash\n\n# Copy the configuration file required for running the example from the installation path of tros.b.\ncp -r /opt/tros/${TROS_DISTRO}/lib/parking_perception/config/ .\n\n# Configuring MIPI camera\nexport CAM_TYPE=mipi\n\n# Start the launch file\nros2 launch parking_perception parking_perception.launch.py \n"})})}),(0,i.jsx)(s.A,{value:"humble",label:"Humble",children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"# Configure the tros.b environment\nsource /opt/tros/humble/setup.bash\n\n# Copy the configuration file required for running the example from the installation path of tros.b.\ncp -r /opt/tros/${TROS_DISTRO}/lib/parking_perception/config/ .\n\n# Configuring MIPI camera\nexport CAM_TYPE=mipi\n\n# Start the launch file\nros2 launch parking_perception parking_perception.launch.py \n"})})})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Publishing images from USB camera"})}),"\n",(0,i.jsxs)(o.A,{groupId:"tros-distro",children:[(0,i.jsx)(s.A,{value:"foxy",label:"Foxy",children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"# Configure the tros.b environment\nsource /opt/tros/setup.bash\n\n# Copy the configuration file required for running the example from the installation path of tros.b.\ncp -r /opt/tros/${TROS_DISTRO}/lib/parking_perception/config/ .\n\n# Configuring USB camera\nexport CAM_TYPE=usb\n\n# Start the launch file\nros2 launch parking_perception parking_perception.launch.py \n"})})}),(0,i.jsx)(s.A,{value:"humble",label:"Humble",children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"# Configure the tros.b environment\nsource /opt/tros/humble/setup.bash\n\n# Copy the configuration file required for running the example from the installation path of tros.b.\ncp -r /opt/tros/${TROS_DISTRO}/lib/parking_perception/config/ .\n\n# Configuring USB camera\nexport CAM_TYPE=usb\n\n# Start the launch file\nros2 launch parking_perception parking_perception.launch.py \n"})})})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Using a single image offline"})}),"\n",(0,i.jsxs)(o.A,{groupId:"tros-distro",children:[(0,i.jsx)(s.A,{value:"foxy",label:"Foxy",children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"# Configure the tros.b environment\nsource /opt/tros/setup.bash\n\n# Copy the configuration file required for running the example from the installation path of tros.b.\ncp -r /opt/tros/${TROS_DISTRO}/lib/parking_perception/config/ .\n\n# Configure the local playback image.\nexport CAM_TYPE=fb\n\n# Start the launch file\nros2 launch parking_perception parking_perception.launch.py \n"})})}),(0,i.jsx)(s.A,{value:"humble",label:"Humble",children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"# Configure the tros.b environment\nsource /opt/tros/humble/setup.bash\n\n# Copy the configuration file required for running the example from the installation path of tros.b.\ncp -r /opt/tros/${TROS_DISTRO}/lib/parking_perception/config/ .\n\n# Configure the local playback image.\nexport CAM_TYPE=fb\n\n# Start the launch file\nros2 launch parking_perception parking_perception.launch.py \n"})})})]}),"\n",(0,i.jsx)(n.h2,{id:"result-analysis",children:"Result Analysis"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Using a MIPI camera to publish images"})}),"\n",(0,i.jsx)(n.p,{children:"After the package is initialized, the following information will be displayed in the terminal:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"[INFO] [launch]: All log files can be found below /root/.ros/log/2022-08-02-06-46-55-605266-ubuntu-3669\n[INFO] [launch]: Default logging verbosity is set to INFO\n[INFO] [mipi_cam-1]: process started with pid [3671]\n[INFO] [hobot_codec_republish-2]: process started with pid [3673]\n[INFO] [parking_perception-3]: process started with pid [3675]\n[INFO] [websocket-4]: process started with pid [3677]\n[parking_perception-3] [WARN] [1659394017.194211788] [parking_perception]: Parameter:\n[parking_perception-3] shared_men:1\n[parking_perception-3]  is_sync_mode_: 1\n[parking_perception-3]  model_file_name_: config/parking_perception_640x320.bin\n[parking_perception-3] feed_image:\n[parking_perception-3] [INFO] [1659394017.194695288] [dnn]: Node init.\n[parking_perception-3] [INFO] [1659394017.194784038] [parking_perception]: Set node para.\n[parking_perception-3] [INFO] [1659394017.194845413] [dnn]: Model init.\n[parking_perception-3] [BPU_PLAT]BPU Platform Version(1.3.1)!\n[parking_perception-3] [C][3675][08-02][06:46:57:202][configuration.cpp:49][EasyDNN]EasyDNN version: 0.4.11\n[parking_perception-3] [HBRT] set log level as 0. version = 3.14.5\n[parking_perception-3] [DNN] Runtime version = 1.9.7_(3.14.5 HBRT)\n[parking_perception-3] [INFO] [1659394017.247423580] [dnn]: The model input 0 width is 640 and height is 320\n[parking_perception-3] [INFO] [1659394017.247664997] [dnn]: Task init.\n[parking_perception-3] [INFO] [1659394017.255848788] [dnn]: Set task_num [2]\n[parking_perception-3] [INFO] [1659394017.255999663] [parking_perception]: The model input width is 640 and height is 320\n[parking_perception-3] [INFO] [1659394017.263431163] [parking_perception]: msg_pub_topic_name: ai_msg_parking_perception\n[parking_perception-3] [INFO] [1659394017.263554788] [parking_perception]: Detect images that use subscriptions\n[parking_perception-3] [WARN] [1659394017.263597997] [parking_perception]: Create hbmem_subscription with topic_name: /hbmem_img\n[parking_perception-3] [WARN] [1659394017.267204163] [parking_perception]: start success!!!\n[parking_perception-3] [WARN] [1662036456.219133588] [parking_perception]: input fps: 29.73, out fps: 29.79\n[parking_perception-3] [WARN] [1662036457.228303881] [parking_perception]: input fps: 29.73, out fps: 29.73\n[parking_perception-3] [WARN] [1662036458.237841548] [parking_perception]: input fps: 29.73, out fps: 29.73\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Using single image offline"})}),"\n",(0,i.jsxs)(n.p,{children:["The result of inference reading a local image offline in the example will be rendered on the image. On the PC-side browser, you can view the image and algorithm rendering effect by entering ",(0,i.jsx)(n.a,{href:"http://IP:8000",children:"http://IP:8000"})," (IP is the IP address of the RDK), and open the settings in the upper right corner of the interface."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/03_boxs/function/image/box_adv/operation_1.png",alt:""})}),"\n",(0,i.jsx)(n.p,{children:'Select the "Full Image Segmentation" option to display the rendering effect.'}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/03_boxs/function/image/box_adv/operation_2.png",alt:""})}),"\n",(0,i.jsx)(n.p,{children:"From the visualization result, we can see that the parking area and driving area in the outdoor scene are effectively segmented, distinguishing the parking lane from the driving lane, and the object detection task also locates the vehicles in the distance."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/03_boxs/function/image/box_adv/render_parking.png",alt:""})}),"\n",(0,i.jsx)(n.p,{children:'When "dump_render_img" is set to "1", the rendering effect will be saved in the "result" directory at the current path.'})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(u,{...e})}):u(e)}}}]);