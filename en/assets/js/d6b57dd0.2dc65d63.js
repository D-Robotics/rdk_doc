"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[93179],{11470:(e,n,o)=>{o.d(n,{A:()=>v});var t=o(96540),s=o(34164),r=o(23104),a=o(56347),i=o(205),d=o(57485),l=o(31682),c=o(70679);function u(e){return t.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,t.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function p(e){const{values:n,children:o}=e;return(0,t.useMemo)(()=>{const e=n??function(e){return u(e).map(({props:{value:e,label:n,attributes:o,default:t}})=>({value:e,label:n,attributes:o,default:t}))}(o);return function(e){const n=(0,l.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,o])}function h({value:e,tabValues:n}){return n.some(n=>n.value===e)}function m({queryString:e=!1,groupId:n}){const o=(0,a.W6)(),s=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,d.aZ)(s),(0,t.useCallback)(e=>{if(!s)return;const n=new URLSearchParams(o.location.search);n.set(s,e),o.replace({...o.location,search:n.toString()})},[s,o])]}function _(e){const{defaultValue:n,queryString:o=!1,groupId:s}=e,r=p(e),[a,d]=(0,t.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!h({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const o=n.find(e=>e.default)??n[0];if(!o)throw new Error("Unexpected error: 0 tabValues");return o.value}({defaultValue:n,tabValues:r})),[l,u]=m({queryString:o,groupId:s}),[_,b]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[o,s]=(0,c.Dv)(n);return[o,(0,t.useCallback)(e=>{n&&s.set(e)},[n,s])]}({groupId:s}),f=(()=>{const e=l??_;return h({value:e,tabValues:r})?e:null})();(0,i.A)(()=>{f&&d(f)},[f]);return{selectedValue:a,selectValue:(0,t.useCallback)(e=>{if(!h({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);d(e),u(e),b(e)},[u,b,r]),tabValues:r}}var b=o(92303);const f={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var g=o(74848);function x({className:e,block:n,selectedValue:o,selectValue:t,tabValues:a}){const i=[],{blockElementScrollPositionUntilNextRender:d}=(0,r.a_)(),l=e=>{const n=e.currentTarget,s=i.indexOf(n),r=a[s].value;r!==o&&(d(n),t(r))},c=e=>{let n=null;switch(e.key){case"Enter":l(e);break;case"ArrowRight":{const o=i.indexOf(e.currentTarget)+1;n=i[o]??i[0];break}case"ArrowLeft":{const o=i.indexOf(e.currentTarget)-1;n=i[o]??i[i.length-1];break}}n?.focus()};return(0,g.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":n},e),children:a.map(({value:e,label:n,attributes:t})=>(0,g.jsx)("li",{role:"tab",tabIndex:o===e?0:-1,"aria-selected":o===e,ref:e=>{i.push(e)},onKeyDown:c,onClick:l,...t,className:(0,s.A)("tabs__item",f.tabItem,t?.className,{"tabs__item--active":o===e}),children:n??e},e))})}function y({lazy:e,children:n,selectedValue:o}){const r=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=r.find(e=>e.props.value===o);return e?(0,t.cloneElement)(e,{className:(0,s.A)("margin-top--md",e.props.className)}):null}return(0,g.jsx)("div",{className:"margin-top--md",children:r.map((e,n)=>(0,t.cloneElement)(e,{key:n,hidden:e.props.value!==o}))})}function j(e){const n=_(e);return(0,g.jsxs)("div",{className:(0,s.A)("tabs-container",f.tabList),children:[(0,g.jsx)(x,{...n,...e}),(0,g.jsx)(y,{...n,...e})]})}function v(e){const n=(0,b.A)();return(0,g.jsx)(j,{...e,children:u(e.children)},String(n))}},19365:(e,n,o)=>{o.d(n,{A:()=>a});o(96540);var t=o(34164);const s={tabItem:"tabItem_Ymn6"};var r=o(74848);function a({children:e,hidden:n,className:o}){return(0,r.jsx)("div",{role:"tabpanel",className:(0,t.A)(s.tabItem,o),hidden:n,children:e})}},28453:(e,n,o)=>{o.d(n,{R:()=>a,x:()=>i});var t=o(96540);const s={},r=t.createContext(s);function a(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(r.Provider,{value:n},e.children)}},47471:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>d,contentTitle:()=>i,default:()=>u,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"Robot_development/boxs/function/mono2d_yolo_pose","title":"Human Detection and Tracking (Yolo-Pose)","description":"Overview","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/05_Robot_development/03_boxs/function/mono2d_yolo_pose.md","sourceDirName":"05_Robot_development/03_boxs/function","slug":"/Robot_development/boxs/function/mono2d_yolo_pose","permalink":"/rdk_doc/en/Robot_development/boxs/function/mono2d_yolo_pose","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1751533161000,"sidebarPosition":21,"frontMatter":{"sidebar_position":21},"sidebar":"tutorialSidebar","previous":{"title":"DOSOD","permalink":"/rdk_doc/en/Robot_development/boxs/function/hobot_dosod"},"next":{"title":"Monocular Elevation Network","permalink":"/rdk_doc/en/Robot_development/boxs/function/elevation_net"}}');var s=o(74848),r=o(28453);o(11470),o(19365);const a={sidebar_position:21},i="Human Detection and Tracking (Yolo-Pose)",d={},l=[{value:"Overview",id:"overview",level:2},{value:"Supported Platforms",id:"supported-platforms",level:2},{value:"Preparation",id:"preparation",level:2},{value:"RDK Platform",id:"rdk-platform",level:3},{value:"Usage",id:"usage",level:2},{value:"RDK Platform",id:"rdk-platform-1",level:3},{value:"Result Analysis",id:"result-analysis",level:2}];function c(e){const n={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"human-detection-and-tracking-yolo-pose",children:"Human Detection and Tracking (Yolo-Pose)"})}),"\n","\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsxs)(n.p,{children:["This example uses ",(0,s.jsx)(n.a,{href:"https://docs.ultralytics.com/tasks/pose/",children:"yolo-pose"})," for human detection and tracking. The algorithm subscribes to image topics, performs inference using the BPU, and publishes messages containing human bounding boxes and keypoint detection results. Multi-target tracking (MOT) is also supported to track detected objects."]}),"\n",(0,s.jsx)(n.p,{children:"The supported detection categories and their corresponding data types in the algorithm messages are as follows:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Category"}),(0,s.jsx)(n.th,{children:"Description"}),(0,s.jsx)(n.th,{children:"Data Type"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"body"}),(0,s.jsx)(n.td,{children:"Human bounding box"}),(0,s.jsx)(n.td,{children:"Roi"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"body_kps"}),(0,s.jsx)(n.td,{children:"Human keypoints"}),(0,s.jsx)(n.td,{children:"Point"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"The keypoint index mapping is shown below:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/03_boxs/function/image/box_adv/kps_yolo_index.jpeg",alt:""})}),"\n",(0,s.jsxs)(n.p,{children:["Code repository: ",(0,s.jsx)(n.a,{href:"https://github.com/D-Robotics/mono2d_body_detection",children:"https://github.com/D-Robotics/mono2d_body_detection"})]}),"\n",(0,s.jsx)(n.p,{children:"Application scenarios: Human detection and tracking algorithms are essential for visual analysis of human motion, enabling pose analysis and people counting. They are mainly used in human-computer interaction, gaming, and entertainment."}),"\n",(0,s.jsxs)(n.p,{children:["Pose detection example: ",(0,s.jsx)(n.a,{href:"../../apps/fall_detection",children:"4.3. Pose Detection"}),(0,s.jsx)(n.br,{}),"\n","Robot following example: ",(0,s.jsx)(n.a,{href:"../../apps/car_tracking",children:"4.4. Robot Human Following"}),(0,s.jsx)(n.br,{}),"\n","Game character control using pose and gesture recognition: ",(0,s.jsx)(n.a,{href:"https://developer.d-robotics.cc/forumDetail/112555512834430487",children:"Play with X3Pi, Fitness and Gaming Combined"})]}),"\n",(0,s.jsx)(n.h2,{id:"supported-platforms",children:"Supported Platforms"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Platform"}),(0,s.jsx)(n.th,{children:"OS/ROS Version"}),(0,s.jsx)(n.th,{children:"Example Functionality"})]})}),(0,s.jsx)(n.tbody,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"RDK S100"}),(0,s.jsx)(n.td,{children:"Ubuntu 22.04 (Humble)"}),(0,s.jsx)(n.td,{children:"Start MIPI/USB camera and display inference results on Web"})]})})]}),"\n",(0,s.jsx)(n.h2,{id:"preparation",children:"Preparation"}),"\n",(0,s.jsx)(n.h3,{id:"rdk-platform",children:"RDK Platform"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"RDK is flashed with Ubuntu 22.04 system image."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"TogetheROS.Bot is installed on RDK."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"MIPI or USB camera is connected to RDK."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Ensure the PC can access the RDK via network."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,s.jsxs)(n.p,{children:["The human detection and tracking (",(0,s.jsx)(n.code,{children:"mono2d_body_detection"}),") package subscribes to images published by the sensor package, performs inference, and publishes algorithm messages. The results can be visualized in a browser via the websocket package."]}),"\n",(0,s.jsx)(n.h3,{id:"rdk-platform-1",children:"RDK Platform"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Using MIPI Camera"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"source /opt/tros/humble/setup.bash\n\n# Copy the required config files from the installation path.\ncp -r /opt/tros/${TROS_DISTRO}/lib/mono2d_body_detection/config/ .\n\n# Set camera type to MIPI\nexport CAM_TYPE=mipi\n\n# Launch the example\nros2 launch mono2d_body_detection mono2d_body_detection.launch.py kps_model_type:=1 kps_image_width:=640 kps_image_height:=640 kps_model_file_name:=config/yolo11x_pose_nashe_640x640_nv12.hbm\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Using USB Camera"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"source /opt/tros/humble/setup.bash\n\n# Copy the required config files from the installation path.\ncp -r /opt/tros/${TROS_DISTRO}/lib/mono2d_body_detection/config/ .\n\n# Set camera type to USB\nexport CAM_TYPE=usb\n\n# Launch the example\nros2 launch mono2d_body_detection mono2d_body_detection.launch.py kps_model_type:=1 kps_image_width:=640 kps_image_height:=640 kps_model_file_name:=config/yolo11x_pose_nashe_640x640_nv12.hbm\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Using Local Image Playback"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"source /opt/tros/humble/setup.bash\n\n# Copy the required config files from the installation path.\ncp -r /opt/tros/${TROS_DISTRO}/lib/mono2d_body_detection/config/ .\ncp -r /opt/tros/${TROS_DISTRO}/lib/dnn_node_example/config/ .\n\n# Set camera type to framebuffer (local image)\nexport CAM_TYPE=fb\n\n# Launch the example\nros2 launch mono2d_body_detection mono2d_body_detection.launch.py publish_image_source:=config/person_body.jpg publish_image_format:=jpg kps_model_type:=1 kps_image_width:=640 kps_image_height:=640 kps_model_file_name:=config/yolo11x_pose_nashe_640x640_nv12.hbm\n"})}),"\n",(0,s.jsx)(n.h2,{id:"result-analysis",children:"Result Analysis"}),"\n",(0,s.jsx)(n.p,{children:"The following output indicates successful operation:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"[mono2d_body_detection-3] [WARN] [1660219823.214730286] [example]: This is mono2d body det example!\n[mono2d_body_detection-3] [WARN] [1747724998.166714029] [mono2d_body_det]: Parameter:\n[mono2d_body_detection-3]  is_sync_mode_: 0\n[mono2d_body_detection-3]  model_file_name_: config/yolo11x_pose_nashe_640x640_nv12.hbm\n[mono2d_body_detection-3]  is_shared_mem_sub: 1\n[mono2d_body_detection-3]  ai_msg_pub_topic_name: /hobot_mono2d_body_detection\n[mono2d_body_detection-3]  ros_img_topic_name: /image_raw\n[mono2d_body_detection-3]  image_gap: 1\n[mono2d_body_detection-3]  dump_render_img: 0\n[mono2d_body_detection-3]  model_type: 1\n[mono2d_body_detection-3] [BPU][[BPU_MONITOR]][281473010090784][INFO]BPULib verison(2, 1, 2)[0d3f195]!\n[mono2d_body_detection-3] [DNN] HBTL_EXT_DNN log level:6\n[mono2d_body_detection-3] [DNN]: 3.3.3_(4.1.17 HBRT)\n[mono2d_body_detection-3] [WARN] [1747724998.912552895] [mono2d_body_det]: Get model name: yolo11x_pose_nashe_640x640_nv12 from load model.\n[mono2d_body_detection-3] [WARN] [1747724998.916663825] [mono2d_body_det]: Enabling zero-copy\n[mono2d_body_detection-3] [WARN] [1747724998.916748774] [mono2d_body_det]: Create hbmem_subscription with topic_name: /hbmem_img\n[mono2d_body_detection-3] [WARN] [1660219824.895102286] [mono2d_body_det]: input fps: 31.34, out fps: 31.22\n[mono2d_body_detection-3] [WARN] [1660219825.921873870] [mono2d_body_det]: input fps: 30.16, out fps: 30.21\n[mono2d_body_detection-3] [WARN] [1660219826.922075496] [mono2d_body_det]: input fps: 30.16, out fps: 30.00\n[mono2d_body_detection-3] [WARN] [1660219827.955463330] [mono2d_body_det]: input fps: 30.01, out fps: 30.01\n[mono2d_body_detection-3] [WARN] [1660219828.955764872] [mono2d_body_det]: input fps: 30.01, out fps: 30.00\n"})}),"\n",(0,s.jsx)(n.p,{children:"The log shows the program is running successfully, with inference input and output at 30fps, and frame rate statistics updated every second."}),"\n",(0,s.jsxs)(n.p,{children:["Open ",(0,s.jsx)(n.code,{children:"http://IP:8000"})," in your browser (replace ",(0,s.jsx)(n.code,{children:"IP"})," with the RDK/X86 device IP address) to view the rendered images and algorithm results (human, head, face, hand detection boxes, box types, tracking IDs, and keypoints):"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/03_boxs/function/image/box_adv/yolo_pose_render.png",alt:""})})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}}}]);