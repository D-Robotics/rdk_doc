"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[99622],{28453:(e,n,o)=>{o.d(n,{R:()=>a,x:()=>s});var t=o(96540);const i={},r=t.createContext(i);function a(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(r.Provider,{value:n},e.children)}},78144:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>d,contentTitle:()=>s,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"Advanced_development/toolchain_development/expert/advanced_content","title":"Deep Exploration","description":"Customizing qconfig","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/07_Advanced_development/04_toolchain_development/expert/advanced_content.md","sourceDirName":"07_Advanced_development/04_toolchain_development/expert","slug":"/Advanced_development/toolchain_development/expert/advanced_content","permalink":"/rdk_doc/en/Advanced_development/toolchain_development/expert/advanced_content","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1753278649000,"sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Development Guide","permalink":"/rdk_doc/en/Advanced_development/toolchain_development/expert/user_guide"},"next":{"title":"API Manual","permalink":"/rdk_doc/en/Advanced_development/toolchain_development/expert/api_reference"}}');var i=o(74848),r=o(28453);const a={sidebar_position:4},s="Deep Exploration",d={},l=[{value:"Customizing qconfig",id:"customizing-qconfig",level:2},{value:"Quantization information of Activation",id:"quantization-information-of-activation",level:3},{value:"Quantization information of Weight",id:"quantization-information-of-weight",level:3},{value:"QConfig",id:"qconfig",level:3},{value:"Introduction to FX Quantization",id:"introduction-to-fx-quantization",level:2},{value:"Quantization Process",id:"quantization-process",level:3},{value:"Fuse (Optional)",id:"fuse-optional",level:4},{value:"Prepare",id:"prepare",level:4},{value:"Convert",id:"convert",level:4},{value:"Eager Mode Compatibility",id:"eager-mode-compatibility",level:4},{value:"RGB888 Data Deployment",id:"rgb888-data-deployment",level:2},{value:"Scenario",id:"scenario",level:3},{value:"Brief on YUV Format",id:"brief-on-yuv-format",level:3},{value:"Preprocessing RGB Input during Training",id:"preprocessing-rgb-input-during-training",level:3},{value:"Real-time Conversion of YUV Input during Inference",id:"real-time-conversion-of-yuv-input-during-inference",level:3},{value:"Insertion of the Operator",id:"insertion-of-the-operator",level:4},{value:"Model Segmented Deployment",id:"model-segmented-deployment",level:2},{value:"Scenario",id:"scenario-1",level:3},{value:"Method",id:"method",level:3},{value:"Operator Fusion",id:"op_fusion",level:2},{value:"Fusion of Add and ReLU(6)",id:"fusion-of-add-and-relu6",level:3},{value:"Operator Fusion",id:"op_fusion",level:3},{value:"Absorbing Batch Normalization (BN)",id:"absorbing-batch-normalization-bn",level:3},{value:"Fusing Add, ReLU(6)",id:"fusing-add-relu6",level:3},{value:"Implementation Principle",id:"implementation-principle",level:3},{value:"Supported Operator Combinations",id:"supported-operator-combinations",level:3}];function c(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"deep-exploration",children:"Deep Exploration"})}),"\n",(0,i.jsx)(n.h2,{id:"customizing-qconfig",children:"Customizing qconfig"}),"\n",(0,i.jsx)(n.p,{children:"Customizing qconfig requires users to have a clear understanding of the specific processor restrictions, a detailed understanding of the working principles of the training tools, and a detailed understanding of how to reflect the processor restrictions through qconfig. Quantization training requires a certain training cost, and errors in qconfig definition may result in the model not being able to converge properly or the model not being able to compile. Therefore, it is not recommended for ordinary users to customize qconfig."}),"\n",(0,i.jsxs)(n.p,{children:["The horizon_plugin_pytorch uses the partial function method provided by PyTorch to define qconfig. For the usage of this method, please refer to the ",(0,i.jsx)(n.a,{href:"https://github.com/pytorch/pytorch/blob/v2.0.0/torch/ao/quantization/observer.py#L77",children:"official documentation"}),". Users who are not familiar with this method should learn it before continuing to read."]}),"\n",(0,i.jsx)(n.p,{children:"Currently, qconfig handles two types of information:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Quantization information of activation"}),"\n",(0,i.jsx)(n.li,{children:"Quantization information of weight"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"quantization-information-of-activation",children:"Quantization information of Activation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'activation_8bit_fake_quant = FakeQuantize.with_args(\n                         observer=MovingAveragePerTensorMinMaxObserver,\n                         dtype="qint8",\n                         ch_axis=0,\n                         averaging_constant=0 # Custom parameter for observer\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"quantization-information-of-weight",children:"Quantization information of Weight"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'weight_8bit_fake_quant = FakeQuantize.with_args(\n                         observer=MovingAveragePerChannelMinMaxObserver,\n                         dtype="qint8",\n                         ch_axis=0,\n                         averaging_constant=1 # Custom parameter for observer\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"qconfig",children:"QConfig"}),"\n",(0,i.jsxs)(n.p,{children:["By encapsulating the quantization information of activation and weight using ",(0,i.jsx)(n.code,{children:"Qconfig"}),", qconfig can be obtained."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"qat_8bit_qconfig = QConfig(\n    activation=activation_8bit_fake_quant, weight=weight_8bit_fake_quant\n)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"introduction-to-fx-quantization",children:"Introduction to FX Quantization"}),"\n",(0,i.jsxs)(n.p,{children:["TheoryBefore reading this document, it is recommended to read ",(0,i.jsx)(n.a,{href:"https://pytorch.org/docs/stable/fx.html",children:"torch.fx \u2014 PyTorch documentation"})," to have a preliminary understanding of the FX mechanism in PyTorch."]}),"\n",(0,i.jsxs)(n.p,{children:["FX adopts a symbolic execution approach to build graphs at the level of ",(0,i.jsx)(n.code,{children:"nn.Module"})," or functions, enabling automated fusion and other graph-based optimizations."]}),"\n",(0,i.jsx)(n.h3,{id:"quantization-process",children:"Quantization Process"}),"\n",(0,i.jsx)(n.h4,{id:"fuse-optional",children:"Fuse (Optional)"}),"\n",(0,i.jsx)(n.p,{children:"FX is able to perceive the computation graph, allowing for automated operator fusion. Users no longer need to manually specify the operators to be fused; they can simply call the interface."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"fused_model = horizon.quantization.fuse_fx(model)\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Note that ",(0,i.jsx)(n.code,{children:"fuse_fx"})," does not have an ",(0,i.jsx)(n.code,{children:"inplace"})," parameter because it needs to perform symbolic tracing on the model to generate a ",(0,i.jsx)(n.code,{children:"GraphModule"}),", thus in-place modification is not possible."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"fused_model"})," and ",(0,i.jsx)(n.code,{children:"model"})," share almost all attributes (including sub-modules and operators), so please refrain from modifying ",(0,i.jsx)(n.code,{children:"model"})," after fusion, as it may affect ",(0,i.jsx)(n.code,{children:"fused_model"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Users do not need to call the ",(0,i.jsx)(n.code,{children:"fuse_fx"})," interface explicitly, as the subsequent ",(0,i.jsx)(n.code,{children:"prepare_qat_fx"})," interface internally integrates the fusion process."]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"prepare",children:"Prepare"}),"\n",(0,i.jsxs)(n.p,{children:["Before calling the ",(0,i.jsx)(n.code,{children:"prepare_qat_fx"})," interface, users must set the global march according to the target hardware platform. The interface will first perform the fusion process (even if the model has already been fused) and then replace the qualifying operators in the model with implementations from ",(0,i.jsx)(n.code,{children:"horizon.nn.qat"}),"."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Users can choose the appropriate qconfig (Calibration or QAT), but note that the two qconfigs cannot be mixed."}),"\n",(0,i.jsxs)(n.li,{children:["Similar to ",(0,i.jsx)(n.code,{children:"fuse_fx"}),", this interface does not support the ",(0,i.jsx)(n.code,{children:"inplace"})," parameter, and refrain from any modifications to the input model after ",(0,i.jsx)(n.code,{children:"prepare_qat_fx"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Set march to BERNOULLI2 for RDK X3, and to BAYES for RDK Ultra.\nhorizon.march.set_march(horizon.march.March.BAYES)\nqat_model = horizon.quantization.prepare_qat_fx(\n    model,\n    {\n        "": horizon.qconfig.default_calib_8bit_fake_quant_qconfig,\n        "module_name": {\n            "<module_name>": custom_qconfig,\n        },\n    },)\n'})}),"\n",(0,i.jsx)(n.h4,{id:"convert",children:"Convert"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Similar to ",(0,i.jsx)(n.code,{children:"fuse_fx"}),", this interface does not support the ",(0,i.jsx)(n.code,{children:"inplace"})," parameter, and refrain from any modifications to the input model after ",(0,i.jsx)(n.code,{children:"convert_fx"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"quantized_model = horizon.quantization.convert_fx(qat_model)\n"})}),"\n",(0,i.jsx)(n.h4,{id:"eager-mode-compatibility",children:"Eager Mode Compatibility"}),"\n",(0,i.jsxs)(n.p,{children:["In most cases, the quantization interfaces in FX can directly replace the quantization interfaces in eager mode (",(0,i.jsx)(n.code,{children:"prepare_qat"})," -> ",(0,i.jsx)(n.code,{children:"prepare_qat_fx"}),", ",(0,i.jsx)(n.code,{children:"convert"})," -> ",(0,i.jsx)(n.code,{children:"convert_fx"}),"). However, they cannot be mixed with the interfaces in eager mode. Some models may require modifications in the code structure under the following circumstances.- Unsupported operations in FX: The operations supported by torch's symbolic trace are limited, for example, it does not support using non-static variables as conditional statements, and default does not support packages outside of torch (such as numpy). Additionally, unexecuted conditional branches will be discarded."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Operations to avoid being handled by FX: If torch operations are used in the pre and post-processing of the model, FX will treat them as part of the model during trace, which may lead to unexpected behavior (e.g., replacing certain function calls with FloatFunctional)."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:'Both of these situations can be avoided using the "wrap" method, illustrated below using RetinaNet as an example.'}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from horizon_plugin_pytorch.utils.fx_helper import wrap as fx_wrap\n\nclass RetinaNet(nn.Module):\n    def __init__(\n        self,\n        backbone: nn.Module,\n        neck: Optional[nn.Module] = None,\n        head: Optional[nn.Module] = None,\n        anchors: Optional[nn.Module] = None,\n        targets: Optional[nn.Module] = None,\n        post_process: Optional[nn.Module] = None,\n        loss_cls: Optional[nn.Module] = None,\n        loss_reg: Optional[nn.Module] = None,\n    ):\n        super(RetinaNet, self).__init__()\n\n        self.backbone = backbone\n        self.neck = neck\n        self.head = head\n        self.anchors = anchors\n        self.targets = targets\n        self.post_process = post_process\n        self.loss_cls = loss_cls\n        self.loss_reg = loss_reg\n\n    def rearrange_head_out(self, inputs: List[torch.Tensor], num: int):\n        outputs = []\n        for t in inputs:\n            outputs.append(t.permute(0, 2, 3, 1).reshape(t.shape[0], -1, num))\n        return torch.cat(outputs, dim=1)\n\n    def forward(self, data: Dict):\n        feat = self.backbone(data["img"])\n        feat = self.neck(feat) if self.neck else feat\n        cls_scores, bbox_preds = self.head(feat)\n\n        if self.post_process is None:\n            return cls_scores, bbox_preds\n\n        # Wrap the operations that do not need to be traced into a method. FX will no longer focus on the logic inside the method,\n        # only preserving it as it is (the modules called within the method can still be set with qconfig, and can be replaced\n        # by prepare_qat_fx and convert_fx)\n        return self._post_process( data, feat, cls_scores, bbox_preds)\n        \n        @ fx_warp() # fx_wrap supports directly decorate class method\n        def _post_process(self, data, feat, cls_scores, bbox_preds)\n        \xa0\xa0\xa0\xa0anchors = self.anchors(feat)\n\n        \xa0\xa0\xa0\xa0# The judgment of self.training must be encapsulated, otherwise, after the symbolic trace, this judgment\n        \xa0\xa0\xa0\xa0# The logic will be lost\n        \xa0\xa0\xa0\xa0if self.training:\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0cls_scores = self.rearrange_head_out(\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0cls_scores, self.head.num_classes\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0)\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0bbox_preds = self.rearrange_head_out(bbox_preds, 4)\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0gt_labels = [\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0torch.cat(\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0[data["gt_bboxes"][i], data["gt_classes"][i][:, None] + 1],\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0dim=-1,\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0)\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0for i in range(len(data["gt_classes"]))\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0]\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0gt_labels = [gt_label.float() for gt_label in gt_labels]\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0_, labels = self.targets(anchors, gt_labels)\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0avg_factor = labels["reg_label_mask"].sum()\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0if avg_factor == 0:\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0avg_factor += 1\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0cls_loss = self.loss_cls(\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0pred=cls_scores.sigmoid(),\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0target=labels["cls_label"],\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0weight=labels["cls_label_mask"],\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0avg_factor=avg_factor,\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0)\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0reg_loss = self.loss_reg(\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0pred=bbox_preds,\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0target=labels["reg_label"],\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0weight=labels["reg_label_mask"],\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0avg_factor=avg_factor,\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0)\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0return {\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0"cls_loss": cls_loss,\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0"reg_loss": reg_loss,\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0}\n        \xa0\xa0\xa0\xa0else:\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0preds = self.post_process(\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0anchors,\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0cls_scores,\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0bbox_preds,\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0[torch.tensor(shape) for shape in data["resized_shape"]],\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0)\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0assert (\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0"pred_bboxes" not in data.keys()\n        \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0), "pred_bboxes has been in data.keys()"data["pred_bboxes"] = preds\n        return data\n'})}),"\n",(0,i.jsx)(n.h2,{id:"rgb888-data-deployment",children:"RGB888 Data Deployment"}),"\n",(0,i.jsx)(n.h3,{id:"scenario",children:"Scenario"}),"\n",(0,i.jsx)(n.p,{children:"In the BPU, the output images from the image pyramid are in a centered YUV444 format with a data range of [-128, 127]. However, your training dataset might be in RGB format, which requires preprocessing to align with the BPU's input requirements. During training, it's recommended to convert RGB images to YUV to ensure compatibility with the model's inference pipeline."}),"\n",(0,i.jsx)(n.p,{children:"Since the compiler currently doesn't support color space conversions, users can manually insert color space conversion nodes to bypass these limitations."}),"\n",(0,i.jsx)(n.h3,{id:"brief-on-yuv-format",children:"Brief on YUV Format"}),"\n",(0,i.jsx)(n.p,{children:"YUV is commonly used to describe color spaces in analog television systems. In BT.601, YUV has two standards: YUV studio swing (Y: 16-235, UV: 16-240) and YUV full swing (YUV: 0-255). The BPU supports full swing YUV."}),"\n",(0,i.jsx)(n.h3,{id:"preprocessing-rgb-input-during-training",children:"Preprocessing RGB Input during Training"}),"\n",(0,i.jsxs)(n.p,{children:["When training, you can use ",(0,i.jsx)(n.code,{children:"horizon.functional.rgb2centered_yuv"})," or ",(0,i.jsx)(n.code,{children:"horizon.functional.bgr2centered_yuv"})," to convert RGB images to the BPU-supported YUV format. For example, the ",(0,i.jsx)(n.code,{children:"rgb2centered_yuv"})," function definition is as follows:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def rgb2centered_yuv(input: Tensor, swing: str = "studio") -> Tensor:\n    """Convert color space.\n\n    Convert images from RGB format to centered YUV444 BT.601\n\n    Args:\n        input: input image in RGB format, ranging 0~255\n        swing: "studio" for YUV studio swing (Y: -112~107,\n                U, V: -112~112)\n                "full" for YUV full swing (Y, U, V: -128~127).\n                default is "studio"\n\n    Returns:\n        output: centered YUV image\n    """\n'})}),"\n",(0,i.jsxs)(n.p,{children:["The input is an RGB image, and the output is a centered YUV image. To match the BPU data flow format, ",(0,i.jsxs)(n.strong,{children:["set ",(0,i.jsx)(n.code,{children:"swing"}),' to "full"`']}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"real-time-conversion-of-yuv-input-during-inference",children:"Real-time Conversion of YUV Input during Inference"}),"\n",(0,i.jsxs)(n.p,{children:["We recommend converting RGB images to YUV during training to avoid extra overhead and accuracy loss during inference. However, if you've trained with RGB images, you can use ",(0,i.jsx)(n.code,{children:"horizon.functional.centered_yuv2rgb"})," or ",(0,i.jsx)(n.code,{children:"horizon.functional.centered_yuv2bgr"})," for on-the-fly conversion at inference time. These functions should be inserted after the QuantStub in your model."]}),"\n",(0,i.jsxs)(n.p,{children:["For instance, the ",(0,i.jsx)(n.code,{children:"centered_yuv2rgb"})," operator definition looks like this:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def centered_yuv2rgb(\n    input: QTensor,\n    swing: str = "studio",\n    mean: Union[List[float], Tensor] = (128.0,),\n    std: Union[List[float], Tensor] = (128.0,),\n    q_scale: Union[float, Tensor] = 1.0 / 128.0,\n) -> QTensor:\n'})}),"\n",(0,i.jsxs)(n.p,{children:["To align with BPU's YUV format, ",(0,i.jsxs)(n.strong,{children:["set ",(0,i.jsx)(n.code,{children:"swing"}),' to "full"`']}),"."]}),"\n",(0,i.jsx)(n.p,{children:"The operator adjusts the input according to the following steps:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Converts the image to RGB using the formula corresponding to the given ",(0,i.jsx)(n.code,{children:"swing"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Normalizes the RGB image using provided ",(0,i.jsx)(n.code,{children:"mean"})," and ",(0,i.jsx)(n.code,{children:"std"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Quantizes the RGB image using the given ",(0,i.jsx)(n.code,{children:"q_scale"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/expert/yuv1.svg",alt:"yuv1"})}),"\n",(0,i.jsx)(n.admonition,{title:"Caution",type:"caution",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Note"}),": This operator is designed specifically for deployment and should not be used during training."]})}),"\n",(0,i.jsx)(n.h4,{id:"insertion-of-the-operator",children:"Insertion of the Operator"}),"\n",(0,i.jsx)(n.p,{children:"To integrate this operator into your model, follow these steps after QAT model conversion:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Retrieve the scale value from the QuantStub and the normalization parameters used during training."}),"\n",(0,i.jsxs)(n.li,{children:["Convert the QAT model to a quantized model using ",(0,i.jsx)(n.code,{children:"convert_fx"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Insert the ",(0,i.jsx)(n.code,{children:"centered_yuv2rgb"})," operator after the QuantStub, providing the gathered parameters."]}),"\n",(0,i.jsxs)(n.li,{children:["Manually set the QuantStub's ",(0,i.jsx)(n.code,{children:"scale"})," parameter to 1."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Here's an example:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import torch\nfrom horizon_plugin_pytorch.quantization import (\n    QuantStub,\n    prepare_qat_fx,\n    convert_fx,\n)\nfrom horizon_plugin_pytorch.functional import centered_yuv2rgb\nfrom horizon_plugin_pytorch.quantization.qconfig import (\n    default_qat_8bit_fake_quant_qconfig,\n)\nfrom horizon_plugin_pytorch import set_march\n\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.quant = QuantStub()\n        self.conv = torch.nn.Conv2d(3, 3, 3)\n        self.bn = torch.nn.BatchNorm2d(3)\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, input):\n        x = self.quant(input)\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n    def set_qconfig(self):\n        self.qconfig = default_qat_8bit_fake_quant_qconfig\n\n\ndata = torch.rand(1, 3, 28, 28)\nnet = Net()\n\n# Set march to BERNOULLI2 for RDK X3, and to BAYES for RDK Ultra.\nset_march("bayes")\n\nnet.set_qconfig()\nqat_net = prepare_qat_fx(net)\nqat_net(data)\nquantized_net = convert_fx(qat_net)\ntraced = quantized_net\nprint("Before centered_yuv2rgb")\ntraced.graph.print_tabular()\n\n# Replace QuantStub nodes with centered_yuv2rgb\npatterns = ["quant"]\nfor n in traced.graph.nodes:\n    if any(n.target == pattern for pattern in patterns):\n        with traced.graph.inserting_after(n):\n            new_node = traced.graph.call_function(centered_yuv2rgb, (n,), {"swing": "full"})\n            n.replace_all_uses_with(new_node)\n            new_node.args = (n,)\n\ntraced.quant.scale.fill_(1.0)\ntraced.recompile()\nprint("\\nAfter centered_yuv2rgb")\ntraced.graph.print_tabular()\n'})}),"\n",(0,i.jsx)(n.p,{children:"The graph comparison will show the insertion of the color space conversion node:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sh",children:"Before centered_yuv2rgb\nopcode       name     target    args        kwargs\n-----------  -------  --------  ----------  --------\nplaceholder  input_1  input     ()          {}\ncall_module  quant    quant     (input_1,)  {}\ncall_module  conv     conv      (quant,)    {}\noutput       output   output    (conv,)     {}\n\nAfter centered_yuv2rgb\nopcode         name              target                                         args                 kwargs\n-------------  ----------------  ---------------------------------------------  -------------------  -----------------\nplaceholder    input_1           input                                          ()                   {}\ncall_module    quant             quant                                          (input_1,)           {}\ncall_function  centered_yuv2rgb  <function centered_yuv2rgb at 0x7fa1c2b48040>  (quant,)             {'swing': 'full'}\ncall_module    conv              conv                                           (centered_yuv2rgb,)  {}\noutput         output            output                                         (conv,)              {}\n"})}),"\n",(0,i.jsx)(n.h2,{id:"model-segmented-deployment",children:"Model Segmented Deployment"}),"\n",(0,i.jsx)(n.h3,{id:"scenario-1",children:"Scenario"}),"\n",(0,i.jsx)(n.p,{children:"In some scenarios, users may need to split a model, which was trained as a whole, into multiple segments for deployment on the board. For example, in a two-stage detection model like the one shown in the figure below, if DPP needs to be executed on the CPU and its output (roi) is used as the input for RoiAlign, users need to split the model into Stage1 and Stage2 and compile them separately. During runtime, the fixed-point data output by the backbone is directly used as the input for RoiAlign."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/expert/segmented_deploy.svg",alt:"segmented_deploy"})}),"\n",(0,i.jsx)(n.h3,{id:"method",children:"Method"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/expert/segmented_deploy_method.svg",alt:"segmented_deploy_method"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Model Modification: As shown in the figure above, on the basis of a model that can be quantization-aware trained (QAT) normally, users need to insert a ",(0,i.jsx)(n.code,{children:"QuantStub"})," after the segmentation point before ",(0,i.jsx)(n.code,{children:"prepare_qat"}),". Note that if ",(0,i.jsx)(n.code,{children:"horizon_plugin_pytorch.quantization.QuantStub"})," is used, ",(0,i.jsx)(n.code,{children:"scale"})," must be set to None."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["QAT Training: Perform quantization-aware training on the modified model as a whole. The inserted ",(0,i.jsx)(n.code,{children:"QuantStub"})," will record the scale of the input data for Stage2 in the buffer."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Conversion to Fixed-Point: Convert the trained QAT model to fixed-point representation using the ",(0,i.jsx)(n.code,{children:"convert"})," interface."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Splitting and Compilation: Split the model according to the form after deployment on the board, and trace and compile each segmented model separately. Note that although the input for Stage2 is quantized data during training, the ",(0,i.jsx)(n.code,{children:"example_input"})," for tracing Stage2 still needs to be in floating-point format. The inserted ",(0,i.jsx)(n.code,{children:"QuantStub"})," in Stage2 will be responsible for configuring the scale of the data correctly and quantizing it."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"op_fusion",children:"Operator Fusion"}),"\n",(0,i.jsx)(n.p,{children:"The operator fusion supported by the training tool can be divided into two categories: 1. Absorbing BN; 2. Fusing Add and ReLU(6).### Absorb BN"}),"\n",(0,i.jsxs)(n.p,{children:["The purpose of absorbing ",(0,i.jsx)(n.code,{children:"BN"})," is to reduce the computational cost of the model. Since ",(0,i.jsx)(n.code,{children:"BN"})," is a linear transformation process, when ",(0,i.jsx)(n.code,{children:"BN"})," appears together with ",(0,i.jsx)(n.code,{children:"Conv"}),", the parameters of ",(0,i.jsx)(n.code,{children:"BN"})," can be absorbed into the parameters of ",(0,i.jsx)(n.code,{children:"Conv"}),", thereby eliminating the computation of ",(0,i.jsx)(n.code,{children:"BN"})," in the deployed model."]}),"\n",(0,i.jsx)(n.p,{children:"The calculation process of absorption is as follows:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/expert/fuse_bn.jpg",alt:"fuse_bn"})}),"\n",(0,i.jsxs)(n.p,{children:["By absorbing ",(0,i.jsx)(n.code,{children:"BN"}),", ",(0,i.jsx)(n.code,{children:"Conv2d + BN2d"})," can be simplified to ",(0,i.jsx)(n.code,{children:"Conv2d"}),"."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/expert/absorb_bn.svg",alt:"absorb_bn"})}),"\n",(0,i.jsx)(n.h3,{id:"fusion-of-add-and-relu6",children:"Fusion of Add and ReLU(6)"}),"\n",(0,i.jsx)(n.p,{children:"Unlike CUDA Kernel Fusion, which fuses CUDA Kernels to improve computational speed, the fusion supported by the training toolkit focuses more on the quantization level."}),"\n",(0,i.jsxs)(n.p,{children:["BPU hardware has been optimized for common model structures. When calculating the combination of ",(0,i.jsx)(n.code,{children:"Conv -> Add -> ReLU"}),", the hardware can preserve high-precision state for data passing between operators, thus improving the overall numerical precision of the model. Therefore, during quantization of the model, we can treat ",(0,i.jsx)(n.code,{children:"Conv -> Add -> ReLU"})," as a whole."]}),"\n",(0,i.jsxs)(n.p,{children:["Since the training toolkit quantizes the model based on ",(0,i.jsx)(n.code,{children:"torch.nn.Module"}),", in order to treat ",(0,i.jsx)(n.code,{children:"Conv -> Add -> ReLU"})," as a whole during quantization, they need to be merged into a single ",(0,i.jsx)(n.code,{children:"Module"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"Operator fusion not only preserves high-precision state for intermediate results, but also eliminates the process of converting intermediate results to low-precision representation. Therefore, the execution speed is faster compared to not fusing the operators."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"(Since operator fusion can improve both model precision and speed, it is generally recommended to fuse all possible parts.)"})}),"\n",(0,i.jsx)(n.h3,{id:"op_fusion",children:"Operator Fusion"}),"\n",(0,i.jsx)(n.p,{children:"The training tools support two main categories of operator fusion: 1. Absorbing Batch Normalization (BN); 2. Fusing Add and ReLU(6)."}),"\n",(0,i.jsx)(n.h3,{id:"absorbing-batch-normalization-bn",children:"Absorbing Batch Normalization (BN)"}),"\n",(0,i.jsx)(n.p,{children:"The purpose of absorbing BN is to reduce model computation. Since BN is a linear transformation, when it follows a Conv layer, the BN parameters can be absorbed into the Conv parameters, eliminating BN computations during deployment."}),"\n",(0,i.jsx)(n.p,{children:"The absorption process looks like this:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/expert/fuse_bn.jpg",alt:"fuse_bn"})}),"\n",(0,i.jsxs)(n.p,{children:["By absorbing BN, a ",(0,i.jsx)(n.code,{children:"Conv2d + BN2d"})," sequence can be simplified to just ",(0,i.jsx)(n.code,{children:"Conv2d"}),"."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/expert/absorb_bn.svg",alt:"absorb_bn"})}),"\n",(0,i.jsx)(n.h3,{id:"fusing-add-relu6",children:"Fusing Add, ReLU(6)"}),"\n",(0,i.jsxs)(n.p,{children:["Unlike CUDA Kernel Fusion that aims to improve computational speed, the fusion supported by training tools leans more towards quantization. The BPU hardware optimizes common model structures, allowing for high-precision data transfer between ",(0,i.jsx)(n.code,{children:"Conv"}),", ",(0,i.jsx)(n.code,{children:"Add"}),", and ",(0,i.jsx)(n.code,{children:"ReLU"})," operators, enhancing overall numerical precision. During quantization, these operators can be treated as a single unit."]}),"\n",(0,i.jsxs)(n.p,{children:["Since training tools quantize models at the ",(0,i.jsx)(n.code,{children:"torch.nn.Module"})," level, to treat ",(0,i.jsx)(n.code,{children:"Conv -> Add -> ReLU"})," as a whole during quantization, they need to be merged into a single ",(0,i.jsx)(n.code,{children:"Module"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"Operator fusion not only retains high precision intermediate results but also eliminates the need for converting them to low-precision representation, resulting in faster execution compared to non-fused scenarios."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"Note: Operator fusion is generally beneficial due to its improvements in both model accuracy and speed, so it should be applied to all eligible parts."})}),"\n",(0,i.jsx)(n.h3,{id:"implementation-principle",children:"Implementation Principle"}),"\n",(0,i.jsx)(n.p,{children:"Thanks to the graph analysis capability provided by FX, the training tools can automatically analyze the model's computation graph and apply fusion patterns to eligible sections. Submodules are replaced to implement the fusion operation. Here's an example:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"Absorbing BN and fusing Add, ReLU(6) can be done using the same mechanism, so there's no need to differentiate during fusion."})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import torch\nfrom torch import nn\nfrom torch.quantization import DeQuantStub\nfrom horizon_plugin_pytorch.quantization import QuantStub\nfrom horizon_plugin_pytorch.quantization import fuse_fx\n\n\nclass ModelForFusion(nn.Module):\n    def __init__(self):\n        super(ModelForFusion, self).__init__()\n        self.quantx = QuantStub()\n        self.quanty = QuantStub()\n        self.conv = nn.Conv2d(3, 3, 3)\n        self.bn = nn.BatchNorm2d(3)\n        self.relu = nn.ReLU()\n        self.dequant = DeQuantStub()\n\n    def forward(self, x, y):\n        x = self.quantx(x)\n        y = self.quanty(y)\n        x = self.conv(x)\n        x = self.bn(x)\n        x = x + y\n        x = self.relu(x)\n        x = self.dequant(x)\n\n        return x\n\n\nfloat_model = ModelForFusion()\nfused_model = fuse_fx(float_model)\n\nprint(fused_model)\n"""\nModelForFusion(\n  (quantx): QuantStub()\n  (quanty): QuantStub()\n  (conv): Identity()\n  (bn): Identity()\n  (relu): Identity()\n  (dequant): DeQuantStub()\n  (_generated_add_0): ConvAddReLU2d(\n    (conv): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n    (relu): ReLU()\n  )\n)\n...\n\ndef forward(self, x, y):\n    quantx = self.quantx(x);  x = None\n    quanty = self.quanty(y);  y = None\n    _generated_add_0 = self._generated_add_0\n    add_1 = _generated_add_0(quantx, quanty);  quantx = quanty = None\n    dequant = self.dequant(add_1);  add_1 = None\n    return dequant\n"""\n'})}),"\n",(0,i.jsxs)(n.p,{children:["After applying operator fusion, the BN is absorbed into the Conv, and Conv, Add, and ReLU are fused into a single ",(0,i.jsx)(n.code,{children:"Module"})," (",(0,i.jsx)(n.code,{children:"_generated_add_0"}),"). Original submodules are replaced with ",(0,i.jsx)(n.code,{children:"Identity"}),", and the calls to them are removed from the ",(0,i.jsx)(n.code,{children:"forward"})," function."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.em,{children:["FX automatically replaces the ",(0,i.jsx)(n.code,{children:"+"})," operator in the model with a ",(0,i.jsx)(n.code,{children:"Module"})," named ",(0,i.jsx)(n.code,{children:"_generated_add_0"})," to support fusion and quantization operations."]})}),"\n",(0,i.jsx)(n.h3,{id:"supported-operator-combinations",children:"Supported Operator Combinations"}),"\n",(0,i.jsx)(n.p,{children:"The current supported combinations for fused operators are defined in the following function:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import operator\nimport torch\nfrom torch import nn\nfrom horizon_plugin_pytorch import nn as horizon_nn\n\n\ndef register_fusion_patterns():\n    convs = (\n        nn.Conv2d,\n        nn.ConvTranspose2d,\n        nn.Conv3d,\n        nn.Linear,\n    )\n    bns = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.SyncBatchNorm)\n    adds = (\n        nn.quantized.FloatFunctional.add,\n        horizon_nn.quantized.FloatFunctional.add,\n        torch.add,\n        operator.add,  # The '+' operator used in code\n    )\n    relus = (nn.ReLU, nn.ReLU6, nn.functional.relu, nn.functional.relu6)\n\n    for conv in convs:\n        for bn in bns:\n            for add in adds:\n                for relu in relus:\n                    # Conv BN\n                    register_fusion_pattern((bn, conv))(ConvBNAddReLUFusion)\n\n                    # Conv ReLU\n                    register_fusion_pattern((relu, conv))(ConvBNAddReLUFusion)\n\n                    # Conv Add\n                    register_fusion_pattern((add, conv, MatchAllNode))(\n                        ConvBNAddReLUFusion\n                    )  # Conv output as first input to add\n                    register_fusion_pattern((add, MatchAllNode, conv))(\n                        ConvBNAddedReLUFusion\n                    )  # Conv output as second input to add\n\n                    # Conv BN ReLU\n                    register_fusion_pattern((relu, (bn, conv)))(\n                        ConvBNAddReLUFusion\n                    )\n\n                    # Conv BN Add\n                    register_fusion_pattern((add, (bn, conv), MatchAllNode))(\n                        ConvBNAddReLUFusion\n                    )\n                    register_fusion_pattern((add, MatchAllNode, (bn, conv)))(\n                        ConvBNAddedReLUFusion\n                    )\n\n                    # Conv Add ReLU\n                    register_fusion_pattern((relu, (add, conv, MatchAllNode)))(\n                        ConvBNAddReLUFusion\n                    )\n                    register_fusion_pattern((relu, (add, MatchAllNode, conv)))(\n                        ConvBNAddedReLUFusion\n                    )\n\n                    # Conv BN Add ReLU\n                    register_fusion_pattern(\n                        (relu, (add, (bn, conv), MatchAllNode))\n                    )(ConvBNAddReLUFusion)\n                    register_fusion_pattern(\n                        (relu, (add, MatchAllNode, (bn, conv)))\n                    )(ConvBNAddedReLUFusion)\n"})}),"\n",(0,i.jsx)(n.p,{children:"These patterns define which combinations of Conv, BN, Add, and ReLU operators can be fused."})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}}}]);