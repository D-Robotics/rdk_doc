"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[98164],{28453:(e,n,r)=>{r.d(n,{R:()=>c,x:()=>o});var i=r(96540);const s={},t=i.createContext(s);function c(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:c(e.components),i.createElement(t.Provider,{value:n},e.children)}},37602:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>c,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"Algorithm_Application/Python_Sample/WebSocket_yolov5x","title":"WebSocket YOLOv5x Inference","description":"This example demonstrates how to perform object detection using the Ultralytics YOLOv5x model on an embedded platform (e.g., RDK S100) equipped with an HBM accelerator and a VIO camera module, and stream JPEG images along with detection bounding boxes in real time via WebSocket. The example code is located in the /app/pydevdemo/11webdisplaycamera_sample/ directory.","source":"@site/i18n/en/docusaurus-plugin-content-docs-docs_s/current/04_Algorithm_Application/02_Python_Sample/15_WebSocket_yolov5x.md","sourceDirName":"04_Algorithm_Application/02_Python_Sample","slug":"/Algorithm_Application/Python_Sample/WebSocket_yolov5x","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/Python_Sample/WebSocket_yolov5x","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1769771781000,"sidebarPosition":15,"frontMatter":{"sidebar_position":15},"sidebar":"tutorialSidebar","previous":{"title":"MIPI Camera YOLOv5x Inference","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/Python_Sample/mipi_camera_yolov5x"},"next":{"title":"4.3\u53c2\u8003\u793a\u4f8b\uff08C++\uff09","permalink":"/rdk_doc/en/rdk_s/03_C++_Sample"}}');var s=r(74848),t=r(28453);const c={sidebar_position:15},o="WebSocket YOLOv5x Inference",l={},d=[{value:"Feature Description",id:"feature-description",level:2},{value:"Model Description",id:"model-description",level:2},{value:"Environment Dependencies",id:"environment-dependencies",level:2},{value:"Hardware Environment",id:"hardware-environment",level:2},{value:"Directory Structure",id:"directory-structure",level:2},{value:"Parameter Description",id:"parameter-description",level:2},{value:"Quick Start",id:"quick-start",level:2},{value:"Notes",id:"notes",level:2},{value:"License",id:"license",level:2}];function a(e){const n={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"websocket-yolov5x-inference",children:"WebSocket YOLOv5x Inference"})}),"\n",(0,s.jsxs)(n.p,{children:["This example demonstrates how to perform object detection using the Ultralytics YOLOv5x model on an embedded platform (e.g., RDK S100) equipped with an HBM accelerator and a VIO camera module, and stream JPEG images along with detection bounding boxes in real time via WebSocket. The example code is located in the ",(0,s.jsx)(n.code,{children:"/app/pydev_demo/11_web_display_camera_sample/"})," directory."]}),"\n",(0,s.jsx)(n.h2,{id:"feature-description",children:"Feature Description"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Model Loading"})}),"\n",(0,s.jsxs)(n.p,{children:["Initialize ",(0,s.jsx)(n.code,{children:"hbm_runtime"}),", load the model, and obtain input/output names and dimensions."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Preprocessing"})}),"\n",(0,s.jsx)(n.p,{children:"Split the raw NV12 image into Y/UV channels, resize it to the required model input dimensions, and generate input tensors in the correct format."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Model Inference"})}),"\n",(0,s.jsxs)(n.p,{children:["Invoke ",(0,s.jsx)(n.code,{children:".run()"})," to execute BPU inference."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Postprocessing"})}),"\n",(0,s.jsx)(n.p,{children:"Decode inference results, filter out low-confidence detections, apply Non-Maximum Suppression (NMS), and scale the results back to the original image dimensions."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsxs)(n.strong,{children:["Camera Management (",(0,s.jsx)(n.code,{children:"CameraManager"}),")"]})}),"\n",(0,s.jsx)(n.p,{children:"Open the camera, acquire raw or model-sized images, and encode them into JPEG format."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"WebSocket Server"})}),"\n",(0,s.jsx)(n.p,{children:"Accept connections from web clients, continuously fetch camera images, perform detection, and return results to the web client using Protocol Buffers."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"model-description",children:"Model Description"}),"\n",(0,s.jsxs)(n.p,{children:["Refer to ",(0,s.jsx)(n.a,{href:"/rdk_doc/en/rdk_s/Algorithm_Application/Python_Sample/Ultralytics_YOLOv5x#object-detection-ultralytics-yolov5x",children:"Ultralytics YOLOv5x Object Detection Example Summary"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"environment-dependencies",children:"Environment Dependencies"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Ensure the dependencies in ",(0,s.jsx)(n.code,{children:"pydev"})," are installed:","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install -r ../requirements.txt\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Install WebSocket-related packages:","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install websockets==15.0.1 protobuf==3.20.3\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"hardware-environment",children:"Hardware Environment"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The MIPI camera interface uses auto-detection mode. Only one MIPI camera (connected to any MIPI port) is supported during sample execution; connecting multiple cameras simultaneously will cause errors."}),"\n",(0,s.jsx)(n.li,{children:"This sample currently supports only the following MIPI sensors: IMX219, SC230AI."}),"\n",(0,s.jsxs)(n.li,{children:["For MIPI camera installation instructions, refer to the section ",(0,s.jsx)(n.a,{href:"/rdk_doc/en/rdk_s/Quick_start/hardware_introduction/rdk_s100_camera_expansion_board",children:"Camera Expansion Board \u2013 MIPI Camera Interface"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"directory-structure",children:"Directory Structure"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:".\n\u251c\u2500\u2500 mipi_camera_web_yolov5x.py      # Main program\n\u2514\u2500\u2500 README.md                       # Usage instructions\n"})}),"\n",(0,s.jsx)(n.h2,{id:"parameter-description",children:"Parameter Description"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Description"}),(0,s.jsx)(n.th,{children:"Default Value"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--model-path"})}),(0,s.jsxs)(n.td,{children:["Path to the BPU quantized model (",(0,s.jsx)(n.code,{children:".hbm"}),")"]}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"/opt/hobot/model/s100/basic/yolov5x_672x672_nv12.hbm"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--priority"})}),(0,s.jsx)(n.td,{children:"Inference priority (0\u2013255, where 255 is highest)"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"0"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--bpu-cores"})}),(0,s.jsxs)(n.td,{children:["List of BPU core indices (e.g., ",(0,s.jsx)(n.code,{children:"0 1"}),")"]}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"[0]"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--label-file"})}),(0,s.jsx)(n.td,{children:"Path to the class label file"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"/app/res/labels/coco_classes.names"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--nms-thres"})}),(0,s.jsx)(n.td,{children:"IoU threshold for Non-Maximum Suppression (NMS)"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"0.45"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--score-thres"})}),(0,s.jsx)(n.td,{children:"Detection confidence threshold"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"0.25"})})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Start the Web Service"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# 1. Navigate to the webservice directory\ncd webservice/\n\n# 2. Start the service\nsudo ./sbin/nginx -p .\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Run the Model"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Return to the current directory:","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd ..\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Run with default parameters:","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python mipi_camera_web_yolov5x.py\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Run with custom parameters:","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python mipi_camera_web_yolov5x.py \\\n--model-path /opt/hobot/model/s100/basic/yolov5x_672x672_nv12.hbm \\\n--priority 0 \\\n--bpu-cores 0 \\\n--label-file /app/res/labels/coco_classes.names \\\n--nms-thres 0.45 \\\n--score-thres 0.25\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"View Results"})}),"\n",(0,s.jsxs)(n.p,{children:["After successful execution, open the web display page by visiting: ",(0,s.jsx)(n.a,{href:"http://IP",children:"http://IP"}),(0,s.jsx)(n.br,{}),"\n",(0,s.jsx)(n.strong,{children:"Note: Do not include a port number."})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Terminate Execution"})}),"\n",(0,s.jsxs)(n.p,{children:["Press ",(0,s.jsx)(n.code,{children:"Ctrl + C"})," in the terminal."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"notes",children:"Notes"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["If the specified model path does not exist, try checking the directory ",(0,s.jsx)(n.code,{children:"/opt/hobot/model/s100/basic/"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"license",children:"License"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-license",children:"Copyright (C) 2025, XiangshunZhao D-Robotics.\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Affero General Public License as\npublished by the Free Software Foundation, either version 3 of the\nLicense, or (at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Affero General Public License for more details.\n\nYou should have received a copy of the GNU Affero General Public License\nalong with this program.  If not, see <https://www.gnu.org/licenses/>.\n"})})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(a,{...e})}):a(e)}}}]);