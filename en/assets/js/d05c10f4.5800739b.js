"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[80114],{11470:(e,t,s)=>{s.d(t,{A:()=>I});var n=s(96540),a=s(34164),r=s(23104),i=s(56347),o=s(205),l=s(57485),c=s(31682),d=s(70679);function u(e){return n.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,n.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function m(e){const{values:t,children:s}=e;return(0,n.useMemo)(()=>{const e=t??function(e){return u(e).map(({props:{value:e,label:t,attributes:s,default:n}})=>({value:e,label:t,attributes:s,default:n}))}(s);return function(e){const t=(0,c.XI)(e,(e,t)=>e.value===t.value);if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[t,s])}function h({value:e,tabValues:t}){return t.some(t=>t.value===e)}function p({queryString:e=!1,groupId:t}){const s=(0,i.W6)(),a=function({queryString:e=!1,groupId:t}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:e,groupId:t});return[(0,l.aZ)(a),(0,n.useCallback)(e=>{if(!a)return;const t=new URLSearchParams(s.location.search);t.set(a,e),s.replace({...s.location,search:t.toString()})},[a,s])]}function g(e){const{defaultValue:t,queryString:s=!1,groupId:a}=e,r=m(e),[i,l]=(0,n.useState)(()=>function({defaultValue:e,tabValues:t}){if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!h({value:e,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${t.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const s=t.find(e=>e.default)??t[0];if(!s)throw new Error("Unexpected error: 0 tabValues");return s.value}({defaultValue:t,tabValues:r})),[c,u]=p({queryString:s,groupId:a}),[g,f]=function({groupId:e}){const t=function(e){return e?`docusaurus.tab.${e}`:null}(e),[s,a]=(0,d.Dv)(t);return[s,(0,n.useCallback)(e=>{t&&a.set(e)},[t,a])]}({groupId:a}),b=(()=>{const e=c??g;return h({value:e,tabValues:r})?e:null})();(0,o.A)(()=>{b&&l(b)},[b]);return{selectedValue:i,selectValue:(0,n.useCallback)(e=>{if(!h({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);l(e),u(e),f(e)},[u,f,r]),tabValues:r}}var f=s(92303);const b={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var x=s(74848);function _({className:e,block:t,selectedValue:s,selectValue:n,tabValues:i}){const o=[],{blockElementScrollPositionUntilNextRender:l}=(0,r.a_)(),c=e=>{const t=e.currentTarget,a=o.indexOf(t),r=i[a].value;r!==s&&(l(t),n(r))},d=e=>{let t=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const s=o.indexOf(e.currentTarget)+1;t=o[s]??o[0];break}case"ArrowLeft":{const s=o.indexOf(e.currentTarget)-1;t=o[s]??o[o.length-1];break}}t?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":t},e),children:i.map(({value:e,label:t,attributes:n})=>(0,x.jsx)("li",{role:"tab",tabIndex:s===e?0:-1,"aria-selected":s===e,ref:e=>{o.push(e)},onKeyDown:d,onClick:c,...n,className:(0,a.A)("tabs__item",b.tabItem,n?.className,{"tabs__item--active":s===e}),children:t??e},e))})}function v({lazy:e,children:t,selectedValue:s}){const r=(Array.isArray(t)?t:[t]).filter(Boolean);if(e){const e=r.find(e=>e.props.value===s);return e?(0,n.cloneElement)(e,{className:(0,a.A)("margin-top--md",e.props.className)}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:r.map((e,t)=>(0,n.cloneElement)(e,{key:t,hidden:e.props.value!==s}))})}function j(e){const t=g(e);return(0,x.jsxs)("div",{className:(0,a.A)("tabs-container",b.tabList),children:[(0,x.jsx)(_,{...t,...e}),(0,x.jsx)(v,{...t,...e})]})}function I(e){const t=(0,f.A)();return(0,x.jsx)(j,{...e,children:u(e.children)},String(t))}},19365:(e,t,s)=>{s.d(t,{A:()=>i});s(96540);var n=s(34164);const a={tabItem:"tabItem_Ymn6"};var r=s(74848);function i({children:e,hidden:t,className:s}){return(0,r.jsx)("div",{role:"tabpanel",className:(0,n.A)(a.tabItem,s),hidden:t,children:e})}},21527:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>d,contentTitle:()=>c,default:()=>h,frontMatter:()=>l,metadata:()=>n,toc:()=>u});const n=JSON.parse('{"id":"Robot_development/boxs/segmentation/yolov8_seg","title":"Ultralytics YOLOv8-Seg","description":"Introduction","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/05_Robot_development/03_boxs/segmentation/yolov8_seg.md","sourceDirName":"05_Robot_development/03_boxs/segmentation","slug":"/Robot_development/boxs/segmentation/yolov8_seg","permalink":"/rdk_doc/en/Robot_development/boxs/segmentation/yolov8_seg","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1754568158000,"sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"mobilenet_unet","permalink":"/rdk_doc/en/Robot_development/boxs/segmentation/mobilenet_unet"},"next":{"title":"Human Detection and Tracking","permalink":"/rdk_doc/en/Robot_development/boxs/function/mono2d_body_detection"}}');var a=s(74848),r=s(28453),i=s(11470),o=s(19365);const l={sidebar_position:2},c="Ultralytics YOLOv8-Seg",d={},u=[{value:"Introduction",id:"introduction",level:2},{value:"Supported Platforms",id:"supported-platforms",level:2},{value:"Preparation",id:"preparation",level:2},{value:"RDK",id:"rdk",level:3},{value:"Use the Camera to Publish Images",id:"use-the-camera-to-publish-images",level:4},{value:"Use a MIPI Camera to Publish Images",id:"use-a-mipi-camera-to-publish-images",level:5},{value:"Use a USB Camera to Publish Images",id:"use-a-usb-camera-to-publish-images",level:5},{value:"Use local images offline",id:"use-local-images-offline",level:4},{value:"Analysis of Results",id:"analysis-of-results",level:2},{value:"Use a Camera to Publishing Images",id:"use-a-camera-to-publishing-images",level:3},{value:"Use Local Images Offline",id:"use-local-images-offline-1",level:3}];function m(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"ultralytics-yolov8-seg",children:"Ultralytics YOLOv8-Seg"})}),"\n","\n",(0,a.jsx)(t.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(t.p,{children:"The Ultralytics YOLOv8-Seg algorithm example uses images as input and performs algorithm inference using BPU. It publishes segmentation result messages."}),"\n",(0,a.jsxs)(t.p,{children:["The YOLOv8-Seg is trained on the ",(0,a.jsx)(t.a,{href:"http://cocodataset.org/",children:"COCO128-seg"})," dataset and the Onnx model. It supports instance segmentation for 80 categories including humans, animals, fruits, and vehicles."]}),"\n",(0,a.jsxs)(t.p,{children:["Code repository: (",(0,a.jsx)(t.a,{href:"https://github.com/D-Robotics/hobot_dnn",children:"https://github.com/D-Robotics/hobot_dnn"}),")"]}),"\n",(0,a.jsx)(t.p,{children:"Applications: YOLOv8-Seg is capable of recognizing objects and performing precise segmentation. It can be applied in the fields of autonomous driving, geological detection, and medical image analysis."}),"\n",(0,a.jsx)(t.h2,{id:"supported-platforms",children:"Supported Platforms"}),"\n",(0,a.jsxs)(t.table,{children:[(0,a.jsx)(t.thead,{children:(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.th,{children:"Platform"}),(0,a.jsx)(t.th,{children:"System"}),(0,a.jsx)(t.th,{children:"Function"})]})}),(0,a.jsx)(t.tbody,{children:(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"RDK X5"}),(0,a.jsx)(t.td,{children:"Ubuntu 22.04 (Humble)"}),(0,a.jsx)(t.td,{children:"- Start MIPI/USB cameras or local image and save the rendered results offline."})]})})]}),"\n",(0,a.jsx)(t.h2,{id:"preparation",children:"Preparation"}),"\n",(0,a.jsx)(t.h3,{id:"rdk",children:"RDK"}),"\n",(0,a.jsxs)(t.ol,{children:["\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"The RDK platform has been flashed with the provided 22.04 system image."}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"TogetheROS.Bot has been successfully installed on the RDK platform."}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"A MIPI or USB camera has been installed on the RDK platform. If there is no camera available, the algorithm's effects can be experienced by using local JPEG/PNG images offline."}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(t.h4,{id:"use-the-camera-to-publish-images",children:"Use the Camera to Publish Images"}),"\n",(0,a.jsx)(t.h5,{id:"use-a-mipi-camera-to-publish-images",children:"Use a MIPI Camera to Publish Images"}),"\n",(0,a.jsxs)(t.p,{children:['The YOLOv8-Seg example subscribes to images published by the sensor package. IF set "dnn_example_dump_render_img:=1", it will save the rendered images automatically in the running directory. The saved images are named in the format of ',(0,a.jsx)(t.code,{children:"render_frameid_timestampInSeconds_timestampInNanoseconds.jpg"}),"."]}),"\n",(0,a.jsx)(i.A,{groupId:"tros-distro",children:(0,a.jsx)(o.A,{value:"humble",label:"Humble",children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"# Configure the tros.b environment\nsource /opt/tros/humble/setup.bash\n"})})})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-shell",children:"# Configuring MIPI camera\nexport CAM_TYPE=mipi\n\n# Start the launch file\nros2 launch dnn_node_example dnn_node_example.launch.py dnn_example_dump_render_img:=0 dnn_example_config_file:=config/yolov8segworkconfig.json dnn_example_image_width:=1920 dnn_example_image_height:=1080\n"})}),"\n",(0,a.jsx)(t.h5,{id:"use-a-usb-camera-to-publish-images",children:"Use a USB Camera to Publish Images"}),"\n",(0,a.jsx)(i.A,{groupId:"tros-distro",children:(0,a.jsx)(o.A,{value:"humble",label:"Humble",children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"# Configure the tros.b environment\nsource /opt/tros/humble/setup.bash\n"})})})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-shell",children:"# Configuring USB camera\nexport CAM_TYPE=usb\n\n# Start the launch file\nros2 launch dnn_node_example dnn_node_example.launch.py dnn_example_dump_render_img:=0 dnn_example_config_file:=config/yolov8segworkconfig.json dnn_example_image_width:=1920 dnn_example_image_height:=1080\n"})}),"\n",(0,a.jsx)(t.h4,{id:"use-local-images-offline",children:"Use local images offline"}),"\n",(0,a.jsx)(t.p,{children:"The YOLOv8-Seg example uses local JPEG/PNG format images for feedback. After inference, the rendered images of the algorithm results are stored in the local running path."}),"\n",(0,a.jsx)(i.A,{groupId:"tros-distro",children:(0,a.jsx)(o.A,{value:"humble",label:"Humble",children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"# Configure the tros.b environment\nsource /opt/tros/humble/setup.bash\n"})})})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-shell",children:"# Start the launch file\nros2 launch dnn_node_example dnn_node_example_feedback.launch.py dnn_example_config_file:=config/yolov8segworkconfig.json dnn_example_image:=config/test.jpeg\n"})}),"\n",(0,a.jsx)(t.h2,{id:"analysis-of-results",children:"Analysis of Results"}),"\n",(0,a.jsx)(t.h3,{id:"use-a-camera-to-publishing-images",children:"Use a Camera to Publishing Images"}),"\n",(0,a.jsx)(t.p,{children:"The output shows the following information:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-shell",children:"[example-3] [WARN] [0000001244.489045384] [example]: Sub img fps: -1.00, Smart fps: 6.00, infer time ms: 12, post process time ms: 31\n[example-3] [WARN] [0000001245.524813052] [example]: Sub img fps: 5.84, Smart fps: 4.99, infer time ms: 8, post process time ms: 64\n[example-3] [WARN] [0000001246.526635344] [example]: Sub img fps: 4.96, Smart fps: 5.00, infer time ms: 8, post process time ms: 66\n[example-3] [WARN] [0000001247.528846136] [example]: Sub img fps: 5.00, Smart fps: 5.00, infer time ms: 8, post process time ms: 68\n[example-3] [WARN] [0000001248.528474095] [example]: Sub img fps: 5.00, Smart fps: 5.00, infer time ms: 8, post process time ms: 68\n[example-3] [WARN] [0000001249.528576345] [example]: Sub img fps: 5.00, Smart fps: 5.00, infer time ms: 8, post process time ms: 68\n[example-3] [WARN] [0000001250.493265846] [example]: Sub img fps: 5.02, Smart fps: 5.00, infer time ms: 8, post process time ms: 32\n[example-3] [WARN] [0000001251.528909346] [example]: Sub img fps: 4.98, Smart fps: 5.00, infer time ms: 8, post process time ms: 67\n"})}),"\n",(0,a.jsxs)(t.p,{children:["The log output shows that the topic used for publishing the algorithm inference results is ",(0,a.jsx)(t.code,{children:"hobot_dnn_detection"}),", and the topic used for subscribing to the images is ",(0,a.jsx)(t.code,{children:"/hbmem_img"}),". The frame rate at which the images are published will adapt according to the algorithm inference output frame rate. Additionally, rendering the semantic segmentation results on the RDK and saving the images in the running path will cause a decrease in frame rate."]}),"\n",(0,a.jsxs)(t.p,{children:["Original image:\n",(0,a.jsx)(t.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/03_boxs/segmentation/image/yolov8_seg/test.jpg",alt:"raw"})]}),"\n",(0,a.jsxs)(t.p,{children:["Rendered image:\n",(0,a.jsx)(t.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/03_boxs/segmentation/image/yolov8_seg/web.jpeg",alt:"render_web"})]}),"\n",(0,a.jsx)(t.h3,{id:"use-local-images-offline-1",children:"Use Local Images Offline"}),"\n",(0,a.jsx)(t.p,{children:"The output shows the following information:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-shell",children:"[INFO] [0000001744.811779665] [example]: Dnn node feed with local image: /userdata/config/test.jpg\n[INFO] [0000001746.237111249] [example]: Output from frame_id: feedback, stamp: 0.0\n[INFO] [0000001746.266157040] [PostProcessBase]: out box size: 6\n[INFO] [0000001746.266340040] [PostProcessBase]: det rect: 90.4946 58.2675 192.103 351.403, det type: person, score:0.927177\n[INFO] [0000001746.267129832] [PostProcessBase]: det rect: 455.518 77.1254 536.289 354.541, det type: person, score:0.909735\n[INFO] [0000001746.267248457] [PostProcessBase]: det rect: 381.604 103.953 464.446 327.9, det type: person, score:0.898899\n[INFO] [0000001746.267331624] [PostProcessBase]: det rect: 204.864 71.6262 303.593 351.835, det type: person, score:0.887814\n[INFO] [0000001746.267404540] [PostProcessBase]: det rect: 317.885 108.287 389.773 338.197, det type: person, score:0.866887\n[INFO] [0000001746.267486457] [PostProcessBase]: det rect: 181.487 111.093 202.097 132.665, det type: car, score:0.443035\n[INFO] [0000001746.267548999] [ClassificationPostProcess]: out cls size: 0\n[INFO] [0000001746.267662832] [SegmentationPostProcess]: features size: 14240, width: 160, height: 89, num_classes: 80, step: 1\n[INFO] [0000001746.270546040] [ImageUtils]: target size: 7\n[INFO] [0000001746.270674082] [ImageUtils]: target type: person, rois.size: 1\n[INFO] [0000001746.270745915] [ImageUtils]: roi.type: person, x_offset: 90 y_offset: 58 width: 101 height: 293\n[INFO] [0000001746.271122207] [ImageUtils]: target type: person, rois.size: 1\n[INFO] [0000001746.271162499] [ImageUtils]: roi.type: person, x_offset: 455 y_offset: 77 width: 80 height: 277\n[INFO] [0000001746.271325499] [ImageUtils]: target type: person, rois.size: 1\n[INFO] [0000001746.271362082] [ImageUtils]: roi.type: person, x_offset: 381 y_offset: 103 width: 82 height: 223\n[INFO] [0000001746.271491040] [ImageUtils]: target type: person, rois.size: 1\n[INFO] [0000001746.271525249] [ImageUtils]: roi.type: person, x_offset: 204 y_offset: 71 width: 98 height: 280\n[INFO] [0000001746.271782749] [ImageUtils]: target type: person, rois.size: 1\n[INFO] [0000001746.271819457] [ImageUtils]: roi.type: person, x_offset: 317 y_offset: 108 width: 71 height: 229\n[INFO] [0000001746.271947790] [ImageUtils]: target type: car, rois.size: 1\n[INFO] [0000001746.271982374] [ImageUtils]: roi.type: car, x_offset: 181 y_offset: 111 width: 20 height: 21\n[INFO] [0000001746.272044124] [ImageUtils]: target type: parking_space, rois.size: 0\n[WARN] [0000001746.276824624] [ImageUtils]: Draw result to file: render_feedback_0_0.jpeg\n"})}),"\n",(0,a.jsxs)(t.p,{children:["The log shows that the algorithm performs inference using the input image ",(0,a.jsx)(t.code,{children:"config/raw_unet.jpeg"}),", and the rendered image is stored with the file ",(0,a.jsx)(t.code,{children:"render_unet_feedback_0_0.jpeg"}),". The rendered image looks like this:"]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/03_boxs/segmentation/image/yolov8_seg/local.jpeg",alt:"render_feedback"})})]})}function h(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(m,{...e})}):m(e)}},28453:(e,t,s)=>{s.d(t,{R:()=>i,x:()=>o});var n=s(96540);const a={},r=n.createContext(a);function i(e){const t=n.useContext(r);return n.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),n.createElement(r.Provider,{value:t},e.children)}}}]);