"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[20664],{5908:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>d,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"Algorithm_Application/Python_Sample/ASR","title":"Automatic Speech Recognition - ASR","description":"This example runs a speech recognition model based on the hbmruntime inference engine to automatically transcribe .wav audio files into corresponding text. The example code is located in the /app/pydevdemo/07speechsample/01_asr/ directory.","source":"@site/i18n/en/docusaurus-plugin-content-docs-docs_s/current/04_Algorithm_Application/02_Python_Sample/11_ASR.md","sourceDirName":"04_Algorithm_Application/02_Python_Sample","slug":"/Algorithm_Application/Python_Sample/ASR","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/Python_Sample/ASR","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767953796000,"sidebarPosition":11,"frontMatter":{"sidebar_position":11},"sidebar":"tutorialSidebar","previous":{"title":"Lane Detection - LaneNet","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/Python_Sample/LaneNet"},"next":{"title":"Text Detection and Recognition - PaddleOCR","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/Python_Sample/PaddleOCR"}}');var r=i(74848),t=i(28453);const d={sidebar_position:11},l="Automatic Speech Recognition - ASR",c={},o=[{value:"Model Description",id:"model-description",level:2},{value:"Functionality Description",id:"functionality-description",level:2},{value:"Environment Dependencies",id:"environment-dependencies",level:2},{value:"Directory Structure",id:"directory-structure",level:2},{value:"Parameter Description",id:"parameter-description",level:2},{value:"Quick Start",id:"quick-start",level:2},{value:"Notes",id:"notes",level:2}];function a(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"automatic-speech-recognition---asr",children:"Automatic Speech Recognition - ASR"})}),"\n",(0,r.jsxs)(n.p,{children:["This example runs a speech recognition model based on the ",(0,r.jsx)(n.code,{children:"hbm_runtime"})," inference engine to automatically transcribe ",(0,r.jsx)(n.code,{children:".wav"})," audio files into corresponding text. The example code is located in the ",(0,r.jsx)(n.code,{children:"/app/pydev_demo/07_speech_sample/01_asr/"})," directory."]}),"\n",(0,r.jsx)(n.h2,{id:"model-description",children:"Model Description"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Introduction"}),":"]}),"\n",(0,r.jsxs)(n.p,{children:["The ASR (Automatic Speech Recognition) model converts audio signals into text. The input is a single-channel audio waveform (after sample rate conversion and normalization), and the output is a character-level token sequence. When used together with a vocabulary (",(0,r.jsx)(n.code,{children:"vocab"}),") file, it supports Chinese speech transcription. This example uses a quantized ",(0,r.jsx)(n.code,{children:".hbm"})," model."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"HBM Model Name"}),": ",(0,r.jsx)(n.code,{children:"asr.hbm"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Input Format"}),": Audio waveform, single-channel, sampled at 16kHz, with a maximum length of 30,000 samples."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Output"}),": Probability distribution (logits) over character tokens. After decoding via argmax, the logits are mapped to the recognized text."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Model Download URL"})," (automatically downloaded by the program):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"https://archive.d-robotics.cc/downloads/rdk_model_zoo/rdk_s100/asr/asr.hbm\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"functionality-description",children:"Functionality Description"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Model Loading"})}),"\n",(0,r.jsxs)(n.p,{children:["Load the ASR model using ",(0,r.jsx)(n.code,{children:"hbm_runtime"}),", which automatically parses the model's input/output shapes and quantization information."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Input Preprocessing"})}),"\n",(0,r.jsxs)(n.p,{children:["Use SoundFile to read audio files (supports ",(0,r.jsx)(n.code,{children:".wav"}),"), and process the audio as follows:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Convert to single-channel"}),"\n",(0,r.jsx)(n.li,{children:"Resample to the target sample rate (default: 16kHz)"}),"\n",(0,r.jsx)(n.li,{children:"Normalize to zero-mean and unit-variance (z-score)"}),"\n",(0,r.jsx)(n.li,{children:"Pad or truncate to a fixed length (e.g., 30,000 samples)"}),"\n",(0,r.jsx)(n.li,{children:"Support generator-based processing for long audio inputs, enabling streaming recognition."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Inference Execution"})}),"\n",(0,r.jsxs)(n.p,{children:["Perform inference using the ",(0,r.jsx)(n.code,{children:".run()"})," method, producing a logits tensor as output."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Post-processing"})}),"\n",(0,r.jsxs)(n.p,{children:["Use ",(0,r.jsx)(n.code,{children:"np.argmax()"})," to obtain token indices from the output logits, then map them to characters using the vocab dictionary file (in JSON format) to produce the final transcribed text."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"environment-dependencies",children:"Environment Dependencies"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Ensure that the dependencies in ",(0,r.jsx)(n.code,{children:"pydev"})," are installed:","\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install -r ../../requirements.txt\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Install the ",(0,r.jsx)(n.code,{children:"soundfile"})," package:","\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install soundfile==0.13.1\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"directory-structure",children:"Directory Structure"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-text",children:"01_asr/\n\u251c\u2500\u2500 asr.py                      # Main inference script\n"})}),"\n",(0,r.jsx)(n.h2,{id:"parameter-description",children:"Parameter Description"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Parameter"}),(0,r.jsx)(n.th,{children:"Description"}),(0,r.jsx)(n.th,{children:"Default Value"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"--model-path"})}),(0,r.jsxs)(n.td,{children:["Path to the model file (",(0,r.jsx)(n.code,{children:".hbm"})," format)"]}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"/opt/hobot/model/s100/basic/asr.hbm"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"--audio-file"})}),(0,r.jsxs)(n.td,{children:["Input audio file (supports ",(0,r.jsx)(n.code,{children:".wav"})," or ",(0,r.jsx)(n.code,{children:".flac"}),")"]}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"/app/res/assets/chi_sound.wav"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"--vocab-file"})}),(0,r.jsx)(n.td,{children:"Vocabulary file mapping tokens to IDs"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"/app/res/labels/vocab.json"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"--priority"})}),(0,r.jsx)(n.td,{children:"Inference priority (0\u2013255; higher values indicate higher priority)"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"0"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"--bpu-cores"})}),(0,r.jsxs)(n.td,{children:["Specify which BPU cores to use (e.g., ",(0,r.jsx)(n.code,{children:"--bpu-cores 0 1"}),")"]}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"[0]"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"--audio_maxlen"})}),(0,r.jsx)(n.td,{children:"Fixed length after audio padding/truncation (in samples)"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"30000"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"--new_rate"})}),(0,r.jsx)(n.td,{children:"Target sample rate; audio will be automatically resampled"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"16000"})})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Run the model"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Using default parameters:","\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"python asr.py\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Running with specified parameters:","\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"python asr.py \\\n--model-path /opt/hobot/model/s100/basic/asr.hbm \\\n--audio-file /app/res/assets/chi_sound.wav \\\n--vocab-file /app/res/labels/vocab.json \\\n--priority 0 \\\n--bpu-cores 0 \\\n--audio_maxlen 30000 \\\n--new_rate 16000\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"View Results"})}),"\n",(0,r.jsx)(n.p,{children:"Upon successful execution, the result will be printed:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"I am Qwen, a large-scale language model developed by Alibaba Cloud.\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"notes",children:"Notes"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"If the specified model path does not exist, the program will attempt to download the model automatically."}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(a,{...e})}):a(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>d,x:()=>l});var s=i(96540);const r={},t=s.createContext(r);function d(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:d(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);