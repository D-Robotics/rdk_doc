"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[28219],{28453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>s});var o=n(96540);const i={},a=o.createContext(i);function r(e){const t=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function s(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),o.createElement(a.Provider,{value:t},e.children)}},47139:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>s,default:()=>l,frontMatter:()=>r,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"Advanced_development/toolchain_development/expert/expert","title":"7.4.3.4 Advanced Guide","description":"Quantization refers to the technique of performing calculations and storing tensors with a bit width lower than floating-point precision. Quantized models use integers instead of floating-point values to perform partial or complete operations on tensors. Compared to typical FP32 models, horizonpluginpytorch supports INT8 quantization, which reduces the model size by 4 times and reduces the memory bandwidth requirement by 4 times. Hardware support for INT8 calculations is usually 2 to 4 times faster than FP32 calculations. Quantization is mainly a technique for accelerating inference, and quantization operations only support forward calculations.","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/07_Advanced_development/04_toolchain_development/expert/expert.md","sourceDirName":"07_Advanced_development/04_toolchain_development/expert","slug":"/Advanced_development/toolchain_development/expert/","permalink":"/rdk_doc/en/Advanced_development/toolchain_development/expert/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1750480041000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"On-board Model Application Development Guide","permalink":"/rdk_doc/en/Advanced_development/toolchain_development/intermediate/runtime_sample"},"next":{"title":"Environment Dependency","permalink":"/rdk_doc/en/Advanced_development/toolchain_development/expert/environment_config"}}');var i=n(74848),a=n(28453);const r={},s="7.4.3.4 Advanced Guide",d={},c=[];function p(e){const t={h1:"h1",header:"header",p:"p",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"7434-advanced-guide",children:"7.4.3.4 Advanced Guide"})}),"\n",(0,i.jsx)(t.p,{children:"Quantization refers to the technique of performing calculations and storing tensors with a bit width lower than floating-point precision. Quantized models use integers instead of floating-point values to perform partial or complete operations on tensors. Compared to typical FP32 models, horizon_plugin_pytorch supports INT8 quantization, which reduces the model size by 4 times and reduces the memory bandwidth requirement by 4 times. Hardware support for INT8 calculations is usually 2 to 4 times faster than FP32 calculations. Quantization is mainly a technique for accelerating inference, and quantization operations only support forward calculations."}),"\n",(0,i.jsx)(t.p,{children:"horizon_plugin_pytorch provides quantization operations adapted to BPU, supporting quantization-aware training. This training uses pseudo-quantization modules to model quantization errors in forward calculations and backpropagation. Please note that the entire computation process of quantization-aware training is performed using floating-point operations. At the end of quantization-aware training, horizon_plugin_pytorch provides conversion functions to transform the trained model into a fixed-point model, using a more compact model representation and high-performance vectorized operations on BPU."}),"\n",(0,i.jsx)(t.p,{children:"This chapter provides a detailed introduction to the quantization training tool of horizon_plugin_pytorch developed based on PyTorch."})]})}function l(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}}}]);