"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[95693],{28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var t=i(96540);const s={},l=t.createContext(s);function r(e){const n=t.useContext(l);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(l.Provider,{value:n},e.children)}},50040:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"Algorithm_Application/C++_Sample/MobileNetV2","title":"Image Classification - MobileNetV2","description":"This example demonstrates how to perform image classification using a BPU-deployed MobileNetV2 model with C/C++ inference. The example code is located in the directory /app/cdevdemo/bpu/01classificationsample/02mobilenetv2/.","source":"@site/i18n/en/docusaurus-plugin-content-docs-docs_s/current/04_Algorithm_Application/03_C++_Sample/03_MobileNetV2.md","sourceDirName":"04_Algorithm_Application/03_C++_Sample","slug":"/Algorithm_Application/C++_Sample/MobileNetV2","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/C++_Sample/MobileNetV2","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1764746491000,"sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Image Classification - ResNet18","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/C++_Sample/ResNet18"},"next":{"title":"Object Detection - Ultralytics YOLOv5x","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/C++_Sample/Ultralytics_YOLOv5x"}}');var s=i(74848),l=i(28453);const r={sidebar_position:3},o="Image Classification - MobileNetV2",d={},c=[{value:"Model Description",id:"model-description",level:2},{value:"Functionality Description",id:"functionality-description",level:2},{value:"Environment Dependencies",id:"environment-dependencies",level:2},{value:"Directory Structure",id:"directory-structure",level:2},{value:"Build the Project",id:"build-the-project",level:2},{value:"Model Download",id:"model-download",level:2},{value:"Parameter Description",id:"parameter-description",level:2},{value:"Quick Start",id:"quick-start",level:2},{value:"Notes",id:"notes",level:2}];function a(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"image-classification---mobilenetv2",children:"Image Classification - MobileNetV2"})}),"\n",(0,s.jsxs)(n.p,{children:["This example demonstrates how to perform image classification using a BPU-deployed ",(0,s.jsx)(n.code,{children:"MobileNetV2"})," model with ",(0,s.jsx)(n.code,{children:"C/C++"})," inference. The example code is located in the directory ",(0,s.jsx)(n.code,{children:"/app/cdev_demo/bpu/01_classification_sample/02_mobilenetv2/"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"model-description",children:"Model Description"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Overview"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"MobileNetV2 is a lightweight convolutional neural network introduced by Google in 2018, designed for efficient image recognition on mobile devices. It incorporates the Inverted Residual and Linear Bottleneck structures to reduce computational cost while improving performance. MobileNetV2 is highly suitable for deployment on edge devices and in resource-constrained scenarios for tasks such as image classification and detection. The MobileNetV2 model used in this example accepts 224\xd7224 input and is a BPU quantized model supporting the NV12 format."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"HBM Model Name"}),": mobilenetv2_224x224_nv12.hbm"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Input Format"}),": NV12, size 224x224 (separated Y and UV planes)"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Output"}),": Softmax probability distribution over 1000 classes (conforming to the ImageNet 1000-class standard)"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"functionality-description",children:"Functionality Description"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Model Loading"})}),"\n",(0,s.jsx)(n.p,{children:"Load the model file and extract information such as model name, number of inputs and outputs, etc."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Input Preprocessing"})}),"\n",(0,s.jsx)(n.p,{children:"Resize the input image from BGR format to 224x224, then convert it to the hardware-required NV12 format (with Y and UV separated), and structure it into a dictionary for inference interface compatibility."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Inference Execution"})}),"\n",(0,s.jsxs)(n.p,{children:["Call the ",(0,s.jsx)(n.code,{children:".infer()"})," method to perform inference."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Result Post-processing"})}),"\n",(0,s.jsx)(n.p,{children:"Retrieve the model output tensor, parse the probabilities, and display the top-K (default top-5) prediction results, including corresponding class names and probabilities."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"environment-dependencies",children:"Environment Dependencies"}),"\n",(0,s.jsx)(n.p,{children:"Before compiling and running, ensure the following dependencies are installed:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sudo apt update\nsudo apt install libgflags-dev\n"})}),"\n",(0,s.jsx)(n.h2,{id:"directory-structure",children:"Directory Structure"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:".\n\u251c\u2500\u2500 CMakeLists.txt              # CMake build script\n\u251c\u2500\u2500 README.md                   # Usage instructions\n\u251c\u2500\u2500 inc\n\u2502   \u2514\u2500\u2500 mobilenetv2.hpp         # Model inference header file\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 main.cc                 # Main program entry point\n    \u2514\u2500\u2500 mobilenetv2.cc          # Model implementation\n"})}),"\n",(0,s.jsx)(n.h2,{id:"build-the-project",children:"Build the Project"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Build the project","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"mkdir build && cd build\ncmake ..\nmake -j$(nproc)\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"model-download",children:"Model Download"}),"\n",(0,s.jsx)(n.p,{children:"If the model is not found during program execution, download it using the following command:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"wget https://archive.d-robotics.cc/downloads/rdk_model_zoo/rdk_s100/MobileNet/mobilenetv2_224x224_nv12.hbm\n"})}),"\n",(0,s.jsx)(n.h2,{id:"parameter-description",children:"Parameter Description"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Description"}),(0,s.jsx)(n.th,{children:"Default Value"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--model_path"})}),(0,s.jsx)(n.td,{children:"Path to the model file (.hbm format)"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"/opt/hobot/model/s100/basic/mobilenetv2_224x224_nv12.hbm"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--test_img"})}),(0,s.jsx)(n.td,{children:"Path to the test image"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"/app/res/assets/zebra_cls.jpg"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--label_file"})}),(0,s.jsx)(n.td,{children:"Path to the class label mapping file (dict format)"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"/app/res/labels/imagenet1000_clsidx_to_labels.txt"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--top_k"})}),(0,s.jsx)(n.td,{children:"Number of top-k classification results to output"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"5"})})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Run the model","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Ensure you are in the ",(0,s.jsx)(n.code,{children:"build"})," directory."]}),"\n",(0,s.jsxs)(n.li,{children:["Run with default parameters:","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"./mobilenetv2\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Run with custom parameters:","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"./mobilenetv2 \\\n--model_path /opt/hobot/model/s100/basic/mobilenetv2_224x224_nv12.hbm \\\n--test_img /app/res/assets/zebra_cls.jpg \\\n--label_file /app/res/labels/imagenet1000_clsidx_to_labels.txt \\\n--top_k 5\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["View results","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"TOP 0: label=zebra, prob=0.992246\nTOP 1: label=tiger, Panthera tigris, prob=0.00404656\nTOP 2: label=hartebeest, prob=0.00133707\nTOP 3: label=tiger cat, prob=0.000722661\nTOP 4: label=impala, Aepyceros melampus, prob=0.000539704\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"notes",children:"Notes"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The output displays the top-K classes with the highest probabilities."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"For more information on deployment methods or model support, please refer to the official documentation or contact platform technical support."}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(a,{...e})}):a(e)}}}]);