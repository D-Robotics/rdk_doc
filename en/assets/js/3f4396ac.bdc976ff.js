"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[17103,91970],{11470:(e,n,t)=>{t.d(n,{A:()=>y});var r=t(96540),a=t(34164),i=t(23104),s=t(56347),o=t(205),l=t(57485),d=t(31682),c=t(70679);function h(e){return r.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function u(e){const{values:n,children:t}=e;return(0,r.useMemo)(()=>{const e=n??function(e){return h(e).map(({props:{value:e,label:n,attributes:t,default:r}})=>({value:e,label:n,attributes:t,default:r}))}(t);return function(e){const n=(0,d.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function m({value:e,tabValues:n}){return n.some(n=>n.value===e)}function p({queryString:e=!1,groupId:n}){const t=(0,s.W6)(),a=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,l.aZ)(a),(0,r.useCallback)(e=>{if(!a)return;const n=new URLSearchParams(t.location.search);n.set(a,e),t.replace({...t.location,search:n.toString()})},[a,t])]}function g(e){const{defaultValue:n,queryString:t=!1,groupId:a}=e,i=u(e),[s,l]=(0,r.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:i})),[d,h]=p({queryString:t,groupId:a}),[g,b]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,a]=(0,c.Dv)(n);return[t,(0,r.useCallback)(e=>{n&&a.set(e)},[n,a])]}({groupId:a}),_=(()=>{const e=d??g;return m({value:e,tabValues:i})?e:null})();(0,o.A)(()=>{_&&l(_)},[_]);return{selectedValue:s,selectValue:(0,r.useCallback)(e=>{if(!m({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);l(e),h(e),b(e)},[h,b,i]),tabValues:i}}var b=t(92303);const _={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var f=t(74848);function x({className:e,block:n,selectedValue:t,selectValue:r,tabValues:s}){const o=[],{blockElementScrollPositionUntilNextRender:l}=(0,i.a_)(),d=e=>{const n=e.currentTarget,a=o.indexOf(n),i=s[a].value;i!==t&&(l(n),r(i))},c=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=o.indexOf(e.currentTarget)+1;n=o[t]??o[0];break}case"ArrowLeft":{const t=o.indexOf(e.currentTarget)-1;n=o[t]??o[o.length-1];break}}n?.focus()};return(0,f.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":n},e),children:s.map(({value:e,label:n,attributes:r})=>(0,f.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{o.push(e)},onKeyDown:c,onClick:d,...r,className:(0,a.A)("tabs__item",_.tabItem,r?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function j({lazy:e,children:n,selectedValue:t}){const i=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=i.find(e=>e.props.value===t);return e?(0,r.cloneElement)(e,{className:(0,a.A)("margin-top--md",e.props.className)}):null}return(0,f.jsx)("div",{className:"margin-top--md",children:i.map((e,n)=>(0,r.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function v(e){const n=g(e);return(0,f.jsxs)("div",{className:(0,a.A)("tabs-container",_.tabList),children:[(0,f.jsx)(x,{...n,...e}),(0,f.jsx)(j,{...n,...e})]})}function y(e){const n=(0,b.A)();return(0,f.jsx)(v,{...e,children:h(e.children)},String(n))}},19365:(e,n,t)=>{t.d(n,{A:()=>s});t(96540);var r=t(34164);const a={tabItem:"tabItem_Ymn6"};var i=t(74848);function s({children:e,hidden:n,className:t}){return(0,i.jsx)("div",{role:"tabpanel",className:(0,r.A)(a.tabItem,t),hidden:n,children:e})}},28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var r=t(96540);const a={},i=r.createContext(a);function s(e){const n=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),r.createElement(i.Provider,{value:n},e.children)}},85099:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"Robot_development/boxs/body/hand_lmk_gesture_mediapipe","title":"Hand Keypoints and Gesture Recognition (Mediapipe)","description":"Introduction","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/05_Robot_development/03_boxs/body/hand_lmk_gesture_mediapipe.md","sourceDirName":"05_Robot_development/03_boxs/body","slug":"/Robot_development/boxs/body/hand_lmk_gesture_mediapipe","permalink":"/rdk_doc/en/Robot_development/boxs/body/hand_lmk_gesture_mediapipe","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1769237656000,"sidebarPosition":8,"frontMatter":{"sidebar_position":8},"sidebar":"tutorialSidebar","previous":{"title":"Human Instance Tracking Reid","permalink":"/rdk_doc/en/Robot_development/boxs/body/reid"},"next":{"title":"Road Structuring","permalink":"/rdk_doc/en/Robot_development/boxs/driver/parking_perception"}}');var a=t(74848),i=t(28453);t(11470),t(19365);const s={sidebar_position:8},o="Hand Keypoints and Gesture Recognition (Mediapipe)",l={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Supported Platforms",id:"supported-platforms",level:2},{value:"Algorithm Information",id:"algorithm-information",level:2},{value:"Preparations",id:"preparations",level:2},{value:"RDK",id:"rdk",level:3},{value:"Usage",id:"usage",level:2},{value:"Analysis of Results",id:"analysis-of-results",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"hand-keypoints-and-gesture-recognition-mediapipe",children:"Hand Keypoints and Gesture Recognition (Mediapipe)"})}),"\n","\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"The hand keypoint detection algorithm example subscribes to images and smart messages containing hand bounding box information, uses BPU for algorithm inference, and publishes algorithm messages containing hand keypoints and gesture information."}),"\n",(0,a.jsx)(n.p,{children:"The hand keypoint indices are shown in the following diagram:"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/03_boxs/function/image/box_adv/hand_lmk_index.jpeg",alt:""})}),"\n",(0,a.jsx)(n.p,{children:"Code Repositories:"}),"\n",(0,a.jsxs)(n.p,{children:["(",(0,a.jsx)(n.a,{href:"https://github.com/D-Robotics/palm_detection_mediapipe",children:"https://github.com/D-Robotics/palm_detection_mediapipe"}),")"]}),"\n",(0,a.jsxs)(n.p,{children:["(",(0,a.jsx)(n.a,{href:"https://github.com/D-Robotics/hand_landmarks_mediapipe",children:"https://github.com/D-Robotics/hand_landmarks_mediapipe"}),")"]}),"\n",(0,a.jsx)(n.p,{children:'The gesture recognition categories supported by the algorithm, along with their corresponding numerical values in the algorithm message (Attribute member, type "gesture"), are as follows:'}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Gesture"}),(0,a.jsx)(n.th,{children:"Description"}),(0,a.jsx)(n.th,{children:"Value"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"ThumbUp"}),(0,a.jsx)(n.td,{children:"Thumbs up"}),(0,a.jsx)(n.td,{children:"2"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Victory"}),(0,a.jsx)(n.td,{children:'"V" gesture'}),(0,a.jsx)(n.td,{children:"3"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Mute"}),(0,a.jsx)(n.td,{children:"mute gesture"}),(0,a.jsx)(n.td,{children:"4"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Palm"}),(0,a.jsx)(n.td,{children:"Palm"}),(0,a.jsx)(n.td,{children:"5"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Okay"}),(0,a.jsx)(n.td,{children:"OK gesture"}),(0,a.jsx)(n.td,{children:"11"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"ThumbLeft"}),(0,a.jsx)(n.td,{children:"Thumb left"}),(0,a.jsx)(n.td,{children:"12"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"ThumbRight"}),(0,a.jsx)(n.td,{children:"Thumb right"}),(0,a.jsx)(n.td,{children:"13"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Awesome"}),(0,a.jsx)(n.td,{children:'"666" gesture'}),(0,a.jsx)(n.td,{children:"14"})]})]})]}),"\n",(0,a.jsx)(n.p,{children:"Application scenarios: Gesture recognition algorithms integrate hand keypoint detection, gesture analysis, and other technologies, enabling computers to interpret human gestures as corresponding commands. It can be used for gesture control, sign language translation, and other functions, primarily in the fields of smart homes, smart cabins, smart wearables, etc."}),"\n",(0,a.jsxs)(n.p,{children:["Example of gesture-controlled car: ",(0,a.jsx)(n.a,{href:"../../apps/car_gesture_control",children:"Car Gesture Control"})]}),"\n",(0,a.jsx)(n.h2,{id:"supported-platforms",children:"Supported Platforms"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Platform"}),(0,a.jsx)(n.th,{children:"System"}),(0,a.jsx)(n.th,{children:"Function"})]})}),(0,a.jsx)(n.tbody,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"RDK S100"}),(0,a.jsx)(n.td,{children:"Ubuntu 22.04 (Humble)"}),(0,a.jsx)(n.td,{children:"Start MIPI/USB camera and display inference results via web"})]})})]}),"\n",(0,a.jsx)(n.h2,{id:"algorithm-information",children:"Algorithm Information"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Model"}),(0,a.jsx)(n.th,{children:"Platform"}),(0,a.jsx)(n.th,{children:"Input Size"}),(0,a.jsx)(n.th,{children:"Inference FPS"})]})}),(0,a.jsx)(n.tbody,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"mediapipe"}),(0,a.jsx)(n.td,{children:"S100"}),(0,a.jsx)(n.td,{children:"224x224"}),(0,a.jsx)(n.td,{children:"1114"})]})})]}),"\n",(0,a.jsx)(n.h2,{id:"preparations",children:"Preparations"}),"\n",(0,a.jsx)(n.h3,{id:"rdk",children:"RDK"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"RDK is flashed with  Ubuntu 20.04/22.04 system image provided by D-Robotics."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"TogetheROS.Bot has been successfully installed on RDK."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"The RDK is installed with a MIPI or USB camera."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Confirm that the PC can access the RDK through the network."}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,a.jsx)(n.p,{children:"The gesture recognition package (hand_landmarks_mediapipe) subscribes to the hand key point detection package and publishes the hand key point detection results. After inference, it publishes the algorithm message. The WebSocket package is used to display the images and corresponding algorithm results on the PC browser."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Use MIPI Camera to Publish Images"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Configure the tros.b environment\nsource /opt/tros/humble/setup.bash\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"# Copy the configuration file required for running the example from the installation path of tros.b.\ncp -r /opt/tros/${TROS_DISTRO}/lib/palm_detection_mediapipe/config/ .\ncp -r /opt/tros/${TROS_DISTRO}/lib/hand_landmarks_mediapipe/config/ .\n\n# Configuring MIPI camera\nexport CAM_TYPE=mipi\n\n# Start the launch file\nros2 launch hand_landmarks_mediapipe hand_landmarks.launch.py\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Use USB Camera to Publish Images"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Configure the tros.b environment\nsource /opt/tros/humble/setup.bash\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"# Copy the configuration file required for running the example from the installation path of tros.b.\ncp -r /opt/tros/${TROS_DISTRO}/lib/palm_detection_mediapipe/config/ .\ncp -r /opt/tros/${TROS_DISTRO}/lib/hand_landmarks_mediapipe/config/ .\n\n# Configuring USB camera\nexport CAM_TYPE=usb\n\n# Start the launch file\nros2 launch hand_landmarks_mediapipe hand_landmarks.launch.py\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Use local image"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Configure the tros.b environment\nsource /opt/tros/humble/setup.bash\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Copy the configuration file required for running the example from the installation path of tros.b.\ncp -r /opt/tros/${TROS_DISTRO}/lib/palm_detection_mediapipe/config/ .\ncp -r /opt/tros/${TROS_DISTRO}/lib/hand_landmarks_mediapipe/config/ .\n\n# Configuring local image\nexport CAM_TYPE=fb\n\n# Start the launch file\nros2 launch hand_landmarks_mediapipe hand_landmarks.launch.py publish_image_source:=config/example.jpg publish_image_format:=jpg publish_output_image_w:=640 publish_output_image_h:=480\n"})}),"\n",(0,a.jsx)(n.h2,{id:"analysis-of-results",children:"Analysis of Results"}),"\n",(0,a.jsx)(n.p,{children:"The following information will be displayed in the terminal output:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"[palm_detection_mediapipe-4] [DNN]: 3.7.3_(4.2.11 HBRT)\n[hand_landmarks_mediapipe-3] [WARN] [1757389272.651945922] [mono2d_hand_lmk]: Get model name: hand_224_224 from load model.\n[palm_detection_mediapipe-4] [WARN] [1757389272.653466536] [mono2d_palm_det]: Get model name: palm_det_192_192 from load model.\n[palm_detection_mediapipe-4] [WARN] [1757389272.657688231] [mono2d_palm_det]: Enabling zero-copy\n[palm_detection_mediapipe-4] [WARN] [1757389272.657755005] [mono2d_palm_det]: Create hbmem_subscription with topic_name: /hbmem_img\n[hand_landmarks_mediapipe-3] [WARN] [1757389272.658734823] [mono2d_hand_lmk]: Enabling zero-copy\n[hand_landmarks_mediapipe-3] [WARN] [1757389272.658829973] [mono2d_hand_lmk]: Create hbmem_subscription with topic_name: /hbmem_img\n[hand_landmarks_mediapipe-3] [WARN] [1757389272.679073504] [mono2d_hand_lmk]: Loaned messages are only safe with const ref subscription callbacks. If you are using any other kind of subscriptions, set the ROS_DISABLE_LOANED_MESSAGES environment variable to 1 (the default).\n[palm_detection_mediapipe-4] [WARN] [1757389272.679083479] [mono2d_palm_det]: Loaned messages are only safe with const ref subscription callbacks. If you are using any other kind of subscriptions, set the ROS_DISABLE_LOANED_MESSAGES environment variable to 1 (the default).\n[hand_landmarks_mediapipe-3] [WARN] [1757389272.679384552] [mono2d_hand_lmk]: SharedMemImgProcess Recved img encoding: nv12, h: 480, w: 640, step: 640, index: 0, stamp: 1757389272_411007134, data size: 460800, comm delay [268.3575]ms\n[palm_detection_mediapipe-4] [WARN] [1757389272.679384452] [mono2d_palm_det]: SharedMemImgProcess Recved img encoding: nv12, h: 480, w: 640, step: 640, index: 0, stamp: 1757389272_411007134, data size: 460800, comm delay [268.3576]ms\n[hand_landmarks_mediapipe-3] [WARN] [1757389273.715343396] [mono2d_hand_lmk]: input fps: 13.58, out fps: 13.94, infer time ms: 71, post process time ms: 1\n[palm_detection_mediapipe-4] [WARN] [1757389273.723452363] [mono2d_palm_det]: input fps: 13.59, out fps: 13.94, infer time ms: 71, post process time ms: 0\n[palm_detection_mediapipe-4] [WARN] [1757389275.711869066] [mono2d_palm_det]: SharedMemImgProcess Recved img encoding: nv12, h: 480, w: 640, step: 640, index: 33, stamp: 1757389275_710984298, data size: 460800, comm delay [0.8785]ms\n[hand_landmarks_mediapipe-3] [WARN] [1757389275.711873416] [mono2d_hand_lmk]: SharedMemImgProcess Recved img encoding: nv12, h: 480, w: 640, step: 640, index: 33, stamp: 1757389275_710984298, data size: 460800, comm delay [0.8835]ms\n[hobot_codec_republish-2] [WARN] [1757389277.211724002] [hobot_codec_decoder]: Pub img fps [9.66]\n[hand_landmarks_mediapipe-3] [WARN] [1757389278.811834846] [mono2d_hand_lmk]: SharedMemImgProcess Recved img encoding: nv12, h: 480, w: 640, step: 640, index: 64, stamp: 1757389278_810957877, data size: 460800, comm delay [0.8710]ms\n"})}),"\n",(0,a.jsx)(n.p,{children:"The output log shows that the program runs successfully, with a single inference time of 0.87ms after initialization."}),"\n",(0,a.jsxs)(n.p,{children:["Enter ",(0,a.jsx)(n.a,{href:"http://IP:8000",children:"http://IP:8000"})," in a PC browser to view the images and algorithm rendering effects (IP is the RDK's IP address):"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/03_boxs/function/image/box_adv/hand_lmk_web.jpg",alt:""})})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}}}]);