"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[19715],{28453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>o});var t=i(96540);const d={},r=t.createContext(d);function s(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(d):e.components||d:s(e.components),t.createElement(r.Provider,{value:n},e.children)}},91626:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"Algorithm_Application/C++_Sample/PaddleOCR","title":"Text Detection and Recognition - PaddleOCR","description":"This example runs the PaddleOCR model on the BPU inference engine for text detection and recognition, supporting OCR recognition and visualization in Chinese scenarios. The example code is located in the /app/cdevdemo/bpu/08OCRsample/01paddleOCR/ directory.","source":"@site/i18n/en/docusaurus-plugin-content-docs-docs_s/current/04_Algorithm_Application/03_C++_Sample/12_PaddleOCR.md","sourceDirName":"04_Algorithm_Application/03_C++_Sample","slug":"/Algorithm_Application/C++_Sample/PaddleOCR","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/C++_Sample/PaddleOCR","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1771043942000,"sidebarPosition":12,"frontMatter":{"sidebar_position":12},"sidebar":"tutorialSidebar","previous":{"title":"Automatic Speech Recognition - ASR","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/C++_Sample/ASR"},"next":{"title":"USB Camera YOLOv5x Inference","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/C++_Sample/usb_camera"}}');var d=i(74848),r=i(28453);const s={sidebar_position:12},o="Text Detection and Recognition - PaddleOCR",l={},c=[{value:"Model Description",id:"model-description",level:2},{value:"Functionality Description",id:"functionality-description",level:2},{value:"Environment Dependencies",id:"environment-dependencies",level:2},{value:"Directory Structure",id:"directory-structure",level:2},{value:"Build Project",id:"build-project",level:2},{value:"Model Download",id:"model-download",level:2},{value:"Parameter Description",id:"parameter-description",level:2},{value:"Quick Start",id:"quick-start",level:2},{value:"Notes",id:"notes",level:2}];function a(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,d.jsxs)(d.Fragment,{children:[(0,d.jsx)(n.header,{children:(0,d.jsx)(n.h1,{id:"text-detection-and-recognition---paddleocr",children:"Text Detection and Recognition - PaddleOCR"})}),"\n",(0,d.jsxs)(n.p,{children:["This example runs the PaddleOCR model on the BPU inference engine for text detection and recognition, supporting OCR recognition and visualization in Chinese scenarios. The example code is located in the ",(0,d.jsx)(n.code,{children:"/app/cdev_demo/bpu/08_OCR_sample/01_paddleOCR/"})," directory."]}),"\n",(0,d.jsx)(n.h2,{id:"model-description",children:"Model Description"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsx)(n.p,{children:"Overview:"}),"\n",(0,d.jsx)(n.p,{children:"This example implements Chinese text detection and recognition (two-stage OCR) based on PaddleOCR v3. The overall pipeline includes detecting text regions (detection model) and recognizing text content region by region (recognition model)."}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsx)(n.p,{children:"HBM Model Names:"}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsx)(n.p,{children:"Detection Model: cn_PP-OCRv3_det_infer-deploy_640x640_nv12.hbm"}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsx)(n.p,{children:"Recognition Model: cn_PP-OCRv3_rec_infer-deploy_48x320_rgb.hbm"}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsx)(n.p,{children:"Input Format:"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsx)(n.p,{children:"Detection Model: BGR image \u2192 resized to 640\xd7640 and converted to NV12 format (separated Y and UV planes)"}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsx)(n.p,{children:"Recognition Model: Rotated and cropped BGR text patch \u2192 resized to 48\xd7320, normalized, and converted to RGB format"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsx)(n.p,{children:"Output:"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsx)(n.p,{children:"Detection Model: Segmentation probability map (1\xd71\xd7H\xd7W); post-processing yields bounding box coordinates of text regions"}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsx)(n.p,{children:"Recognition Model: Character token logits; decoded via CTC to obtain recognized text strings"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,d.jsx)(n.h2,{id:"functionality-description",children:"Functionality Description"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsx)(n.p,{children:"Model Loading"}),"\n",(0,d.jsx)(n.p,{children:"Load the text detection and recognition models and parse their input/output specifications."}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsx)(n.p,{children:"Input Preprocessing"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsx)(n.p,{children:"Detection Model: Resize the original image to 640\xd7640 and convert it to NV12 format (for BPU inference)."}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsx)(n.p,{children:"Recognition Model: Resize each rotated and cropped text patch to 48\xd7320, convert to RGB format, normalize, and finally reshape into NCHW layout."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsx)(n.p,{children:"Inference Execution"}),"\n",(0,d.jsxs)(n.p,{children:["Call the ",(0,d.jsx)(n.code,{children:".infer()"})," method to perform forward inference, producing a probability map (detection) and logits (recognition)."]}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsx)(n.p,{children:"Post-processing"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsx)(n.p,{children:"Detection Model:"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsx)(n.p,{children:"Binarize the probability map using a predefined threshold"}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsx)(n.p,{children:"Find contours of text regions and dilate them"}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsx)(n.p,{children:"Extract rotated bounding boxes and crop corresponding image regions"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsx)(n.p,{children:"Recognition Model:"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["Decode logits using ",(0,d.jsx)(n.code,{children:"CTCLabelDecode"})," to map them into text strings"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,d.jsx)(n.p,{children:"Finally, overlay the recognition results as red text on a blank canvas and concatenate it with the original image for visualization."}),"\n"]}),"\n"]}),"\n",(0,d.jsx)(n.h2,{id:"environment-dependencies",children:"Environment Dependencies"}),"\n",(0,d.jsx)(n.p,{children:"Before compiling and running, ensure the following dependencies are installed:"}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{className:"language-bash",children:"sudo apt update\nsudo apt install -y libgflags-dev libpolyclipping-dev\n"})}),"\n",(0,d.jsx)(n.h2,{id:"directory-structure",children:"Directory Structure"}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{className:"language-text",children:".\n|-- CMakeLists.txt                 # CMake build script: targets/dependencies/include paths/link libraries\n|-- FangSong.ttf                   # Chinese font (used to render recognized text on the visualization canvas)\n|-- README.md                      # Usage instructions (this file)\n|-- inc\n|   `-- paddleOCR.hpp              # OCR wrapper header: detection/recognition class interfaces (loading/preprocessing/inference/post-processing)\n`-- src\n    |-- main.cc                    # Program entry point: parse arguments \u2192 detect \u2192 crop \u2192 recognize \u2192 visualize \u2192 save\n    `-- paddleOCR.cc               # Implementation details: polygon generation, cropping, CTC decoding, text rendering\n"})}),"\n",(0,d.jsx)(n.h2,{id:"build-project",children:"Build Project"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["Configuration and Compilation","\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{className:"language-bash",children:"mkdir build && cd build\ncmake ..\nmake -j$(nproc)\n"})}),"\n"]}),"\n"]}),"\n",(0,d.jsx)(n.h2,{id:"model-download",children:"Model Download"}),"\n",(0,d.jsx)(n.p,{children:"If models are not found during runtime, download them using the following commands:"}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{className:"language-bash",children:"# Detection model\nwget https://archive.d-robotics.cc/downloads/rdk_model_zoo/rdk_s100/paddle_ocr/cn_PP-OCRv3_det_infer-deploy_640x640_nv12.hbm\n# Recognition model\nwget https://archive.d-robotics.cc/downloads/rdk_model_zoo/rdk_s100/paddle_ocr/cn_PP-OCRv3_rec_infer-deploy_48x320_rgb.hbm\n"})}),"\n",(0,d.jsx)(n.h2,{id:"parameter-description",children:"Parameter Description"}),"\n",(0,d.jsxs)(n.table,{children:[(0,d.jsx)(n.thead,{children:(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.th,{children:"Parameter"}),(0,d.jsx)(n.th,{children:"Description"}),(0,d.jsx)(n.th,{children:"Default Value"})]})}),(0,d.jsxs)(n.tbody,{children:[(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:(0,d.jsx)(n.code,{children:"--det_model_path"})}),(0,d.jsxs)(n.td,{children:["Text detection model (",(0,d.jsx)(n.code,{children:".hbm"}),")"]}),(0,d.jsx)(n.td,{children:(0,d.jsx)(n.code,{children:"/opt/hobot/model/s100/basic/cn_PP-OCRv3_det_infer-deploy_640x640_nv12.hbm"})})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:(0,d.jsx)(n.code,{children:"--rec_model_path"})}),(0,d.jsxs)(n.td,{children:["Text recognition model (",(0,d.jsx)(n.code,{children:".hbm"}),")"]}),(0,d.jsx)(n.td,{children:(0,d.jsx)(n.code,{children:"/opt/hobot/model/s100/basic/cn_PP-OCRv3_rec_infer-deploy_48x320_rgb.hbm"})})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:(0,d.jsx)(n.code,{children:"--test_image"})}),(0,d.jsx)(n.td,{children:"Path to input test image"}),(0,d.jsx)(n.td,{children:(0,d.jsx)(n.code,{children:"/app/res/assets/gt_2322.jpg"})})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:(0,d.jsx)(n.code,{children:"--label_file"})}),(0,d.jsx)(n.td,{children:"Recognition label file"}),(0,d.jsx)(n.td,{children:(0,d.jsx)(n.code,{children:"/app/res/labels/ppocr_keys_v1.txt"})})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:(0,d.jsx)(n.code,{children:"--threshold"})}),(0,d.jsx)(n.td,{children:"Binarization threshold for text regions (used in detection post-processing)"}),(0,d.jsx)(n.td,{children:(0,d.jsx)(n.code,{children:"0.5"})})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:(0,d.jsx)(n.code,{children:"--ratio_prime"})}),(0,d.jsx)(n.td,{children:"Text box expansion factor (used in detection post-processing, affects polygon dilation)"}),(0,d.jsx)(n.td,{children:(0,d.jsx)(n.code,{children:"2.7"})})]})]})]}),"\n",(0,d.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsx)(n.p,{children:"Run the model"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["Ensure you are in the ",(0,d.jsx)(n.code,{children:"build"})," directory"]}),"\n",(0,d.jsxs)(n.li,{children:["Run with default parameters","\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{className:"language-bash",children:"./paddleOCR\n"})}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:["Run with custom parameters","\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{className:"language-bash",children:"./paddleOCR \\\n    --det_model_path /opt/hobot/model/s100/basic/cn_PP-OCRv3_det_infer-deploy_640x640_nv12.hbm \\\n    --rec_model_path /opt/hobot/model/s100/basic/cn_PP-OCRv3_rec_infer-deploy_48x320_rgb.hbm \\\n    --test_image     /app/res/assets/gt_2322.jpg \\\n    --label_file     /app/res/labels/ppocr_keys_v1.txt \\\n    --threshold 0.5 \\\n    --ratio_prime 2.7\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsx)(n.p,{children:"View Results"}),"\n",(0,d.jsxs)(n.p,{children:["Upon successful execution, results will be overlaid on the original image and saved as ",(0,d.jsx)(n.code,{children:"build/result.jpg"}),":"]}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{className:"language-bash",children:"[Saved] Result saved to: result.jpg\n"})}),"\n"]}),"\n"]}),"\n",(0,d.jsx)(n.h2,{id:"notes",children:"Notes"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsxs)(n.p,{children:["The output result is saved as ",(0,d.jsx)(n.code,{children:"result.jpg"}),", which users can inspect directly."]}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsx)(n.p,{children:"For more information about deployment options or model support, please refer to the official documentation or contact platform technical support."}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,d.jsx)(n,{...e,children:(0,d.jsx)(a,{...e})}):a(e)}}}]);