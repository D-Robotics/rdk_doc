"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[43935],{11364:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"Basic_Application/pydev_demo_sample/yolov5_sample","title":"3.3.4 YOLOv5 Model Example Introduction","description":"Example Overview","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/03_Basic_Application/03_pydev_demo_sample/07_yolov5_sample.md","sourceDirName":"03_Basic_Application/03_pydev_demo_sample","slug":"/Basic_Application/pydev_demo_sample/yolov5_sample","permalink":"/rdk_doc/en/Basic_Application/pydev_demo_sample/yolov5_sample","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1770605065000,"sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"3.3.3 YOLOv3 Model Example Introduction","permalink":"/rdk_doc/en/Basic_Application/pydev_demo_sample/yolov3_sample"},"next":{"title":"3.3.5 YOLOv5x Model Example Introduction","permalink":"/rdk_doc/en/Basic_Application/pydev_demo_sample/yolov5x_sample"}}');var t=o(74848),r=o(28453);const s={sidebar_position:4},a="3.3.4 YOLOv5 Model Example Introduction",d={},l=[{value:"Example Overview",id:"example-overview",level:2},{value:"Effect Demonstration",id:"effect-demonstration",level:2},{value:"Hardware Preparation",id:"hardware-preparation",level:2},{value:"Hardware Connection",id:"hardware-connection",level:3},{value:"Quick Start",id:"quick-start",level:2},{value:"Code and Board Location",id:"code-and-board-location",level:3},{value:"Compilation and Execution",id:"compilation-and-execution",level:3},{value:"Execution Effect",id:"execution-effect",level:3},{value:"Detailed Introduction",id:"detailed-introduction",level:2},{value:"Example Program Parameter Options Description",id:"example-program-parameter-options-description",level:3},{value:"Software Architecture Description",id:"software-architecture-description",level:3},{value:"API Process Description",id:"api-process-description",level:3},{value:"FAQ",id:"faq",level:3}];function c(e){const n={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"334-yolov5-model-example-introduction",children:"3.3.4 YOLOv5 Model Example Introduction"})}),"\n",(0,t.jsx)(n.h2,{id:"example-overview",children:"Example Overview"}),"\n",(0,t.jsxs)(n.p,{children:["The YOLOv5 object detection example is a ",(0,t.jsx)(n.strong,{children:"Python interface"})," development code example located in ",(0,t.jsx)(n.code,{children:"/app/pydev_demo/07_yolov5_sample/"}),", demonstrating how to use the YOLOv5 model for object detection tasks. YOLOv5 is the latest version in the YOLO series, offering higher detection accuracy and faster inference speed compared to YOLOv3. This example shows how to perform object detection on static images, identify multiple objects in the image, and draw detection boxes with confidence information on the image."]}),"\n",(0,t.jsx)(n.h2,{id:"effect-demonstration",children:"Effect Demonstration"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/03_pydev_demo_sample/image/pydev_07_runing.png",alt:"output-img"})}),"\n",(0,t.jsx)(n.h2,{id:"hardware-preparation",children:"Hardware Preparation"}),"\n",(0,t.jsx)(n.h3,{id:"hardware-connection",children:"Hardware Connection"}),"\n",(0,t.jsxs)(n.p,{children:["This example only requires the RDK development board itself, without additional peripheral connections. Ensure the development board is properly powered and the system is booted.\n",(0,t.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/03_pydev_demo_sample/image/pydev_07_hw_connect.png",alt:"connect-img"})]}),"\n",(0,t.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,t.jsx)(n.h3,{id:"code-and-board-location",children:"Code and Board Location"}),"\n",(0,t.jsxs)(n.p,{children:["Navigate to ",(0,t.jsx)(n.code,{children:"/app/pydev_demo/07_yolov5_sample/"})," location, where you can see the YOLOv5 example contains the following files:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"root@ubuntu:/app/pydev_demo/07_yolov5_sample# tree\n.\n\u251c\u2500\u2500 coco_classes.names\n\u251c\u2500\u2500 kite.jpg\n\u2514\u2500\u2500 test_yolov5.py\n"})}),"\n",(0,t.jsx)(n.h3,{id:"compilation-and-execution",children:"Compilation and Execution"}),"\n",(0,t.jsx)(n.p,{children:"The Python example does not require compilation and can be run directly:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"python3 test_yolov5.py\n"})}),"\n",(0,t.jsx)(n.h3,{id:"execution-effect",children:"Execution Effect"}),"\n",(0,t.jsx)(n.p,{children:"After running, the program will load the pre-trained YOLOv5 model, perform object detection on the kite.jpg image, and generate the result image output_image.jpg with detection boxes."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"root@ubuntu:/app/pydev_demo/07_yolov5_sample# ./test_yolov5.py \n[BPU_PLAT]BPU Platform Version(1.3.6)!\n[HBRT] set log level as 0. version = 3.15.55.0\n[DNN] Runtime version = 1.24.5_(3.15.55 HBRT)\n[A][DNN][packed_model.cpp:247][Model](2025-09-10,10:48:04.337.848) [HorizonRT] The model builder version = 1.23.5\n[W][DNN]bpu_model_info.cpp:491][Version](2025-09-10,10:48:04.479.654) Model: yolov5s_v2_672x672_bayese_nv12. Inconsistency between the hbrt library version 3.15.55.0 and the model build version 3.15.47.0 detected, in order to ensure correct model results, it is recommended to use compilation tools and the BPU SDK from the same OpenExplorer package.\ntensor type: NV12\ndata type: uint8\nlayout: NCHW\nshape: (1, 3, 672, 672)\n3\ntensor type: float32\ndata type: float32\nlayout: NHWC\nshape: (1, 84, 84, 255)\ntensor type: float32\ndata type: float32\nlayout: NHWC\nshape: (1, 42, 42, 255)\ntensor type: float32\ndata type: float32\nlayout: NHWC\nshape: (1, 21, 21, 255)\nbbox: [593.949768, 80.819038, 672.215027, 147.131607], score: 0.856997, id: 33, name: kite\nbbox: [215.716019, 696.537476, 273.653442, 855.298706], score: 0.852251, id: 0, name: person\nbbox: [278.934448, 236.631256, 305.838867, 281.294922], score: 0.834647, id: 33, name: kite\nbbox: [115.184196, 615.987, 167.202667, 761.042542], score: 0.781627, id: 0, name: person\nbbox: [577.261719, 346.008453, 601.795349, 370.308624], score: 0.705358, id: 33, name: kite\nbbox: [1083.22998, 394.714569, 1102.146729, 422.34787], score: 0.673642, id: 33, name: kite\nbbox: [80.515938, 511.157104, 107.181572, 564.28363], score: 0.662, id: 0, name: person\nbbox: [175.470078, 541.949219, 194.192871, 572.981812], score: 0.623189, id: 0, name: person\nbbox: [518.504333, 508.224396, 533.452759, 531.92926], score: 0.597822, id: 0, name: person\nbbox: [469.970398, 340.634796, 486.181305, 358.508972], score: 0.5593, id: 33, name: kite\nbbox: [32.987705, 512.65033, 57.665741, 554.898804], score: 0.508812, id: 0, name: person\nbbox: [345.142609, 486.988464, 358.24762, 504.551331], score: 0.50672, id: 0, name: person\nbbox: [530.825439, 513.695679, 555.200256, 536.498352], score: 0.459818, id: 0, name: person\ndraw result time is : 0.03627920150756836\n"})}),"\n",(0,t.jsx)(n.h2,{id:"detailed-introduction",children:"Detailed Introduction"}),"\n",(0,t.jsx)(n.h3,{id:"example-program-parameter-options-description",children:"Example Program Parameter Options Description"}),"\n",(0,t.jsx)(n.p,{children:"The YOLOv5 object detection example does not require command line parameters and can be run directly. The program will automatically load the kite.jpg image in the same directory for detection processing."}),"\n",(0,t.jsx)(n.h3,{id:"software-architecture-description",children:"Software Architecture Description"}),"\n",(0,t.jsx)(n.p,{children:"The software architecture of the YOLOv5 object detection example includes the following core components:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Model Loading: Using the pyeasy_dnn module to load the pre-trained YOLOv5 model"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Image Preprocessing: Converting the input image to the NV12 format and specified dimensions (672x672) required by the model"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Model Inference: Calling the model for forward computation to generate feature maps"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Post-processing: Using the libpostprocess library to parse model outputs and generate detection results"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Result Visualization: Drawing detection boxes and category information on the original image"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Result Saving: Saving the visualized results as an image file"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)("center",{children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/03_pydev_demo_sample/image/pydev_07_yolov5_sample_software_arch.png",alt:"software_arch"})})}),"\n",(0,t.jsx)(n.h3,{id:"api-process-description",children:"API Process Description"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Model Loading: models = dnn.load('../models/yolov5s_672x672_nv12.bin')"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Image Preprocessing: Adjusting image dimensions and converting to NV12 format"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Model Inference: outputs = models[0].forward(nv12_data)"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Post-processing Configuration: Setting post-processing parameters (dimensions, thresholds, etc.)"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Result Parsing: Calling the post-processing library to parse output tensors"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Result Visualization: Drawing detection boxes and label information on the original image"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Result Saving: Saving the results as an image file"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)("center",{children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/03_pydev_demo_sample/image/pydev_07_yolov5_sample_api_flow.png",alt:"API_Flow"})})}),"\n",(0,t.jsx)(n.h3,{id:"faq",children:"FAQ"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Q:"})," What's the difference between YOLOv5 and YOLOv3?",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"A:"})," YOLOv5 has improvements in network architecture, training strategies, and post-processing algorithms, typically offering higher detection accuracy and faster inference speed."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Q:"})," What should I do if I encounter \"No module named 'hobot_dnn'\" error when running the example?",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"A:"})," Please ensure the RDK Python environment is correctly installed, including the hobot_dnn module and other official dedicated inference libraries."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Q:"})," How to change the test image?",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"A:"})," Place the new image file in the example directory and modify ",(0,t.jsx)(n.code,{children:"img_file = cv2.imread('your_image_path')"})," in the code."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Q:"})," What should I do if the detection results are inaccurate?",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"A:"})," The YOLOv5 model is trained on the COCO dataset and may require fine-tuning for specific scenarios or using more suitable models."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Q:"})," How to adjust the detection threshold?",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"A:"})," Modify the value of ",(0,t.jsx)(n.code,{children:"yolov5_postprocess_info.score_threshold"})," in the code. For example, changing it to 0.5 can increase detection sensitivity."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Q:"})," Can real-time video stream processing be achieved?",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"A:"})," The current example is designed for single images, but the code can be modified to achieve real-time object detection on video streams."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Q:"})," How to obtain YOLOv5 models of other sizes?",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"A:"})," You can refer to the ",(0,t.jsx)(n.a,{href:"https://github.com/D-Robotics/rdk_model_zoo",children:"model_zoo repository"})," or ",(0,t.jsx)(n.a,{href:"https://github.com/D-Robotics/hobot_model",children:"Toolchain's basic model repository"})]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Q:"})," How to further improve detection speed?",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"A:"})," You can try using lighter YOLOv5 variants like YOLOv5n or YOLOv5s, or reduce the input image resolution."]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},28453:(e,n,o)=>{o.d(n,{R:()=>s,x:()=>a});var i=o(96540);const t={},r=i.createContext(t);function s(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);