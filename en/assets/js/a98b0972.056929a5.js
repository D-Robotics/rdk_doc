"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[65637],{22131:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>d,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"Algorithm_Application/Python_Sample/PaddleOCR","title":"Text Detection and Recognition - PaddleOCR","description":"This example runs the PaddleOCR model using the hbmruntime inference engine for text detection and recognition, supporting OCR recognition and visualization in Chinese scenarios. The example code is located in the /app/pydevdemo/08OCRsample/01paddleOCR/ directory. The example code is also located in the /app/pydevdemo/02detectionsample/02ultralyticsyolo11/ directory.","source":"@site/i18n/en/docusaurus-plugin-content-docs-docs_s/current/04_Algorithm_Application/02_Python_Sample/12_PaddleOCR.md","sourceDirName":"04_Algorithm_Application/02_Python_Sample","slug":"/Algorithm_Application/Python_Sample/PaddleOCR","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/Python_Sample/PaddleOCR","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1765546934000,"sidebarPosition":12,"frontMatter":{"sidebar_position":12},"sidebar":"tutorialSidebar","previous":{"title":"Automatic Speech Recognition - ASR","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/Python_Sample/ASR"},"next":{"title":"USB Camera YOLOv5x Inference","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/Python_Sample/USB_Camera_yolov5x"}}');var s=i(74848),r=i(28453);const d={sidebar_position:12},o="Text Detection and Recognition - PaddleOCR",l={},c=[{value:"Model Description",id:"model-description",level:2},{value:"Functionality Description",id:"functionality-description",level:2},{value:"Environment Dependencies",id:"environment-dependencies",level:2},{value:"Directory Structure",id:"directory-structure",level:2},{value:"Parameter Description",id:"parameter-description",level:2},{value:"Quick Start",id:"quick-start",level:2},{value:"Notes",id:"notes",level:2}];function a(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"text-detection-and-recognition---paddleocr",children:"Text Detection and Recognition - PaddleOCR"})}),"\n",(0,s.jsxs)(n.p,{children:["This example runs the PaddleOCR model using the ",(0,s.jsx)(n.code,{children:"hbm_runtime"})," inference engine for text detection and recognition, supporting OCR recognition and visualization in Chinese scenarios. The example code is located in the ",(0,s.jsx)(n.code,{children:"/app/pydev_demo/08_OCR_sample/01_paddleOCR/"})," directory. The example code is also located in the ",(0,s.jsx)(n.code,{children:"/app/pydev_demo/02_detection_sample/02_ultralytics_yolo11/"})," directory."]}),"\n",(0,s.jsx)(n.h2,{id:"model-description",children:"Model Description"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Overview"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"This example implements a two-stage OCR task for Chinese text detection and recognition based on PaddleOCR v3. The overall pipeline includes detecting text regions (using a detection model) and recognizing text content region by region (using a recognition model)."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"HBM Model Names"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Detection Model"}),": ",(0,s.jsx)(n.code,{children:"cn_PP-OCRv3_det_infer-deploy_640x640_nv12.hbm"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Recognition Model"}),": ",(0,s.jsx)(n.code,{children:"cn_PP-OCRv3_rec_infer-deploy_48x320_rgb.hbm"})]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Input Format"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Detection Model"}),": BGR image \u2192 resized to 640\xd7640 and converted to NV12 format (separated Y and UV planes)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Recognition Model"}),": Rotated and cropped BGR text patch \u2192 resized to 48\xd7320, normalized, and converted to RGB format"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Output"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Detection Model"}),": Segmentation probability map (1\xd71\xd7H\xd7W); post-processing yields text bounding box coordinates"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Recognition Model"}),": Character token logits; decoded via CTC to produce recognized text strings"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Model Download URLs"})," (automatically downloaded by the program):"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Detection model\nhttps://archive.d-robotics.cc/downloads/rdk_model_zoo/rdk_s100/paddle_ocr/cn_PP-OCRv3_det_infer-deploy_640x640_nv12.hbm\n# Recognition model\nhttps://archive.d-robotics.cc/downloads/rdk_model_zoo/rdk_s100/paddle_ocr/cn_PP-OCRv3_rec_infer-deploy_48x320_rgb.hbm\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"functionality-description",children:"Functionality Description"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Model Loading"})}),"\n",(0,s.jsxs)(n.p,{children:["Uses ",(0,s.jsx)(n.code,{children:"hbm_runtime"})," to load the text detection and recognition models separately, parses input/output names and shapes, and supports setting inference priority and BPU core binding."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Input Preprocessing"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Detection Model"}),": Resizes the original image to 640\xd7640 and converts it to NV12 format (for BPU inference)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Recognition Model"}),": Resizes each rotated and cropped text patch to 48\xd7320, converts to RGB format, normalizes the pixel values, and reshapes to NCHW layout."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Inference Execution"})}),"\n",(0,s.jsxs)(n.p,{children:["Calls the ",(0,s.jsx)(n.code,{children:".run()"})," method to perform forward inference, producing a probability map (detection) and logits (recognition)."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Post-processing"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Detection Model"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Binarizes the probability map using a specified threshold"}),"\n",(0,s.jsx)(n.li,{children:"Finds contours of text regions and dilates them"}),"\n",(0,s.jsx)(n.li,{children:"Extracts rotated bounding boxes and crops corresponding image regions"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Recognition Model"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Decodes logits using ",(0,s.jsx)(n.code,{children:"CTCLabelDecode"})," to map them to text strings"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Finally, the recognized text is annotated in red on a blank canvas and stitched with the original image for visualization."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"environment-dependencies",children:"Environment Dependencies"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Ensure the dependencies in ",(0,s.jsx)(n.code,{children:"pydev"})," are installed:","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install -r ../../requirements.txt\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Install packages required for OCR processing:","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install pyclipper==1.3.0.post6 Pillow==9.0.1 paddlepaddle\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"directory-structure",children:"Directory Structure"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:".\n\u251c\u2500\u2500 FangSong.ttf                # Font for Chinese character display\n\u251c\u2500\u2500 paddle_ocr.py               # Main program implementing text detection and recognition\n\u251c\u2500\u2500 postprocess/                # Post-processing logic (sorting, merging, decoding, etc.)\n\u2514\u2500\u2500 README.md                   # Usage instructions\n"})}),"\n",(0,s.jsx)(n.h2,{id:"parameter-description",children:"Parameter Description"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Default Value"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--det-model-path"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"/opt/hobot/model/s100/basic/cn_PP-OCRv3_det_infer-deploy_640x640_nv12.hbm"})}),(0,s.jsx)(n.td,{children:"Path to the text detection model"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--rec-model-path"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"/opt/hobot/model/s100/basic/cn_PP-OCRv3_rec_infer-deploy_48x320_rgb.hbm"})}),(0,s.jsx)(n.td,{children:"Path to the text recognition model"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--priority"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"0"})}),(0,s.jsx)(n.td,{children:"Inference priority (higher value = higher priority)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--bpu-cores"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"[0]"})}),(0,s.jsx)(n.td,{children:"Indices of BPU cores to run inference on"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--test-img"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"/app/res/assets/gt_2322.jpg"})}),(0,s.jsx)(n.td,{children:"Input image path"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--label-file"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"/app/res/labels/ppocr_keys_v1.txt"})}),(0,s.jsx)(n.td,{children:"Path to label file required for text recognition"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--img-save-path"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"result.jpg"})}),(0,s.jsx)(n.td,{children:"Path to save the inference result image"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--threshold"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"0.5"})}),(0,s.jsx)(n.td,{children:"Threshold for binarizing text regions"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--ratio-prime"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"2.7"})}),(0,s.jsx)(n.td,{children:"Expansion factor for text bounding boxes"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Run the model"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["With default parameters:","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python paddle_ocr.py\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["With custom parameters:","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python paddle_ocr.py \\\n--det-model-path /opt/hobot/model/s100/basic/cn_PP-OCRv3_det_infer-deploy_640x640_nv12.hbm \\\n--rec-model-path /opt/hobot/model/s100/basic/cn_PP-OCRv3_rec_infer-deploy_48x320_rgb.hbm \\\n--test-img /app/res/assets/gt_2322.jpg \\\n--label-file /app/res/labels/ppocr_keys_v1.txt \\\n--img-save-path result.jpg \\\n--priority 0 \\\n--bpu-cores 0 \\\n--threshold 0.5 \\\n--ratio-prime 2.7\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"View Results"})}),"\n",(0,s.jsxs)(n.p,{children:["Upon successful execution, the results will be overlaid on the original image and saved to the path specified by ",(0,s.jsx)(n.code,{children:"--img-save-path"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"[Saved] Result saved to: result.jpg\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"notes",children:"Notes"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"If the specified model path does not exist, the program will attempt to download the model automatically."}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(a,{...e})}):a(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>d,x:()=>o});var t=i(96540);const s={},r=t.createContext(s);function d(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:d(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);