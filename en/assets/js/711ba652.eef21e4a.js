"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[17970],{28453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>d});var o=i(96540);const t={},r=o.createContext(t);function s(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),o.createElement(r.Provider,{value:n},e.children)}},39422:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>d,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"Basic_Application/cdev_demo_sample/bpu","title":"3.2.1 BPU Sample Introduction","description":"Sample Overview","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/03_Basic_Application/02_cdev_demo_sample/bpu.md","sourceDirName":"03_Basic_Application/02_cdev_demo_sample","slug":"/Basic_Application/cdev_demo_sample/bpu","permalink":"/rdk_doc/en/Basic_Application/cdev_demo_sample/bpu","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1764745544000,"sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"3.2 C DEV \u63a5\u53e3\u793a\u4f8b ","permalink":"/rdk_doc/en/03_Basic_Application/02_cdev_demo_sample"},"next":{"title":"3.2.2 decode2display Sample Introduction","permalink":"/rdk_doc/en/Basic_Application/cdev_demo_sample/decode2display"}}');var t=i(74848),r=i(28453);const s={sidebar_position:1},d="3.2.1 BPU Sample Introduction",a={},c=[{value:"Sample Overview",id:"sample-overview",level:2},{value:"Effect Demonstration",id:"effect-demonstration",level:2},{value:"Hardware Preparation",id:"hardware-preparation",level:2},{value:"Hardware Connection",id:"hardware-connection",level:3},{value:"Quick Start",id:"quick-start",level:2},{value:"Code and Board Location",id:"code-and-board-location",level:3},{value:"Compilation and Execution",id:"compilation-and-execution",level:3},{value:"Execution Effect",id:"execution-effect",level:3},{value:"Detailed Introduction",id:"detailed-introduction",level:2},{value:"Sample Program Parameter Options Description",id:"sample-program-parameter-options-description",level:3},{value:"Software Architecture Description",id:"software-architecture-description",level:3},{value:"API Flow Description",id:"api-flow-description",level:3},{value:"FAQ",id:"faq",level:3}];function l(e){const n={br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"321-bpu-sample-introduction",children:"3.2.1 BPU Sample Introduction"})}),"\n",(0,t.jsx)(n.h2,{id:"sample-overview",children:"Sample Overview"}),"\n",(0,t.jsxs)(n.p,{children:["BPU is a ",(0,t.jsx)(n.strong,{children:"C language interface"})," development code example located in the ",(0,t.jsx)(n.code,{children:"/app/cdev_demo"})," directory, demonstrating how to call models already supported by BPU using C language. By referencing this example, users can understand and develop related applications."]}),"\n",(0,t.jsx)(n.h2,{id:"effect-demonstration",children:"Effect Demonstration"}),"\n",(0,t.jsx)(n.p,{children:"The BPU sample supports two scenarios: one with a camera, which fixedly uses the YOLO model, and one without a camera, which performs inference on backfilled data."}),"\n",(0,t.jsx)(n.p,{children:"The following shows the effect of using a camera with YOLOv5 for inference. The monitor displays that a water bottle has been detected:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/02_cdev_demo_sample/image/cdev_bpu_running_yolo5v.png",alt:"output-img"})}),"\n",(0,t.jsx)(n.p,{children:"The following shows the effect of using FCOS for inference. The monitor displays the inference results using an H264 file as input data:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/02_cdev_demo_sample/image/cdev_bpu_running_fcos.png",alt:"output-img"})}),"\n",(0,t.jsx)(n.h2,{id:"hardware-preparation",children:"Hardware Preparation"}),"\n",(0,t.jsx)(n.h3,{id:"hardware-connection",children:"Hardware Connection"}),"\n",(0,t.jsx)(n.p,{children:"(1) YOLOv5 inference with camera"}),"\n",(0,t.jsx)(n.p,{children:"This example does not require a mouse or keyboard, so only a camera, HDMI display, Ethernet port, and power cable are connected here:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/02_cdev_demo_sample/image/vio_display_hardware_connect.png",alt:"connect-img"})}),"\n",(0,t.jsx)(n.p,{children:"(2) Inference without camera using H264 stream, with results displayed on monitor"}),"\n",(0,t.jsx)(n.p,{children:"This example does not require a mouse or keyboard, so only an HDMI display, Ethernet port, and power cable are connected here:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/02_cdev_demo_sample/image/hardware-connect.png",alt:"connect-img"})}),"\n",(0,t.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,t.jsx)(n.h3,{id:"code-and-board-location",children:"Code and Board Location"}),"\n",(0,t.jsxs)(n.p,{children:["Navigate to the ",(0,t.jsx)(n.code,{children:"/app/cdev_demo/bpu"})," location, where you can see two directories and a README.md file. The ",(0,t.jsx)(n.code,{children:"include"})," directory contains the header files required for the sample models, while the ",(0,t.jsx)(n.code,{children:"src"})," directory contains the program entry point and implementations of pre-processing, inference, post-processing, etc., for various models."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"root@ubuntu:/app/cdev_demo/bpu# tree -L 1\n.\n\u251c\u2500\u2500 include\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 src\n"})}),"\n",(0,t.jsx)(n.h3,{id:"compilation-and-execution",children:"Compilation and Execution"}),"\n",(0,t.jsxs)(n.p,{children:["We need to enter the ",(0,t.jsx)(n.code,{children:"src"})," directory and execute ",(0,t.jsx)(n.code,{children:"make"}),". The output will appear in the ",(0,t.jsx)(n.code,{children:"src/bin"})," directory."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\nroot@ubuntu:/app/cdev_demo/bpu/src/bin# tree\n.\n\u251c\u2500\u2500 1080p_.h264\n\u2514\u2500\u2500 sample\n\n"})}),"\n",(0,t.jsxs)(n.p,{children:["We need to execute in the ",(0,t.jsx)(n.code,{children:"/app/cdev_demo/bpu/src/bin"})," directory. Here, we use the YOLOv5 inference with camera as the execution example."]}),"\n",(0,t.jsx)(n.h3,{id:"execution-effect",children:"Execution Effect"}),"\n",(0,t.jsxs)(n.p,{children:["(1) Use the ",(0,t.jsx)(n.code,{children:"systemctl stop lightdm"})," command to close the graphical interface service."]}),"\n",(0,t.jsxs)(n.p,{children:["(2) YOLOv5 inference with camera\nIn the ",(0,t.jsx)(n.code,{children:"root@ubuntu:/app/cdev_demo/bpu/src/bin#"})," directory, use the ",(0,t.jsx)(n.code,{children:"./sample -f /app/model/basic/yolov5s_672x672_nv12.bin -m 0"})," command."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"root@ubuntu:/app/cdev_demo/bpu/src/bin# ./sample -f /app/model/basic/yolov5s_672x672_nv12.bin -m 0\nOpened DRM device: /dev/dri/card0\n[BPU_PLAT]BPU Platform Version(1.3.6)!\n[HBRT] set log level as 0. version = 3.15.55.0\n[DNN] Runtime version = 1.24.5_(3.15.55 HBRT)\n[A][DNN][packed_model.cpp:247][Model](2025-10-25,23:45:13.56.570) [HorizonRT] The model builder version = 1.23.5\n[W][DNN]bpu_model_info.cpp:491][Version](2025-10-25,23:45:13.222.713) Model: yolov5s_v2_672x672_bayese_nv12. Inconsistency between the hbrt library version 3.15.55.0 and the model build version 3.15.47.0 detected, in order to ensure correct model results, it is recommended to use compilation tools and the BPU SDK from the same OpenExplorer package.\nModel info:\nmodel_name: yolov5s_v2_672x672_bayese_nv12Input count: 1input[0]: tensorLayout: 2 tensorType: 1 validShape:(1, 3, 672, 672, ), alignedShape:(1, 3, 672, 672, )\nOutput count: 3Output[0]: tensorLayout: 0 tensorType: 13 validShape:(1, 84, 84, 255, ), alignedShape:(1, 84, 84, 255, )\nOutput[1]: tensorLayout: 0 tensorType: 13 validShape:(1, 42, 42, 255, ), alignedShape:(1, 42, 42, 255, )\nOutput[2]: tensorLayout: 0 tensorType: 13 validShape:(1, 21, 21, 255, ), alignedShape:(1, 21, 21, 255, )\n2025/10/25 23:45:13.229 !INFO [OpenCamera][0447]hbn module\nset camera fps: -1,width: 1920,height: 1080\nCamera 0:\n        mipi_host: 0\n......\n......\n......\n\n"})}),"\n",(0,t.jsxs)(n.p,{children:["You will see the same effect as shown in the image at the beginning of the document:\n",(0,t.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/02_cdev_demo_sample/image/cdev_bpu_running_yolo5v.png",alt:"output-img"})]}),"\n",(0,t.jsxs)(n.p,{children:["(3) Inference using FCOS model after decoding H264\nIn the ",(0,t.jsx)(n.code,{children:"root@ubuntu:/app/cdev_demo/bpu/src/bin#"})," directory, use the ",(0,t.jsx)(n.code,{children:"./sample -f /app/model/basic/fcos_512x512_nv12.bin -m 1 -i 1080p_.h264 -w 1920 -h 1080"})," command."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"root@ubuntu:/app/cdev_demo/bpu/src/bin# ./sample -f /app/model/basic/fcos_512x512_nv12.bin -m 1 -i 1080p_.h264 -w 1920 -h 1080\nOpened DRM device: /dev/dri/card0\n[BPU_PLAT]BPU Platform Version(1.3.6)!\n[HBRT] set log level as 0. version = 3.15.55.0\n[DNN] Runtime version = 1.24.5_(3.15.55 HBRT)\n[A][DNN][packed_model.cpp:247][Model](2025-10-25,23:46:08.985.465) [HorizonRT] The model builder version = 1.23.5\n[W][DNN]bpu_model_info.cpp:491][Version](2025-10-25,23:46:09.127.913) Model: fcos_efficientnetb0_512x512_nv12. Inconsistency between the hbrt library version 3.15.55.0 and the model build version 3.15.47.0 detected, in order to ensure correct model results, it is recommended to use compilation tools and the BPU SDK from the same OpenExplorer package.\nModel info:\nmodel_name: fcos_efficientnetb0_512x512_nv12Input count: 1input[0]: tensorLayout: 2 tensorType: 1 validShape:(1, 3, 512, 512, ), alignedShape:(1, 3, 512, 512, )\nOutput count: 15Output[0]: tensorLayout: 0 tensorType: 14 validShape:(1, 64, 64, 80, ), alignedShape:(1, 64, 64, 80, )\n......\n......\n......\n\n"})}),"\n",(0,t.jsxs)(n.p,{children:["You will see the same effect as shown in the image at the beginning of the document:\n",(0,t.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/02_cdev_demo_sample/image/cdev_bpu_running_fcos.png",alt:"output-img"})]}),"\n",(0,t.jsx)(n.h2,{id:"detailed-introduction",children:"Detailed Introduction"}),"\n",(0,t.jsx)(n.p,{children:"This sample code is relatively extensive, but there are fixed correspondences during execution. Our YOLOv5 is paired with a camera."}),"\n",(0,t.jsx)(n.h3,{id:"sample-program-parameter-options-description",children:"Sample Program Parameter Options Description"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\nroot@ubuntu:/app/cdev_demo/bpu/src/bin# ./sample\nUsage: sample [OPTION...]\nbpu sample -- An C++ example of using bpu\n\n  -d, --debug                Print lots of debugging information.\n  -f, --file=modle_file      path of model file\n  -h, --video_height=height  height of video\n  -i, --input_video=video path   path of video\n  -m, --mode=type            0:yolov5;1:fcos\n  -w, --video_width=width    width of video\n  -?, --help                 Give this help list\n      --usage                Give a short usage message\n\nMandatory or optional arguments to long options are also mandatory or optional\nfor any corresponding short options.\nroot@ubuntu:/app/cdev_demo/bpu/src/bin#\n\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Where:",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.code,{children:"-d"})," represents printing debug information.",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.code,{children:"-f"})," represents the location of the model.",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.code,{children:"-i"})," represents the path to the video file input to the program. Note that this is only input when not using a camera; it is not needed when running the example with a camera.",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.code,{children:"-m"})," represents the selected model.",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.code,{children:"-w"})," represents the width of the output video.",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.code,{children:"-h"})," represents the height of the output video.",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.code,{children:"-?"})," represents printing help information."]}),"\n",(0,t.jsx)(n.h3,{id:"software-architecture-description",children:"Software Architecture Description"}),"\n",(0,t.jsx)(n.p,{children:"This Sample is implemented based on the spcdev interface. It parses the parameters passed to main, uses the libspcdev.so API to obtain the display resolution, then initializes the model module, display module, and video input module. Based on the adapted resolution and the display resolution, it determines whether to use VPS for scaling. Through appropriate pre-processing and post-processing threads, the inference results are converted into coordinates and presented on the display. Since this sample code includes inference examples for multiple models, we have extracted the main core logic for display in the software architecture diagram."}),"\n",(0,t.jsx)("center",{children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/02_cdev_demo_sample/image/cdev_bpu_single_software_arch.png",alt:"software_arch"})})}),"\n",(0,t.jsx)(n.h3,{id:"api-flow-description",children:"API Flow Description"}),"\n",(0,t.jsx)("center",{children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/02_cdev_demo_sample/image/cdev_bpu_api_flow.png",alt:"API_Flow"})})}),"\n",(0,t.jsx)(n.h3,{id:"faq",children:"FAQ"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Q:"})," Is the pre-processing different for different models?",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"A:"})," It depends on the characteristics of the model; generally, different models require different pre-processing."]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}}}]);