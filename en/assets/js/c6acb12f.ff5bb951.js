"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[58311],{11470:(e,n,t)=>{t.d(n,{A:()=>y});var i=t(96540),r=t(34164),o=t(23104),l=t(56347),a=t(205),s=t(57485),c=t(31682),d=t(70679);function u(e){return i.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,i.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function p(e){const{values:n,children:t}=e;return(0,i.useMemo)(()=>{const e=n??function(e){return u(e).map(({props:{value:e,label:n,attributes:t,default:i}})=>({value:e,label:n,attributes:t,default:i}))}(t);return function(e){const n=(0,c.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function h({value:e,tabValues:n}){return n.some(n=>n.value===e)}function m({queryString:e=!1,groupId:n}){const t=(0,l.W6)(),r=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,s.aZ)(r),(0,i.useCallback)(e=>{if(!r)return;const n=new URLSearchParams(t.location.search);n.set(r,e),t.replace({...t.location,search:n.toString()})},[r,t])]}function g(e){const{defaultValue:n,queryString:t=!1,groupId:r}=e,o=p(e),[l,s]=(0,i.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!h({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:o})),[c,u]=m({queryString:t,groupId:r}),[g,f]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,r]=(0,d.Dv)(n);return[t,(0,i.useCallback)(e=>{n&&r.set(e)},[n,r])]}({groupId:r}),b=(()=>{const e=c??g;return h({value:e,tabValues:o})?e:null})();(0,a.A)(()=>{b&&s(b)},[b]);return{selectedValue:l,selectValue:(0,i.useCallback)(e=>{if(!h({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);s(e),u(e),f(e)},[u,f,o]),tabValues:o}}var f=t(92303);const b={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var x=t(74848);function _({className:e,block:n,selectedValue:t,selectValue:i,tabValues:l}){const a=[],{blockElementScrollPositionUntilNextRender:s}=(0,o.a_)(),c=e=>{const n=e.currentTarget,r=a.indexOf(n),o=l[r].value;o!==t&&(s(n),i(o))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=a.indexOf(e.currentTarget)+1;n=a[t]??a[0];break}case"ArrowLeft":{const t=a.indexOf(e.currentTarget)-1;n=a[t]??a[a.length-1];break}}n?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":n},e),children:l.map(({value:e,label:n,attributes:i})=>(0,x.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{a.push(e)},onKeyDown:d,onClick:c,...i,className:(0,r.A)("tabs__item",b.tabItem,i?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function j({lazy:e,children:n,selectedValue:t}){const o=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=o.find(e=>e.props.value===t);return e?(0,i.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:o.map((e,n)=>(0,i.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function v(e){const n=g(e);return(0,x.jsxs)("div",{className:(0,r.A)("tabs-container",b.tabList),children:[(0,x.jsx)(_,{...n,...e}),(0,x.jsx)(j,{...n,...e})]})}function y(e){const n=(0,f.A)();return(0,x.jsx)(v,{...e,children:u(e.children)},String(n))}},19365:(e,n,t)=>{t.d(n,{A:()=>l});t(96540);var i=t(34164);const r={tabItem:"tabItem_Ymn6"};var o=t(74848);function l({children:e,hidden:n,className:t}){return(0,o.jsx)("div",{role:"tabpanel",className:(0,i.A)(r.tabItem,t),hidden:n,children:e})}},28453:(e,n,t)=>{t.d(n,{R:()=>l,x:()=>a});var i=t(96540);const r={},o=i.createContext(r);function l(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),i.createElement(o.Provider,{value:n},e.children)}},55509:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>u});const i=JSON.parse('{"id":"Robot_development/boxs/function/hobot_clip","title":"CLIP","description":"Introduction","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/05_Robot_development/03_boxs/function/hobot_clip.md","sourceDirName":"05_Robot_development/03_boxs/function","slug":"/Robot_development/boxs/function/hobot_clip","permalink":"/rdk_doc/en/Robot_development/boxs/function/hobot_clip","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1750324060000,"sidebarPosition":13,"frontMatter":{"sidebar_position":13},"sidebar":"tutorialSidebar","previous":{"title":"Visual Inertial Odometry Algorithm","permalink":"/rdk_doc/en/Robot_development/boxs/function/hobot_vio"},"next":{"title":"Stereo Depth Algorithm","permalink":"/rdk_doc/en/Robot_development/boxs/function/hobot_stereonet"}}');var r=t(74848),o=t(28453),l=t(11470),a=t(19365);const s={sidebar_position:13},c="CLIP",d={},u=[{value:"Introduction",id:"introduction",level:2},{value:"Supported Platforms",id:"supported-platforms",level:2},{value:"Preparation",id:"preparation",level:2},{value:"RDK",id:"rdk",level:3},{value:"Dependency Installation",id:"dependency-installation",level:3},{value:"Model Download",id:"model-download",level:3},{value:"Usage",id:"usage",level:2},{value:"RDK",id:"rdk-1",level:3},{value:"Result Analysis",id:"result-analysis",level:2}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"clip",children:"CLIP"})}),"\n","\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://github.com/openai/CLIP/",children:"CLIP"})," is a multimodal machine learning model proposed by OpenAI. This model uses contrastive learning on large-scale image-text pairs to process both images and text, mapping them into a shared vector space. This example demonstrates the functionality of using CLIP for image management and text query on the RDK platform."]}),"\n",(0,r.jsxs)(n.p,{children:["Code repository:  (",(0,r.jsx)(n.a,{href:"https://github.com/D-Robotics/hobot_clip.git",children:"https://github.com/D-Robotics/hobot_clip.git"}),")"]}),"\n",(0,r.jsx)(n.p,{children:"Application scenario: Using CLIP image feature extractor to manage images, usr text or image to query images, etc."}),"\n",(0,r.jsx)(n.h1,{id:"component",children:"Component"}),"\n",(0,r.jsx)(n.p,{children:"The project consists of four parts."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://github.com/D-Robotics/hobot_clip/tree/develop/clip_encode_image",children:"clip_encode_image"}),": an dnn node for the CLIP image encoder, currently supporting two modes:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Local mode: Supports input backpropagation, outputting text encoding features."}),"\n",(0,r.jsx)(n.li,{children:"Service mode: Based on ROS Action Server, supports client nodes sending inference requests and calculating the returned text encoding features."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://github.com/D-Robotics/hobot_clip/tree/develop/clip_encode_text",children:"clip_encode_text"}),": an dnn node for the CLIP text encoder, currently supporting two modes:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Local mode: Supports input backpropagation, outputting text encoding features."}),"\n",(0,r.jsx)(n.li,{children:"Service mode: Based on ROS Action Server, supports client nodes sending inference requests and calculating the returned text encoding features."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://github.com/D-Robotics/hobot_clip/tree/develop/clip_manage",children:"clip_manage"}),": CLIP relay node responsible for clienting and servicing. Currently, it supports two modes:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Storage mode: Send encoding requests to the image encoding node clip_encode_image, retrieve image encoding features from the target folder, and store the image encoding features in the local SQLite database."}),"\n",(0,r.jsx)(n.li,{children:"Query mode: Send an encoding request to the text encoding node clip_encode_text to obtain the encoding features of the target text. Next step, match the text features with image features in the database to obtain the matching results."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://github.com/D-Robotics/hobot_clip/tree/develop/clip_msgs",children:"clip_msgs"}),": CLIP app topic definition, action server control msg definition\u3002"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"supported-platforms",children:"Supported Platforms"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Platform"}),(0,r.jsx)(n.th,{children:"System"}),(0,r.jsx)(n.th,{children:"Function"})]})}),(0,r.jsx)(n.tbody,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"RDK X5"}),(0,r.jsx)(n.td,{children:"Ubuntu 22.04 (Humble)"}),(0,r.jsx)(n.td,{children:"Start CLIP Storage/Query mode, Storage database saved locally while query results display on the Web"})]})})]}),"\n",(0,r.jsx)(n.h2,{id:"preparation",children:"Preparation"}),"\n",(0,r.jsx)(n.h3,{id:"rdk",children:"RDK"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"The RDK has burned the  Ubuntu 22.04 system image provided by D-Robotics."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"The RDK has successfully installed TogetheROS.Bot."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"dependency-installation",children:"Dependency Installation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"pip3 install onnxruntime\npip3 install ftfy\npip3 install wcwidth\npip3 install regex\n"})}),"\n",(0,r.jsx)(n.h3,{id:"model-download",children:"Model Download"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"# Download the model file from the web.\nwget http://archive.d-robotics.cc/models/clip_encode_text/text_encoder.tar.gz\nsudo tar -xf text_encoder.tar.gz -C config\n"})}),"\n",(0,r.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,r.jsx)(n.h3,{id:"rdk-1",children:"RDK"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mode One: Storage"})}),"\n",(0,r.jsx)(n.p,{children:'Set clip_mode to "0" to store the image files from the "/root/config" directory into the "clip.db" database.'}),"\n",(0,r.jsx)(n.p,{children:'(Users can change the image folder path "clip_storage_folder" and the database name "clip_db_file" as needed. It is recommended to use absolute paths.)'}),"\n",(0,r.jsx)(l.A,{groupId:"tros-distro",children:(0,r.jsx)(a.A,{value:"humble",label:"Humble",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"# Configure the tros.b environment\nsource /opt/tros/humble/setup.bash\n\n# Copy the configuration file required for running the example from the installation path of tros.b.\ncp -r /opt/tros/${TROS_DISTRO}/lib/clip_encode_image/config/ .\n\n# Start the launch file\nros2 launch clip_manage hobot_clip_manage.launch.py clip_mode:=0 clip_db_file:=clip.db clip_storage_folder:=/root/config\n"})})})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mode Two: Query"})}),"\n",(0,r.jsx)(n.p,{children:'Set clip_mode to "1", set database path to "clip.db" and set query text "a diagram". the query result is saved in "result" folder.'}),"\n",(0,r.jsx)(n.p,{children:'(Users can change the database name "clip_db_file", query text "clip_text", and query result path "clip_result_folder" as needed. It is recommended to use absolute paths.)'}),"\n",(0,r.jsx)(l.A,{groupId:"tros-distro",children:(0,r.jsx)(a.A,{value:"humble",label:"Humble",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:'# Configure the tros.b environment\nsource /opt/tros/humble/setup.bash\n\n# Start the launch file\nros2 launch clip_manage hobot_clip_manage.launch.py clip_mode:=1 clip_db_file:=clip.db clip_result_folder:=result clip_text:="a diagram"\n'})})})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Display Query Result"})}),"\n",(0,r.jsx)(n.p,{children:"Run command in another terminal:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"cp -r /opt/tros/${TROS_DISTRO}/lib/clip_manage/config/index.html .\npython -m http.server 8080\n"})}),"\n",(0,r.jsx)(n.h2,{id:"result-analysis",children:"Result Analysis"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mode One: Storage"})}),"\n",(0,r.jsx)(n.p,{children:"The following information will be displayed in the terminal:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"[clip_manage-3] [WARN] [0000434374.492834334] [image_action_client]: Action client recved goal\n[clip_manage-3] [WARN] [0000434374.493161250] [image_action_client]: Action client got lock\n[clip_manage-3] [WARN] [0000434374.493402834] [image_action_client]: Sending goal, type: 1, urls size: 0\n[clip_encode_image-1] [WARN] [0000434374.494557250] [encode_image_server]: Received goal request with type: 1\n[clip_encode_image-1] [WARN] [0000434374.495408375] [encode_image_server]: Executing goal\n[clip_encode_image-1] [WARN] [0000434379.674204836] [ClipImageNode]: Sub img fps: 1.58, Smart fps: 1.58, preprocess time ms: 1422, infer time ms: 218, post process time ms: 0\n[clip_encode_image-1] [WARN] [0000434380.881684628] [ClipImageNode]: Sub img fps: 3.31, Smart fps: 3.31, preprocess time ms: 44, infer time ms: 216, post process time ms: 0\n[clip_encode_image-1] [WARN] [0000434380.882277045] [encode_image_server]: Goal complete, task_result: 1\n[clip_manage-3] [WARN] [0000434381.704573129] [image_action_client]: Get Result errorcode: 0\n[clip_manage-3] [WARN] [0000434381.704934504] [ClipNode]: Storage finish, current num of database: 7.\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mode Two: Query"})}),"\n",(0,r.jsx)(n.p,{children:"The following information will be displayed in the terminal:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"[clip_manage-3] [WARN] [0000435148.509009119] [ClipNode]: Query start, num of database: 7.\n[clip_manage-3] [WARN] [0000435148.509820786] [ClipNode]: Query finished! Cost 1 ms.\n[clip_encode_text_node-2] [WARN] [0000435148.514026703] [clip_encode_text_node]: Clip Encode Text Node work success.\n[clip_manage-3] [WARN] [0000435148.532558536] [ClipNode]: Query Result config/CLIP.png, similarity: 0.289350\n[clip_manage-3] [WARN] [0000435148.540040328] [ClipNode]: Query Result config/dog.jpg, similarity: 0.228837\n[clip_manage-3] [WARN] [0000435148.547667078] [ClipNode]: Query Result config/target_class.jpg, similarity: 0.224744\n[clip_manage-3] [WARN] [0000435148.555092286] [ClipNode]: Query Result config/target.jpg, similarity: 0.207572\n[clip_manage-3] [WARN] [0000435148.562450494] [ClipNode]: Query Result config/raw_unet.jpg, similarity: 0.198459\n[clip_manage-3] [WARN] [0000435148.569500536] [ClipNode]: Query Result config/people.jpg, similarity: 0.174074\n[clip_manage-3] [WARN] [0000435148.576885453] [ClipNode]: Query Result config/test.jpg, similarity: 0.174074\n[clip_manage-3] [WARN] [0000435148.584450703] [text_action_client]: Get Result errorcode: 0\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Display Query Result"})}),"\n",(0,r.jsxs)(n.p,{children:["Use Google Chrome or Edge and enter ",(0,r.jsx)(n.a,{href:"http://IP:8080",children:"http://IP:8080"})," to view the image retrieval results (where IP is the device's IP address)."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/03_boxs/function/image/box_adv/query_display.png",alt:""})}),"\n",(0,r.jsx)(n.p,{children:"Result Analysis: You can sequentially see the retrieval results based on the similarity between the query text and images. Among them, only the CLIP.png image is provided for this example, while the other images are from the user's actual configuration. Therefore, it is expected that only the first image in the visualization result will be the same as in the example."})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}}}]);