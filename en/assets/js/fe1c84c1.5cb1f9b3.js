"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[30376],{28453:(e,n,o)=>{o.d(n,{R:()=>s,x:()=>a});var i=o(96540);const t={},r=i.createContext(t);function s(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),i.createElement(r.Provider,{value:n},e.children)}},42120:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"Basic_Application/pydev_demo_sample/yolov3_sample","title":"3.3.3 YOLOv3 Model Example Introduction","description":"Example Overview","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/03_Basic_Application/03_pydev_demo_sample/06_yolov3_sample.md","sourceDirName":"03_Basic_Application/03_pydev_demo_sample","slug":"/Basic_Application/pydev_demo_sample/yolov3_sample","permalink":"/rdk_doc/en/Basic_Application/pydev_demo_sample/yolov3_sample","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1770017089000,"sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"3.3.2 Segment Model Example Introduction","permalink":"/rdk_doc/en/Basic_Application/pydev_demo_sample/segment_sample"},"next":{"title":"3.3.4 YOLOv5 Model Example Introduction","permalink":"/rdk_doc/en/Basic_Application/pydev_demo_sample/yolov5_sample"}}');var t=o(74848),r=o(28453);const s={sidebar_position:3},a="3.3.3 YOLOv3 Model Example Introduction",d={},l=[{value:"Example Overview",id:"example-overview",level:2},{value:"Effect Demonstration",id:"effect-demonstration",level:2},{value:"Hardware Preparation",id:"hardware-preparation",level:2},{value:"Hardware Connection",id:"hardware-connection",level:3},{value:"Quick Start",id:"quick-start",level:2},{value:"Code and Board Location",id:"code-and-board-location",level:3},{value:"Compilation and Execution",id:"compilation-and-execution",level:3},{value:"Execution Effect",id:"execution-effect",level:3},{value:"Detailed Introduction",id:"detailed-introduction",level:2},{value:"Example Program Parameter Options Description",id:"example-program-parameter-options-description",level:3},{value:"Software Architecture Description",id:"software-architecture-description",level:3},{value:"API Process Description",id:"api-process-description",level:3},{value:"FAQ",id:"faq",level:3}];function c(e){const n={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"333-yolov3-model-example-introduction",children:"3.3.3 YOLOv3 Model Example Introduction"})}),"\n",(0,t.jsx)(n.h2,{id:"example-overview",children:"Example Overview"}),"\n",(0,t.jsxs)(n.p,{children:["The YOLOv3 object detection example is a ",(0,t.jsx)(n.strong,{children:"Python interface"})," development code example located in ",(0,t.jsx)(n.code,{children:"/app/pydev_demo/06_yolov3_sample/"}),", demonstrating how to use the YOLOv3 model for object detection tasks. This example shows how to perform object detection on static images, identify multiple objects in the image, and draw detection boxes with confidence information on the image."]}),"\n",(0,t.jsx)(n.h2,{id:"effect-demonstration",children:"Effect Demonstration"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/03_pydev_demo_sample/image/pydev_06_runing.png",alt:"output-img"})}),"\n",(0,t.jsx)(n.h2,{id:"hardware-preparation",children:"Hardware Preparation"}),"\n",(0,t.jsx)(n.h3,{id:"hardware-connection",children:"Hardware Connection"}),"\n",(0,t.jsxs)(n.p,{children:["This example only requires the RDK development board itself, without additional peripheral connections. Ensure the development board is properly powered and the system is booted.\n",(0,t.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/03_pydev_demo_sample/image/pydev_06_hw_connect.png",alt:"connect-img"})]}),"\n",(0,t.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,t.jsx)(n.h3,{id:"code-and-board-location",children:"Code and Board Location"}),"\n",(0,t.jsxs)(n.p,{children:["Navigate to ",(0,t.jsx)(n.code,{children:"/app/pydev_demo/06_yolov3_sample/"})," location, where you can see the YOLOv3 example contains the following files:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"root@ubuntu:/app/pydev_demo/06_yolov3_sample# tree\n.\n\u251c\u2500\u2500 coco_classes.names\n\u251c\u2500\u2500 kite.jpg\n\u2514\u2500\u2500 test_yolov3.py\n\n"})}),"\n",(0,t.jsx)(n.h3,{id:"compilation-and-execution",children:"Compilation and Execution"}),"\n",(0,t.jsx)(n.p,{children:"The Python example does not require compilation and can be run directly:"}),"\n",(0,t.jsx)(n.h3,{id:"execution-effect",children:"Execution Effect"}),"\n",(0,t.jsx)(n.p,{children:"After running, the program will load the pre-trained YOLOv3 model, perform object detection on the kite.jpg image, and generate the result image output_image.jpg with detection boxes."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"root@ubuntu:/app/pydev_demo/06_yolov3_sample# ./test_yolov3.py \n[BPU_PLAT]BPU Platform Version(1.3.6)!\n[HBRT] set log level as 0. version = 3.15.55.0\n[DNN] Runtime version = 1.24.5_(3.15.55 HBRT)\n[A][DNN][packed_model.cpp:247][Model](2025-09-10,10:35:11.677.325) [HorizonRT] The model builder version = 1.23.5\n[W][DNN]bpu_model_info.cpp:491][Version](2025-09-10,10:35:12.46.71) Model: yolov3_vargdarknet_416x416_nv12. Inconsistency between the hbrt library version 3.15.55.0 and the model build version 3.15.47.0 detected, in order to ensure correct model results, it is recommended to use compilation tools and the BPU SDK from the same OpenExplorer package.\ntensor type: NV12\ndata type: uint8\nlayout: NCHW\nshape: (1, 3, 416, 416)\n3\ntensor type: int32\ndata type: int32\nlayout: NHWC\nshape: (1, 13, 13, 255)\ntensor type: int32\ndata type: int32\nlayout: NHWC\nshape: (1, 26, 26, 255)\ntensor type: int32\ndata type: int32\nlayout: NHWC\nshape: (1, 52, 52, 255)\ninferece time is : 0.020566940307617188\nchannel_valid: 255\nchannel_aligned: 256\nchannel_valid: 255\nchannel_aligned: 256\nchannel_valid: 255\nchannel_aligned: 256\npostprocess time is : 0.01925802230834961\nbbox: [112.40535, 612.55957, 167.20108, 763.162781], score: 0.988552, id: 0, name: person\nbbox: [590.504944, 80.579185, 668.850891, 152.585663], score: 0.979709, id: 33, name: kite\nbbox: [215.867859, 697.843567, 271.909821, 853.475281], score: 0.976957, id: 0, name: person\nbbox: [347.447327, 486.957092, 355.643707, 505.142273], score: 0.971156, id: 0, name: person\nbbox: [576.44989, 345.108185, 599.307068, 369.19931], score: 0.963799, id: 33, name: kite\nbbox: [278.754852, 236.495605, 305.968567, 280.169434], score: 0.939796, id: 33, name: kite\nbbox: [468.2388, 339.285095, 485.910553, 358.016907], score: 0.930308, id: 33, name: kite\nbbox: [178.86084, 540.466187, 192.142792, 572.972656], score: 0.896047, id: 0, name: person\nbbox: [304.221893, 375.879303, 326.426636, 399.683228], score: 0.889891, id: 33, name: kite\nbbox: [541.845886, 516.654236, 554.71283, 535.644592], score: 0.849118, id: 0, name: person\nbbox: [28.978374, 525.916199, 41.111099, 555.264893], score: 0.845627, id: 0, name: person\nbbox: [523.410095, 505.26712, 533.049866, 526.813965], score: 0.82792, id: 0, name: person\nbbox: [762.491577, 381.598389, 774.152283, 390.471924], score: 0.769005, id: 33, name: kite\nbbox: [528.595642, 516.481018, 540.382019, 531.763367], score: 0.690236, id: 0, name: person\nbbox: [34.753479, 512.438354, 51.458652, 552.005005], score: 0.651956, id: 0, name: person\nbbox: [1082.991089, 395.685669, 1099.050781, 423.359985], score: 0.617869, id: 33, name: kite\nbbox: [84.095276, 514.01886, 103.595062, 566.563904], score: 0.605346, id: 0, name: person\nbbox: [1206.786499, 451.33432, 1215.150146, 462.050873], score: 0.363324, id: 0, name: person\ndraw result time is : 0.0428011417388916\nroot@ubuntu:/app/pydev_demo/06_yolov3_sample# \n"})}),"\n",(0,t.jsx)(n.h2,{id:"detailed-introduction",children:"Detailed Introduction"}),"\n",(0,t.jsx)(n.h3,{id:"example-program-parameter-options-description",children:"Example Program Parameter Options Description"}),"\n",(0,t.jsx)(n.p,{children:"The YOLOv3 object detection example does not require command line parameters and can be run directly. The program will automatically load the kite.jpg image in the same directory for detection processing."}),"\n",(0,t.jsx)(n.h3,{id:"software-architecture-description",children:"Software Architecture Description"}),"\n",(0,t.jsx)(n.p,{children:"The software architecture of the YOLOv3 object detection example includes the following core components:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Model Loading: Using the pyeasy_dnn module to load the pre-trained YOLOv3 model"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Image Preprocessing: Converting input images to the required NV12 format and specified dimensions (416x416) for the model"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Model Inference: Calling the model for forward computation to generate feature maps"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Post-processing: Using the libpostprocess library to parse model outputs and generate detection results"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Result Visualization: Drawing detection bounding boxes and category information on the original image"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Result Saving: Saving the visualized results as image files"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)("center",{children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/03_pydev_demo_sample/image/pydev_06_yolov3_sample_software_arch.png",alt:"software_arch"})})}),"\n",(0,t.jsx)(n.h3,{id:"api-process-description",children:"API Process Description"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Model Loading: models = dnn.load('../models/yolov3_416x416_nv12.bin')"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Image Preprocessing: Adjusting image dimensions and converting to NV12 format"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Model Inference: outputs = models[0].forward(nv12_data)"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Post-processing Configuration: Setting post-processing parameters (dimensions, thresholds, etc.)"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Result Parsing: Calling the post-processing library to parse output tensors"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Result Visualization: Drawing detection boxes and label information on the original image"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Result Saving: Saving the results as an image file"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)("center",{children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"http://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/03_Basic_Application/03_pydev_demo_sample/image/pydev_06_yolov3_sample_api_flow.png",alt:"API_Flow"})})}),"\n",(0,t.jsx)(n.h3,{id:"faq",children:"FAQ"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Q:"})," What should I do if I encounter \"No module named 'hobot_dnn'\" error when running the example?",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"A:"})," Please ensure the RDK Python environment is correctly installed, including the hobot_dnn module and other official dedicated inference libraries."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Q:"})," How to change the test image?",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"A:"})," Place the new image file in the example directory and modify ",(0,t.jsx)(n.code,{children:"img_file = cv2.imread('your_image_path')"})," in the code."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Q:"})," What should I do if the detection results are inaccurate?",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"A:"})," The YOLOv3 model is trained on the COCO dataset and may require fine-tuning for specific scenarios or using more suitable models."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Q:"})," How to adjust the detection threshold?",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"A:"})," Modify the value of ",(0,t.jsx)(n.code,{children:"yolov3_postprocess_info.score_threshold"})," in the code. For example, changing it to 0.5 can improve detection sensitivity."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Q:"})," Can real-time video stream processing be achieved?",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"A:"})," The current example is designed for single images, but the code can be modified to achieve real-time object detection for video streams."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Q:"})," How to obtain other pre-trained YOLO models?",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"A:"})," You can refer to the ",(0,t.jsx)(n.a,{href:"https://github.com/D-Robotics/rdk_model_zoo",children:"model_zoo repository"})," or ",(0,t.jsx)(n.a,{href:"https://github.com/D-Robotics/hobot_model",children:"Toolchain's basic model repository"})]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}}}]);