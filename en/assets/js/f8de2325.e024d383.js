"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[56178],{11470:(e,t,n)=>{n.d(t,{A:()=>v});var r=n(96540),o=n(34164),a=n(23104),s=n(56347),l=n(205),i=n(57485),c=n(31682),d=n(70679);function u(e){return r.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function h(e){const{values:t,children:n}=e;return(0,r.useMemo)(()=>{const e=t??function(e){return u(e).map(({props:{value:e,label:t,attributes:n,default:r}})=>({value:e,label:t,attributes:n,default:r}))}(n);return function(e){const t=(0,c.XI)(e,(e,t)=>e.value===t.value);if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[t,n])}function p({value:e,tabValues:t}){return t.some(t=>t.value===e)}function m({queryString:e=!1,groupId:t}){const n=(0,s.W6)(),o=function({queryString:e=!1,groupId:t}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:e,groupId:t});return[(0,i.aZ)(o),(0,r.useCallback)(e=>{if(!o)return;const t=new URLSearchParams(n.location.search);t.set(o,e),n.replace({...n.location,search:t.toString()})},[o,n])]}function b(e){const{defaultValue:t,queryString:n=!1,groupId:o}=e,a=h(e),[s,i]=(0,r.useState)(()=>function({defaultValue:e,tabValues:t}){if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!p({value:e,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${t.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const n=t.find(e=>e.default)??t[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:t,tabValues:a})),[c,u]=m({queryString:n,groupId:o}),[b,g]=function({groupId:e}){const t=function(e){return e?`docusaurus.tab.${e}`:null}(e),[n,o]=(0,d.Dv)(t);return[n,(0,r.useCallback)(e=>{t&&o.set(e)},[t,o])]}({groupId:o}),f=(()=>{const e=c??b;return p({value:e,tabValues:a})?e:null})();(0,l.A)(()=>{f&&i(f)},[f]);return{selectedValue:s,selectValue:(0,r.useCallback)(e=>{if(!p({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);i(e),u(e),g(e)},[u,g,a]),tabValues:a}}var g=n(92303);const f={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var x=n(74848);function _({className:e,block:t,selectedValue:n,selectValue:r,tabValues:s}){const l=[],{blockElementScrollPositionUntilNextRender:i}=(0,a.a_)(),c=e=>{const t=e.currentTarget,o=l.indexOf(t),a=s[o].value;a!==n&&(i(t),r(a))},d=e=>{let t=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const n=l.indexOf(e.currentTarget)+1;t=l[n]??l[0];break}case"ArrowLeft":{const n=l.indexOf(e.currentTarget)-1;t=l[n]??l[l.length-1];break}}t?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.A)("tabs",{"tabs--block":t},e),children:s.map(({value:e,label:t,attributes:r})=>(0,x.jsx)("li",{role:"tab",tabIndex:n===e?0:-1,"aria-selected":n===e,ref:e=>{l.push(e)},onKeyDown:d,onClick:c,...r,className:(0,o.A)("tabs__item",f.tabItem,r?.className,{"tabs__item--active":n===e}),children:t??e},e))})}function y({lazy:e,children:t,selectedValue:n}){const a=(Array.isArray(t)?t:[t]).filter(Boolean);if(e){const e=a.find(e=>e.props.value===n);return e?(0,r.cloneElement)(e,{className:(0,o.A)("margin-top--md",e.props.className)}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:a.map((e,t)=>(0,r.cloneElement)(e,{key:t,hidden:e.props.value!==n}))})}function j(e){const t=b(e);return(0,x.jsxs)("div",{className:(0,o.A)("tabs-container",f.tabList),children:[(0,x.jsx)(_,{...t,...e}),(0,x.jsx)(y,{...t,...e})]})}function v(e){const t=(0,g.A)();return(0,x.jsx)(j,{...e,children:u(e.children)},String(t))}},19365:(e,t,n)=>{n.d(t,{A:()=>s});n(96540);var r=n(34164);const o={tabItem:"tabItem_Ymn6"};var a=n(74848);function s({children:e,hidden:t,className:n}){return(0,a.jsx)("div",{role:"tabpanel",className:(0,r.A)(o.tabItem,n),hidden:t,children:e})}},28453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>l});var r=n(96540);const o={},a=r.createContext(o);function s(e){const t=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),r.createElement(a.Provider,{value:t},e.children)}},28563:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>c,default:()=>p,frontMatter:()=>i,metadata:()=>r,toc:()=>u});const r=JSON.parse('{"id":"Robot_development/apps/car_tracking","title":"5.4.4 Robot Follows the Human Body","description":"Introduction","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/05_Robot_development/04_apps/car_tracking.md","sourceDirName":"05_Robot_development/04_apps","slug":"/Robot_development/apps/car_tracking","permalink":"/rdk_doc/en/Robot_development/apps/car_tracking","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1756732076000,"sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"5.4.3 Pose Detection","permalink":"/rdk_doc/en/Robot_development/apps/fall_detection"},"next":{"title":"5.4.5 Gesture Control The Car","permalink":"/rdk_doc/en/Robot_development/apps/car_gesture_control"}}');var o=n(74848),a=n(28453),s=n(11470),l=n(19365);const i={sidebar_position:4},c="5.4.4 Robot Follows the Human Body",d={},u=[{value:"Introduction",id:"introduction",level:2},{value:"Supported Platforms",id:"supported-platforms",level:2},{value:"Preparation",id:"preparation",level:2},{value:"RDK",id:"rdk",level:3},{value:"Usage",id:"usage",level:2},{value:"RDK",id:"rdk-1",level:3},{value:"Result Analysis",id:"result-analysis",level:2}];function h(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.header,{children:(0,o.jsx)(t.h1,{id:"544-robot-follows-the-human-body",children:"5.4.4 Robot Follows the Human Body"})}),"\n","\n",(0,o.jsx)(t.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(t.p,{children:"The app is used to control the robot to follow the movement of the human body. The app consists of MIPI image acquisition, body detection and tracking, body tracking strategy, image coding and decoding, and a web display interface. The workflow is shown in the following diagram:"}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/04_apps/image/car_tracking/body_tracking_workflow.jpg",alt:""})}),"\n",(0,o.jsx)(t.p,{children:"The app is demonstrated using a virtual car in the PC-side Gazebo simulation environment, but the control commands can also be directly used to control a physical robot."}),"\n",(0,o.jsxs)(t.p,{children:["Code Repository:  (",(0,o.jsx)(t.a,{href:"https://github.com/D-Robotics/body_tracking",children:"https://github.com/D-Robotics/body_tracking"}),")"]}),"\n",(0,o.jsx)(t.h2,{id:"supported-platforms",children:"Supported Platforms"}),"\n",(0,o.jsxs)(t.table,{children:[(0,o.jsx)(t.thead,{children:(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.th,{children:"Platform"}),(0,o.jsx)(t.th,{children:"System"}),(0,o.jsx)(t.th,{children:"Function"})]})}),(0,o.jsx)(t.tbody,{children:(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.td,{children:"RDK X3, RDK X3 Module, RDK X5"}),(0,o.jsx)(t.td,{children:"Ubuntu 20.04"}),(0,o.jsx)(t.td,{children:"Start MIPI/USB camera to capture images, perform body keypoints detection and body tracking, and display the tracking effect in Gazebo"})]})})]}),"\n",(0,o.jsx)(t.h2,{id:"preparation",children:"Preparation"}),"\n",(0,o.jsx)(t.h3,{id:"rdk",children:"RDK"}),"\n",(0,o.jsxs)(t.ol,{children:["\n",(0,o.jsxs)(t.li,{children:["\n",(0,o.jsx)(t.p,{children:"RDK has been flashed with the  Ubuntu 20.04/22.04 system image provided by D-Robotics."}),"\n"]}),"\n",(0,o.jsxs)(t.li,{children:["\n",(0,o.jsx)(t.p,{children:"TogetheROS.Bot has been successfully installed on RDK."}),"\n"]}),"\n",(0,o.jsxs)(t.li,{children:["\n",(0,o.jsx)(t.p,{children:"MIPI or USB camera has been installed on RDK."}),"\n"]}),"\n",(0,o.jsxs)(t.li,{children:["\n",(0,o.jsx)(t.p,{children:"The PC used for RDK should be in the same network segment (either wired or connected to the same wireless network, with the first three parts of the IP address being consistent). The PC should have the following environment installed:"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(s.A,{groupId:"tros-distro",children:[(0,o.jsxs)(l.A,{value:"foxy",label:"Foxy",children:[(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsxs)(t.li,{children:["Ubuntu 20.04 system and ",(0,o.jsx)(t.a,{href:"https://docs.ros.org/en/foxy/Installation/Ubuntu-Install-Debians.html",children:"ROS2 Foxy Desktop Full"})]}),"\n",(0,o.jsx)(t.li,{children:"Gazebo and Turtlebot3 related packages. Installation commands:"}),"\n"]}),(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-shell",children:"sudo apt-get install ros-foxy-gazebo-*\nsudo apt install ros-foxy-turtlebot3\nsudo apt install ros-foxy-turtlebot3-simulations\n"})})]}),(0,o.jsxs)(l.A,{value:"humble",label:"Humble",children:[(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsxs)(t.li,{children:["Ubuntu 22.04 system and ",(0,o.jsx)(t.a,{href:"https://docs.ros.org/en/humble/Installation/Ubuntu-Install-Debians.html",children:"ROS2 Humble Desktop Full"})]}),"\n",(0,o.jsx)(t.li,{children:"Gazebo and Turtlebot3 related packages. Installation commands:"}),"\n"]}),(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-shell",children:"sudo apt-get install ros-humble-gazebo-*\nsudo apt install ros-humble-turtlebot3\nsudo apt install ros-humble-turtlebot3-simulations\n"})})]})]}),"\n",(0,o.jsx)(t.h2,{id:"usage",children:"Usage"}),"\n",(0,o.jsx)(t.h3,{id:"rdk-1",children:"RDK"}),"\n",(0,o.jsxs)(t.p,{children:["After running the app, the car motion control package will select the human body closest to the front of the car (with the largest width of the body detection box) as the tracking target. When the human body is far from the car, the car starts to move forward to approach the body and keeps it in front of the car.After the app is launched, the sensor will publish images and corresponding algorithm results, which can be rendered and displayed on the PC browser. (Enter ",(0,o.jsx)(t.a,{href:"http://IP:8000",children:"http://IP:8000"})," in the browser, where IP is the IP address of the RDK)."]}),"\n",(0,o.jsx)(t.p,{children:"Launch the simulation environment on the PC side:"}),"\n",(0,o.jsxs)(s.A,{groupId:"tros-distro",children:[(0,o.jsx)(l.A,{value:"foxy",label:"Foxy",children:(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-shell",children:"source /opt/ros/foxy/setup.bash\n"})})}),(0,o.jsx)(l.A,{value:"humble",label:"Humble",children:(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-shell",children:"source /opt/ros/humble/setup.bash\n"})})})]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-shell",children:"export TURTLEBOT3_MODEL=burger\nros2 launch turtlebot3_gazebo empty_world.launch.py\n"})}),"\n",(0,o.jsx)(t.p,{children:"After successful launch, the car effect in the simulation environment is as follows:"}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/04_apps/image/car_tracking/gazebo.jpeg",alt:""})}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.strong,{children:"Publish images using MIPI camera"})}),"\n",(0,o.jsxs)(s.A,{groupId:"tros-distro",children:[(0,o.jsx)(l.A,{value:"foxy",label:"Foxy",children:(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-shell",children:"source /opt/tros/setup.bash\n\ncp -r /opt/tros/${TROS_DISTRO}/lib/mono2d_body_detection/config/ .\n\nexport CAM_TYPE=mipi\n\nros2 launch body_tracking body_tracking_without_gesture.launch.py\n"})})}),(0,o.jsx)(l.A,{value:"humble",label:"Humble",children:(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-shell",children:"source /opt/tros/humble/setup.bash\n\ncp -r /opt/tros/${TROS_DISTRO}/lib/mono2d_body_detection/config/ .\n\nexport CAM_TYPE=mipi\n\nros2 launch body_tracking body_tracking_without_gesture.launch.py\n"})})})]}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.strong,{children:"Publish images using USB camera"})}),"\n",(0,o.jsxs)(s.A,{groupId:"tros-distro",children:[(0,o.jsx)(l.A,{value:"foxy",label:"Foxy",children:(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-shell",children:"source /opt/tros/setup.bash\n\ncp -r /opt/tros/${TROS_DISTRO}/lib/mono2d_body_detection/config/ .\n\nexport CAM_TYPE=usb\n\nros2 launch body_tracking body_tracking_without_gesture.launch.py\n"})})}),(0,o.jsx)(l.A,{value:"humble",label:"Humble",children:(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-shell",children:"source /opt/tros/humble/setup.bash\n\ncp -r /opt/tros/${TROS_DISTRO}/lib/mono2d_body_detection/config/ .\n\nexport CAM_TYPE=usb\n\nros2 launch body_tracking body_tracking_without_gesture.launch.py\n"})})})]}),"\n",(0,o.jsx)(t.h2,{id:"result-analysis",children:"Result Analysis"}),"\n",(0,o.jsx)(t.p,{children:"The following information is outputted in the terminal when running on the RDK."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-text",children:"[body_tracking-7] [WARN] [1653430533.523069034] [ParametersClass]: TrackCfg param are\n[body_tracking-7] activate_wakeup_gesture: 0\n[body_tracking-7] track_serial_lost_num_thr: 100\n[body_tracking-7] activate_robot_rotate_thr: 45\n[body_tracking-7] activate_robot_move_thr: 5\n[body_tracking-7] move_step: 0.3\n[body_tracking-7] rotate_step: 0.5\n[body_tracking-7] img_width: 960\n[body_tracking-7] img_height: 544\n[body_tracking-7] \n[body_tracking-7] [WARN] [1653430533.712812076] [TrackingManager]: update frame_ts 395787, 873\n[body_tracking-7] [WARN] [1653430533.713105576] [TrackingManager]: Tracking body start!, track_id: 1, frame_ts: 395787, tracking_sta(0:INITING, 1:TRACKING, 2:LOST): 1, gesture: 0\n[body_tracking-7] [WARN] [1653430535.018442618] [TrackingManager]: Do move! body_rect_width: 353, thr: 864, move_step_ratio: 1, body_rect_to_top: 20, img_height: 544, move_step: 0.3\n[body_tracking-7] [WARN] [1653430535.220268535] [TrackingManager]: Do rotate move, ts sec: 3397, nanosec: 387800000\n[body_tracking-7] [WARN] [1653430535.220408576] [RobotCmdVelNode]: RobotCtl, angular: 0 0 0, linear: 0.3 0 0, pub twist ts: 1653430535220394\n"})}),"\n",(0,o.jsx)(t.p,{children:"The above log captures a section of the output after the app is launched. Print the relevant configuration (TrackCfg param) first after startup. After detecting the human body, the car starts to enter a following state (tracking_sta value is 1) and moves forward at a speed of 0.3m/s (RobotCtl, angular: 0 0 0, linear: 0.3 0 0) to approach the human body."}),"\n",(0,o.jsxs)(t.p,{children:["Use the command ",(0,o.jsx)(t.code,{children:"ros2 topic list"}),"on PC,the topic is as below\uff1a"]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-shell",children:"$ ros2 topic list\n/camera_info\n/cmd_vel\n/hbmem_img04054242060426080500012020112713\n/hobot_mono2d_body_detection\n/image\n/parameter_events\n/rosout\n"})}),"\n",(0,o.jsxs)(t.p,{children:["Among them, ",(0,o.jsx)(t.code,{children:"/image"})," is the image captured by the RDK from the MIPI sensor and encoded in JPEG format, ",(0,o.jsx)(t.code,{children:"/hobot_mono2d_body_detection"})," is the algorithm message published by the RDK which contains the human body detection results, and ",(0,o.jsx)(t.code,{children:"/cmd_vel"})," is the motion control command published by the RDK."]}),"\n",(0,o.jsxs)(t.p,{children:["On the PC, using the ",(0,o.jsx)(t.code,{children:"ros2 topic echo /cmd_vel"})," command on the terminal can view the motion control commands issued by RDK:"]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-shell",children:"linear:\n  x: 0.5\n  y: 0.0\n  z: 0.0\nangular:\n  x: 0.0\n  y: 0.0\n  z: -0.5\n---\nlinear:x: 0.5\ny: 0.0\nz: 0.0\nangular:\nx: 0.0\ny: 0.0\nz: -0.5\n---\n"})}),"\n",(0,o.jsx)(t.p,{children:"In the PC simulation environment, the car follows the movement of the human body. The simulated car movement effect is as follows:"}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/04_apps/image/car_tracking/tracking.gif",alt:""})})]})}function p(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}}}]);