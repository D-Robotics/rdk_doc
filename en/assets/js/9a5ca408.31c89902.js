"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[45582],{11470:(e,t,n)=>{n.d(t,{A:()=>y});var s=n(96540),a=n(34164),o=n(23104),r=n(56347),i=n(205),l=n(57485),c=n(31682),d=n(70679);function u(e){return s.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,s.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function h(e){const{values:t,children:n}=e;return(0,s.useMemo)(()=>{const e=t??function(e){return u(e).map(({props:{value:e,label:t,attributes:n,default:s}})=>({value:e,label:t,attributes:n,default:s}))}(n);return function(e){const t=(0,c.XI)(e,(e,t)=>e.value===t.value);if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[t,n])}function p({value:e,tabValues:t}){return t.some(t=>t.value===e)}function m({queryString:e=!1,groupId:t}){const n=(0,r.W6)(),a=function({queryString:e=!1,groupId:t}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:e,groupId:t});return[(0,l.aZ)(a),(0,s.useCallback)(e=>{if(!a)return;const t=new URLSearchParams(n.location.search);t.set(a,e),n.replace({...n.location,search:t.toString()})},[a,n])]}function b(e){const{defaultValue:t,queryString:n=!1,groupId:a}=e,o=h(e),[r,l]=(0,s.useState)(()=>function({defaultValue:e,tabValues:t}){if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!p({value:e,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${t.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const n=t.find(e=>e.default)??t[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:t,tabValues:o})),[c,u]=m({queryString:n,groupId:a}),[b,g]=function({groupId:e}){const t=function(e){return e?`docusaurus.tab.${e}`:null}(e),[n,a]=(0,d.Dv)(t);return[n,(0,s.useCallback)(e=>{t&&a.set(e)},[t,a])]}({groupId:a}),f=(()=>{const e=c??b;return p({value:e,tabValues:o})?e:null})();(0,i.A)(()=>{f&&l(f)},[f]);return{selectedValue:r,selectValue:(0,s.useCallback)(e=>{if(!p({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);l(e),u(e),g(e)},[u,g,o]),tabValues:o}}var g=n(92303);const f={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var x=n(74848);function v({className:e,block:t,selectedValue:n,selectValue:s,tabValues:r}){const i=[],{blockElementScrollPositionUntilNextRender:l}=(0,o.a_)(),c=e=>{const t=e.currentTarget,a=i.indexOf(t),o=r[a].value;o!==n&&(l(t),s(o))},d=e=>{let t=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const n=i.indexOf(e.currentTarget)+1;t=i[n]??i[0];break}case"ArrowLeft":{const n=i.indexOf(e.currentTarget)-1;t=i[n]??i[i.length-1];break}}t?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":t},e),children:r.map(({value:e,label:t,attributes:s})=>(0,x.jsx)("li",{role:"tab",tabIndex:n===e?0:-1,"aria-selected":n===e,ref:e=>{i.push(e)},onKeyDown:d,onClick:c,...s,className:(0,a.A)("tabs__item",f.tabItem,s?.className,{"tabs__item--active":n===e}),children:t??e},e))})}function j({lazy:e,children:t,selectedValue:n}){const o=(Array.isArray(t)?t:[t]).filter(Boolean);if(e){const e=o.find(e=>e.props.value===n);return e?(0,s.cloneElement)(e,{className:(0,a.A)("margin-top--md",e.props.className)}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:o.map((e,t)=>(0,s.cloneElement)(e,{key:t,hidden:e.props.value!==n}))})}function _(e){const t=b(e);return(0,x.jsxs)("div",{className:(0,a.A)("tabs-container",f.tabList),children:[(0,x.jsx)(v,{...t,...e}),(0,x.jsx)(j,{...t,...e})]})}function y(e){const t=(0,g.A)();return(0,x.jsx)(_,{...e,children:u(e.children)},String(t))}},19365:(e,t,n)=>{n.d(t,{A:()=>r});n(96540);var s=n(34164);const a={tabItem:"tabItem_Ymn6"};var o=n(74848);function r({children:e,hidden:t,className:n}){return(0,o.jsx)("div",{role:"tabpanel",className:(0,s.A)(a.tabItem,n),hidden:t,children:e})}},28453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>i});var s=n(96540);const a={},o=s.createContext(a);function r(e){const t=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(o.Provider,{value:t},e.children)}},71183:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>c,default:()=>p,frontMatter:()=>l,metadata:()=>s,toc:()=>u});const s=JSON.parse('{"id":"Robot_development/boxs/spatial/orb_slam3","title":"Visual SLAM","description":"Introduction","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/05_Robot_development/03_boxs/spatial/orb_slam3.md","sourceDirName":"05_Robot_development/03_boxs/spatial","slug":"/Robot_development/boxs/spatial/orb_slam3","permalink":"/rdk_doc/en/Robot_development/boxs/spatial/orb_slam3","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1756900553000,"sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Monocular 3D Indoor Detection","permalink":"/rdk_doc/en/Robot_development/boxs/spatial/mono3d_indoor_detection"},"next":{"title":"Visual Inertial Odometry Algorithm","permalink":"/rdk_doc/en/Robot_development/boxs/spatial/hobot_vio"}}');var a=n(74848),o=n(28453),r=n(11470),i=n(19365);const l={sidebar_position:3},c="Visual SLAM",d={},u=[{value:"Introduction",id:"introduction",level:2},{value:"Supported Platforms",id:"supported-platforms",level:2},{value:"Preparation",id:"preparation",level:2},{value:"Usage",id:"usage",level:2},{value:"Use EuRoC Dataset",id:"use-euroc-dataset",level:3},{value:"Use RealSense D435i Camera",id:"use-realsense-d435i-camera",level:3},{value:"Using SuperPoint optimized ORB-SLAM3",id:"using-superpoint-optimized-orb-slam3",level:3}];function h(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"visual-slam",children:"Visual SLAM"})}),"\n","\n",(0,a.jsx)(t.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(t.p,{children:"SLAM stands for Simultaneous Localization and Mapping. ORB-SLAM3 is one of the most researched algorithms in this field. TogetheROS.Bot integrates, improves, and optimizes ORB-SLAM3 to facilitate the development of visual SLAM-based applications."}),"\n",(0,a.jsxs)(t.ol,{children:["\n",(0,a.jsx)(t.li,{children:"Integrated and adapted the SuperPoint feature extraction model to optimize the robustness of image feature extraction in visual SLAM's frontend, and reduce CPU workload.\nThe model is converted to a fixed-point model runnable on the RDK using the D-Robotics floating-point model conversion tool, thus reducing the CPU workload of RDK."}),"\n",(0,a.jsx)(t.li,{children:"Wrapped the point cloud and pose information publishing, as well as image and IMU subscribing of ORB-SLAM3 with ROS2."}),"\n",(0,a.jsx)(t.li,{children:"Added Track asynchronous interface to separate feature extraction and feature point tracking into different threads, improving processing frame rate and facilitating practical engineering applications."}),"\n",(0,a.jsx)(t.li,{children:"Added bag-of-words library creation program to help developers build their own bag-of-words libraries."}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"In this chapter, ORB-SLAM3 is used as the mapping algorithm, and the EuRoC open dataset and RealSense D435i camera are used as the data sources for testing."}),"\n",(0,a.jsxs)(t.p,{children:["Code repository: (",(0,a.jsx)(t.a,{href:"https://github.com/D-Robotics/orb_slam3",children:"https://github.com/D-Robotics/orb_slam3"}),")"]}),"\n",(0,a.jsx)(t.p,{children:"Application scenarios: Visual SLAM algorithm can calculate the three-dimensional structure of the environment while computing its own position and orientation, enabling real-time localization and map construction. It is mainly used in fields such as autonomous driving, smart homes, and 3D reconstruction."}),"\n",(0,a.jsxs)(t.p,{children:["SLAM Mapping Example: ",(0,a.jsx)(t.a,{href:"../../apps/slam",children:"SLAM Mapping"})]}),"\n",(0,a.jsx)(t.h2,{id:"supported-platforms",children:"Supported Platforms"}),"\n",(0,a.jsxs)(t.table,{children:[(0,a.jsx)(t.thead,{children:(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.th,{children:"Platform"}),(0,a.jsx)(t.th,{children:"System"})]})}),(0,a.jsx)(t.tbody,{children:(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"RDK X3, RDK X3 Module, RDK X5"}),(0,a.jsx)(t.td,{children:"Ubuntu 20.04 (Foxy), Ubuntu 22.04 (Humble)"})]})})]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Note"}),": SuperPoint optimization only supports RDK X3, RDK X3 Module, RDK X5 platforms."]}),"\n",(0,a.jsx)(t.h2,{id:"preparation",children:"Preparation"}),"\n",(0,a.jsxs)(t.ol,{children:["\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"RDK is flashed with the provided  Ubuntu 20.04/22.04 system image."}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"TogetheROS.Bot is successfully installed on RDK."}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"ORB-SLAM3 algorithm package is successfully installed on RDK using the command:"}),"\n",(0,a.jsxs)(r.A,{groupId:"tros-distro",children:[(0,a.jsx)(i.A,{value:"foxy",label:"Foxy",children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"sudo apt update\nsudo apt install tros-orb-slam3 tros-orb-slam3-example-ros2\n"})})}),(0,a.jsx)(i.A,{value:"humble",label:"Humble",children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"sudo apt update\nsudo apt install tros-humble-orb-slam3 tros-humble-orb-slam3-example-ros2\n"})})})]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"RealSense D435i camera is installed on RDK."}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"EuRoC open dataset."}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"A PC on the same network segment as RDK, with Ubuntu 20.04, ROS2 Foxy Desktop edition, and data visualization tool Rviz2 installed."}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"usage",children:"Usage"}),"\n",(0,a.jsx)(t.p,{children:"ORB-SLAM3 project itself integrates various types of test programs, such as mono/stereo and mono/stereo+IMU, and also classifies them according to different benchmark datasets and sensors."}),"\n",(0,a.jsx)(t.h3,{id:"use-euroc-dataset",children:"Use EuRoC Dataset"}),"\n",(0,a.jsxs)(t.p,{children:["Dataset URL: (",(0,a.jsx)(t.a,{href:"http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/vicon_room2/V2_01_easy/V2_01_easy.zip",children:"http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/vicon_room2/V2_01_easy/V2_01_easy.zip"}),"). After downloading the dataset, go to the ORB-SLAM3 project directory. Extract the dataset and the bag-of-words library to your local machine, and run the test program. If you want to achieve a higher frame rate, you can overclock the RDK CPU, but this will also increase power consumption."]}),"\n",(0,a.jsx)(t.p,{children:"Command to run:"}),"\n",(0,a.jsxs)(r.A,{groupId:"tros-distro",children:[(0,a.jsx)(i.A,{value:"foxy",label:"Foxy",children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"source /opt/tros/setup.bash\n"})})}),(0,a.jsx)(i.A,{value:"humble",label:"Humble",children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"source /opt/tros/humble/setup.bash\n"})})})]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"# Overclock X3 CPU to 1.5GHz\nsudo bash -c 'echo 1 > /sys/devices/system/cpu/cpufreq/boost'\n# Enable X3 CPU performance mode\nsudo bash -c 'echo performance > /sys/devices/system/cpu/cpufreq/policy0/scaling_governor'\n# Go to the ORB_SLAM3 project directory\ncd /opt/tros/${TROS_DISTRO}/share/orb_slam3\n# Unzip the dataset - V2_01_easy.zip needs to be downloaded separately!\nunzip V2_01_easy.zip -d V2_01_easy\n# Extract the bag-of-words library\ntar -xvf ./Vocabulary/ORBvoc.txt.tar.gz\n# Grant execution permission to the program\nsudo chmod +x ./Examples/Monocular/mono_euroc\n# Run the program - V2_01_easy directory is the directory of the EuRoC open dataset downloaded from the internet, developers need to download it themselves!\n./Examples/Monocular/mono_euroc ./ORBvoc_refine.txt ./Examples/Monocular/EuRoC.yaml ./V2_01_easy/ ./Examples/Monocular/EuRoC_TimeStamps/V201.txt\n"})}),"\n",(0,a.jsxs)(t.p,{children:["The program will take some time to load the bag-of-words library. After a while, the program will print the current frame rate.\n",(0,a.jsx)(t.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/03_boxs/function/image/box_adv/euroc_result.png",alt:""})]}),"\n",(0,a.jsx)(t.h3,{id:"use-realsense-d435i-camera",children:"Use RealSense D435i Camera"}),"\n",(0,a.jsx)(t.p,{children:"The tros.b has developed a set of sample programs based on ORB-SLAM3 and ROS2, which integrate image and IMU data subscriptions and publish map point clouds, poses, and travel trajectories as topics. It is convenient to observe the running result of the program through the visualization software Rviz2, which helps developers in ROS2 development and debugging of ORB-SLAM3."}),"\n",(0,a.jsxs)(t.p,{children:["The latest version of the image has applied UVC and HID driver patches for the RealSense series camera to the kernel. After installing the RealSense SDK and ROS2 package using the apt command, you can directly use the test program. The installation method for ROS2 package coexisting with tros.b can be found in ",(0,a.jsx)(t.a,{href:"/rdk_doc/en/Robot_development/quick_start/install_tros",children:"Using ROS2 package"})]}),"\n",(0,a.jsxs)(r.A,{groupId:"tros-distro",children:[(0,a.jsx)(i.A,{value:"foxy",label:"Foxy",children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"# Configure the tros.b environment\nsource /opt/tros/setup.bash\n"})})}),(0,a.jsx)(i.A,{value:"humble",label:"Humble",children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"# Configure the tros.b environment\nsource /opt/tros/humble/setup.bash\n"})})})]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"echo $ROS_DISTRO \nsudo apt-get install ros-$ROS_DISTRO-librealsense2* -y \nsudo apt-get install ros-$ROS_DISTRO-realsense2-camera -y\nsudo apt-get install ros-$ROS_DISTRO-realsense2-description -y\n"})}),"\n",(0,a.jsx)(t.p,{children:"After installation, we start the Realsense camera as an image publishing node and the visual SLAM node as an image subscriber. It subscribes to the image topic and publishes pose and point cloud information."}),"\n",(0,a.jsx)(t.p,{children:"Next, we log in to RDK using the root account (password: root) and start the Realsense D435i camera. Otherwise, insufficient permissions will prevent the camera from starting correctly."}),"\n",(0,a.jsxs)(r.A,{groupId:"tros-distro",children:[(0,a.jsx)(i.A,{value:"foxy",label:"Foxy",children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"# Configure the tros.b environment\nsource /opt/tros/setup.bash\n"})})}),(0,a.jsx)(i.A,{value:"humble",label:"Humble",children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"# Configure the tros.b environment\nsource /opt/tros/humble/setup.bash\n"})})})]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"ros2 launch realsense2_camera rs_launch.py enable_depth:=false enable_color:=false enable_infra1:=true depth_module.profile:=640x480x15 \n"})}),"\n",(0,a.jsxs)(t.p,{children:["After the camera is started, you can observe the following logs from the console:\n",(0,a.jsx)(t.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/03_boxs/function/image/box_adv/realsense.png",alt:""})]}),"\n",(0,a.jsx)(t.p,{children:"Next, we start the visual SLAM node:"}),"\n",(0,a.jsxs)(r.A,{groupId:"tros-distro",children:[(0,a.jsx)(i.A,{value:"foxy",label:"Foxy",children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"source /opt/tros/setup.bash\n"})})}),(0,a.jsx)(i.A,{value:"humble",label:"Humble",children:(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"source /opt/tros/humble/setup.bash\n"})})})]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"# Overclock X3 CPU to 1.5GHz\nsudo bash -c 'echo 1 > /sys/devices/system/cpu/cpufreq/boost'\n# Enable X3 CPU performance mode\nsudo bash -c 'echo performance > /sys/devices/system/cpu/cpufreq/policy0/scaling_governor'\n# Enter working directory\ncd /opt/tros/${TROS_DISTRO}/share/orb_slam3\n# Unzip the bag-of-words library\ntar -xvf ./Vocabulary/ORBvoc.txt.tar.gz\n# Start ORB-SLAM3 monocular processing node\nros2 run orb_slam3_example_ros2 mono ./ORBvoc.txt ./Examples/Monocular/RealSense_D435i.yaml \n"})}),"\n",(0,a.jsx)(t.p,{children:'The visual SLAM node on the RDK starts and receives camera image data, and then starts printing the current frame rate "fps".'}),"\n",(0,a.jsx)(t.p,{children:"At the same time, open the Rviz2 visualization software on the PC (in the same network segment as the tros.b), add relevant visualization information, and subscribe to the following topics:"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/03_boxs/function/image/box_adv/rviz2_1.png",alt:""})}),"\n",(0,a.jsxs)(t.p,{children:["After subscribing to the topics, you can observe the rendering results of the feature points in the RVIZ2 software, and also observe the generated white map point cloud and green camera trajectory information on the right side of the window as the camera moves.\n",(0,a.jsx)(t.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/03_boxs/function/image/box_adv/rviz2_2.png",alt:""})]}),"\n",(0,a.jsx)(t.h3,{id:"using-superpoint-optimized-orb-slam3",children:"Using SuperPoint optimized ORB-SLAM3"}),"\n",(0,a.jsx)(t.p,{children:"As we all know, deep learning methods have shown amazing advantages and potential compared to traditional algorithms, especially in terms of stability, efficiency, and accuracy in detection and classification tasks. In the field of visual SLAM, many works have emerged that use deep learning methods to replace traditional SLAM front-ends and back-ends, and have demonstrated significant advantages."}),"\n",(0,a.jsxs)(t.p,{children:["SuperPoint and SuperGlue are examples of such methods. SuperPoint is a self-supervised deep learning network model that can extract both the position and descriptor of image feature points. TROS.b integrates SuperPoint with ORB-SLAM3, and developers can freely switch between feature point extraction methods in the configuration files located in ",(0,a.jsx)(t.code,{children:"/opt/tros/share/orb_slam3/Examples/\\*/*.yaml"}),'. As shown in the figure below, the feature point extraction algorithm used is "SUPERPOINT":']}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/03_boxs/function/image/box_adv/superpoint.png",alt:""})}),"\n",(0,a.jsx)(t.p,{children:"The result of using the SuperPoint feature extraction algorithm is shown in the following figure. It can be seen that the feature points are extracted very densely and the contours of objects are detected."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/03_boxs/function/image/box_adv/superpoint_result.png",alt:""})})]})}function p(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}}}]);