"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[90832],{11470:(e,n,o)=>{o.d(n,{A:()=>v});var t=o(96540),r=o(34164),i=o(23104),s=o(56347),a=o(205),d=o(57485),l=o(31682),c=o(70679);function u(e){return t.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,t.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function h(e){const{values:n,children:o}=e;return(0,t.useMemo)(()=>{const e=n??function(e){return u(e).map(({props:{value:e,label:n,attributes:o,default:t}})=>({value:e,label:n,attributes:o,default:t}))}(o);return function(e){const n=(0,l.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,o])}function m({value:e,tabValues:n}){return n.some(n=>n.value===e)}function p({queryString:e=!1,groupId:n}){const o=(0,s.W6)(),r=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,d.aZ)(r),(0,t.useCallback)(e=>{if(!r)return;const n=new URLSearchParams(o.location.search);n.set(r,e),o.replace({...o.location,search:n.toString()})},[r,o])]}function b(e){const{defaultValue:n,queryString:o=!1,groupId:r}=e,i=h(e),[s,d]=(0,t.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const o=n.find(e=>e.default)??n[0];if(!o)throw new Error("Unexpected error: 0 tabValues");return o.value}({defaultValue:n,tabValues:i})),[l,u]=p({queryString:o,groupId:r}),[b,_]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[o,r]=(0,c.Dv)(n);return[o,(0,t.useCallback)(e=>{n&&r.set(e)},[n,r])]}({groupId:r}),f=(()=>{const e=l??b;return m({value:e,tabValues:i})?e:null})();(0,a.A)(()=>{f&&d(f)},[f]);return{selectedValue:s,selectValue:(0,t.useCallback)(e=>{if(!m({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);d(e),u(e),_(e)},[u,_,i]),tabValues:i}}var _=o(92303);const f={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var g=o(74848);function x({className:e,block:n,selectedValue:o,selectValue:t,tabValues:s}){const a=[],{blockElementScrollPositionUntilNextRender:d}=(0,i.a_)(),l=e=>{const n=e.currentTarget,r=a.indexOf(n),i=s[r].value;i!==o&&(d(n),t(i))},c=e=>{let n=null;switch(e.key){case"Enter":l(e);break;case"ArrowRight":{const o=a.indexOf(e.currentTarget)+1;n=a[o]??a[0];break}case"ArrowLeft":{const o=a.indexOf(e.currentTarget)-1;n=a[o]??a[a.length-1];break}}n?.focus()};return(0,g.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":n},e),children:s.map(({value:e,label:n,attributes:t})=>(0,g.jsx)("li",{role:"tab",tabIndex:o===e?0:-1,"aria-selected":o===e,ref:e=>{a.push(e)},onKeyDown:c,onClick:l,...t,className:(0,r.A)("tabs__item",f.tabItem,t?.className,{"tabs__item--active":o===e}),children:n??e},e))})}function y({lazy:e,children:n,selectedValue:o}){const i=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=i.find(e=>e.props.value===o);return e?(0,t.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,g.jsx)("div",{className:"margin-top--md",children:i.map((e,n)=>(0,t.cloneElement)(e,{key:n,hidden:e.props.value!==o}))})}function j(e){const n=b(e);return(0,g.jsxs)("div",{className:(0,r.A)("tabs-container",f.tabList),children:[(0,g.jsx)(x,{...n,...e}),(0,g.jsx)(y,{...n,...e})]})}function v(e){const n=(0,_.A)();return(0,g.jsx)(j,{...e,children:u(e.children)},String(n))}},19365:(e,n,o)=>{o.d(n,{A:()=>s});o(96540);var t=o(34164);const r={tabItem:"tabItem_Ymn6"};var i=o(74848);function s({children:e,hidden:n,className:o}){return(0,i.jsx)("div",{role:"tabpanel",className:(0,t.A)(r.tabItem,o),hidden:n,children:e})}},28453:(e,n,o)=>{o.d(n,{R:()=>s,x:()=>a});var t=o(96540);const r={},i=t.createContext(r);function s(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(i.Provider,{value:n},e.children)}},75139:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>m,frontMatter:()=>d,metadata:()=>t,toc:()=>u});const t=JSON.parse('{"id":"Robot_development/boxs/body/mono2d_body_detection","title":"Human Detection and Tracking","description":"Introduction","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/05_Robot_development/03_boxs/body/mono2d_body_detection.md","sourceDirName":"05_Robot_development/03_boxs/body","slug":"/Robot_development/boxs/body/mono2d_body_detection","permalink":"/rdk_doc/en/Robot_development/boxs/body/mono2d_body_detection","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1762784650000,"sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"EdgeSAM Segment Anything","permalink":"/rdk_doc/en/Robot_development/boxs/segmentation/mono_edgesam"},"next":{"title":"Hand Keypoint Detection","permalink":"/rdk_doc/en/Robot_development/boxs/body/hand_lmk_detection"}}');var r=o(74848),i=o(28453),s=o(11470),a=o(19365);const d={sidebar_position:1},l="Human Detection and Tracking",c={},u=[{value:"Introduction",id:"introduction",level:2},{value:"Supported Platforms",id:"supported-platforms",level:2},{value:"Preparation",id:"preparation",level:2},{value:"RDK",id:"rdk",level:3},{value:"Usage",id:"usage",level:2},{value:"RDK Platform",id:"rdk-platform",level:3},{value:"Result analysis",id:"result-analysis",level:2}];function h(e){const n={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"human-detection-and-tracking",children:"Human Detection and Tracking"})}),"\n","\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsx)(n.p,{children:"The human detection and tracking algorithm example subscribes to images and utilizes BPU for inference. It publishes messages containing detection results for human bodies, heads, faces, hand boxes, and body keypoints, and achieves tracking of detection boxes through the multi-target tracking (MOT)."}),"\n",(0,r.jsx)(n.p,{children:"The supported detection categories and their corresponding data types in the algorithm are as follows:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Category"}),(0,r.jsx)(n.th,{children:"Description"}),(0,r.jsx)(n.th,{children:"Data Type"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"body"}),(0,r.jsx)(n.td,{children:"body box"}),(0,r.jsx)(n.td,{children:"Roi"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"head"}),(0,r.jsx)(n.td,{children:"head box"}),(0,r.jsx)(n.td,{children:"Roi"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"face"}),(0,r.jsx)(n.td,{children:"face box"}),(0,r.jsx)(n.td,{children:"Roi"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"hand"}),(0,r.jsx)(n.td,{children:"hand box"}),(0,r.jsx)(n.td,{children:"Roi"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"body_kps"}),(0,r.jsx)(n.td,{children:"body keypoints"}),(0,r.jsx)(n.td,{children:"Point"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"The index of the body keypoints algorithm result is shown in the following figure:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/03_boxs/function/image/box_adv/kps_index.jpeg",alt:""})}),"\n",(0,r.jsxs)(n.p,{children:["Code Repository:  (",(0,r.jsx)(n.a,{href:"https://github.com/D-Robotics/mono2d_body_detection",children:"https://github.com/D-Robotics/mono2d_body_detection"}),")"]}),"\n",(0,r.jsx)(n.p,{children:"Application Scenarios: Human detection and tracking algorithms are an important component of human motion visual analysis, which can achieve functions such as human pose analysis and people counting. They are mainly applied in fields like human-computer interaction and gaming entertainment."}),"\n",(0,r.jsxs)(n.p,{children:["Pose Detection Example: ",(0,r.jsx)(n.a,{href:"../../apps/fall_detection",children:"Pose Detection"}),(0,r.jsx)(n.br,{}),"\n","Human Tracking for Car Example: ",(0,r.jsx)(n.a,{href:"../../apps/car_tracking",children:"Car Tracking"})]}),"\n",(0,r.jsx)(n.h2,{id:"supported-platforms",children:"Supported Platforms"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Platform"}),(0,r.jsx)(n.th,{children:"System"}),(0,r.jsx)(n.th,{children:"Function"})]})}),(0,r.jsx)(n.tbody,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"RDK X3, RDK X3 Module, RDK X5"}),(0,r.jsx)(n.td,{children:"Ubuntu 20.04 (Foxy), Ubuntu 22.04 (Humble)"}),(0,r.jsx)(n.td,{children:"Start MIPI/USB camera/local video and display inference rendering results via web"})]})})]}),"\n",(0,r.jsx)(n.h2,{id:"preparation",children:"Preparation"}),"\n",(0,r.jsx)(n.h3,{id:"rdk",children:"RDK"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"RDK has flashed the  Ubuntu 20.04/22.04 system image provided by D-Robotics."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"RDK has successfully installed TogetheROS.Bot."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"RDK has installed the MIPI or USB camera."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Confirm that the PC is able to access the RDK via the network."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,r.jsx)(n.p,{children:"The mono2d_body_detection package for human detection and tracking subscribes to images published by the sensor package, performs inference, and publishes algorithm messages. The websocket package is used to render and display the images and corresponding algorithm results on a PC browser."}),"\n",(0,r.jsx)(n.h3,{id:"rdk-platform",children:"RDK Platform"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Use MIPI Camera to Publish Images"})}),"\n",(0,r.jsxs)(s.A,{groupId:"tros-distro",children:[(0,r.jsx)(a.A,{value:"foxy",label:"Foxy",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Configure the tros.b environment\nsource /opt/tros/setup.bash\n"})})}),(0,r.jsx)(a.A,{value:"humble",label:"Humble",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Configure the tros.b environment\nsource /opt/tros/humble/setup.bash\n"})})})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"# Copy the configuration file required for running the example from the installation path of tros.b.\ncp -r /opt/tros/${TROS_DISTRO}/lib/mono2d_body_detection/config/ .\n\n# Configuring MIPI camera\nexport CAM_TYPE=mipi\n\n# Start the launch file\nros2 launch mono2d_body_detection mono2d_body_detection.launch.py\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Use USB Camera to Publish Images"})}),"\n",(0,r.jsxs)(s.A,{groupId:"tros-distro",children:[(0,r.jsx)(a.A,{value:"foxy",label:"Foxy",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Configure the tros.b environment\nsource /opt/tros/setup.bash\n"})})}),(0,r.jsx)(a.A,{value:"humble",label:"Humble",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Configure the tros.b environment\nsource /opt/tros/humble/setup.bash\n"})})})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"\n# Copy the configuration file required for running the example from the installation path of tros.b.\ncp -r /opt/tros/${TROS_DISTRO}/lib/mono2d_body_detection/config/ .\n\n# Configuring USB camera\nexport CAM_TYPE=usb\n\n# Start the launch file\nros2 launch mono2d_body_detection mono2d_body_detection.launch.py\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Use Local Image Offline"})}),"\n",(0,r.jsxs)(s.A,{groupId:"tros-distro",children:[(0,r.jsx)(a.A,{value:"foxy",label:"Foxy",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Configure the tros.b environment\nsource /opt/tros/setup.bash\n"})})}),(0,r.jsx)(a.A,{value:"humble",label:"Humble",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Configure the tros.b environment\nsource /opt/tros/humble/setup.bash\n"})})})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"# Copy the configuration file required for running the example from the installation path of tros.b.\ncp -r /opt/tros/${TROS_DISTRO}/lib/mono2d_body_detection/config/ .\ncp -r /opt/tros/${TROS_DISTRO}/lib/dnn_node_example/config/ .\n\n# Configure the local playback image.\nexport CAM_TYPE=fb\n\n# Start the launch file\nros2 launch mono2d_body_detection mono2d_body_detection.launch.py publish_image_source:=config/person_body.jpg publish_image_format:=jpg publish_output_image_w:=960 publish_output_image_h:=544\n"})}),"\n",(0,r.jsx)(n.h2,{id:"result-analysis",children:"Result analysis"}),"\n",(0,r.jsx)(n.p,{children:"The following information is outputted in the terminal:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"[mono2d_body_detection-3] [WARN] [1660219823.214730286] [example]: This is mono2d body det example!\n[mono2d_body_detection-3] [WARN] [1660219823.417856952] [mono2d_body_det]: Parameter:\n[mono2d_body_detection-3]  is_sync_mode_: 0\n[mono2d_body_detection-3]  model_file_name_: config/multitask_body_head_face_hand_kps_960x544.hbm\n[mono2d_body_detection-3]  is_shared_mem_sub: 1\n[mono2d_body_detection-3]  ai_msg_pub_topic_name: /hobot_mono2d_body_detection\n[mono2d_body_detection-3] [C][31082][08-11][20:10:23:425][configuration.cpp:49][EasyDNN]EasyDNN version: 0.4.11\n[mono2d_body_detection-3] [BPU_PLAT]BPU Platform Version(1.3.1)!\n[mono2d_body_detection-3] [HBRT] set log level as 0. version = 3.14.5\n[mono2d_body_detection-3] [DNN] Runtime version = 1.9.7_(3.14.5 HBRT)\n[mono2d_body_detection-3] [WARN] [1660219823.545293244] [mono2d_body_det]: Create hbmem_subscription with topic_name: /hbmem_img\n[mono2d_body_detection-3] (MOTMethod.cpp:39): MOTMethod::Init config/iou2_euclid_method_param.json\n[mono2d_body_detection-3] \n[mono2d_body_detection-3] (IOU2.cpp:34): IOU2 Mot::Init config/iou2_euclid_method_param.json\n[mono2d_body_detection-3] \n[mono2d_body_detection-3] (MOTMethod.cpp:39): MOTMethod::Init config/iou2_method_param.json\n[mono2d_body_detection-3] \n[mono2d_body_detection-3] (IOU2.cpp:34): IOU2 Mot::Init config/iou2_method_param.json\n[mono2d_body_detection-3] \n[mono2d_body_detection-3] (MOTMethod.cpp:39): MOTMethod::Init config/iou2_method_param.json\n[mono2d_body_detection-3] \n[mono2d_body_detection-3] (IOU2.cpp:34): IOU2 Mot::Init config/iou2_method_param.json\n[mono2d_body_detection-3] \n[mono2d_body_detection-3] (MOTMethod.cpp:39): MOTMethod::Init config/iou2_method_param.json\n[mono2d_body_detection-3] \n[mono2d_body_detection-3] (IOU2.cpp:34): IOU2 Mot::Init config/iou2_method_param.json\n[mono2d_body_detection-3] \n[mono2d_body_detection-3] [WARN] [1660219824.895102286] [mono2d_body_det]: input fps: 31.34, out fps: 31.22\n[mono2d_body_detection-3] [WARN] [1660219825.921873870] [mono2d_body_det]: input fps: 30.16, out fps: 30.21\n[mono2d_body_detection-3] [WARN] [1660219826.922075496] [mono2d_body_det]: input fps: 30.16, out fps: 30.00\n[mono2d_body_detection-3] [WARN] [1660219827.955463330] [mono2d_body_det]: input fps: 30.01, out fps: 30.01\n[mono2d_body_detection-3] [WARN] [1660219828.955764872] [mono2d_body_det]: input fps: 30.01, out fps: 30.00\n"})}),"\n",(0,r.jsx)(n.p,{children:"The log shows that frame rates of the algorithm during inference are 30fps, and the statistics are refreshed every second."}),"\n",(0,r.jsxs)(n.p,{children:["On the PC browser, enter ",(0,r.jsx)(n.a,{href:"http://IP:8000",children:"http://IP:8000"})," to view the rendering effect of the image and the algorithm (body, head, face, and hand detection boxes, detection box type and target tracking ID, and human body keypoints) (IP is the IP address of the RDK):"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/05_Robot_development/03_boxs/function/image/box_adv/body_render.jpeg",alt:""})})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}}}]);