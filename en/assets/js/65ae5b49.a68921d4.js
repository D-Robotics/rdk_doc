"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[9950],{28453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>r});var s=i(96540);const t={},o=s.createContext(t);function l(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),s.createElement(o.Provider,{value:n},e.children)}},29322:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>r,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"Algorithm_Application/C++_Sample/Ultralytics_YOLO11_Pose","title":"Pose Estimation - Ultralytics YOLO11","description":"This example demonstrates how to run the Ultralytics YOLO11 pose estimation model on the BPU to perform human keypoint detection and visualization. It supports model preprocessing, inference execution, and post-processing (including keypoint decoding, bounding box drawing, and keypoint annotation). The example code is located in the /app/cdevdemo/bpu/04posesample/01ultralyticsyolo11pose/ directory.","source":"@site/i18n/en/docusaurus-plugin-content-docs-docs_s/current/04_Algorithm_Application/03_C++_Sample/08_Ultralytics_YOLO11_Pose.md","sourceDirName":"04_Algorithm_Application/03_C++_Sample","slug":"/Algorithm_Application/C++_Sample/Ultralytics_YOLO11_Pose","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/C++_Sample/Ultralytics_YOLO11_Pose","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1764750792000,"sidebarPosition":8,"frontMatter":{"sidebar_position":8},"sidebar":"tutorialSidebar","previous":{"title":"Instance Segmentation - Ultralytics YOLO11","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/C++_Sample/Ultralytics_YOLO11_Seg"},"next":{"title":"Instance Segmentation - Ultralytics YOLOE11","permalink":"/rdk_doc/en/rdk_s/Algorithm_Application/C++_Sample/Ultralytics_YOLOE11_Seg"}}');var t=i(74848),o=i(28453);const l={sidebar_position:8},r="Pose Estimation - Ultralytics YOLO11",d={},c=[{value:"Model Description",id:"model-description",level:2},{value:"Functionality Description",id:"functionality-description",level:2},{value:"Environment Dependencies",id:"environment-dependencies",level:2},{value:"Directory Structure",id:"directory-structure",level:2},{value:"Build Project",id:"build-project",level:2},{value:"Model Download",id:"model-download",level:2},{value:"Parameter Description",id:"parameter-description",level:2},{value:"Quick Run",id:"quick-run",level:2},{value:"Notes",id:"notes",level:2},{value:"License",id:"license",level:2}];function a(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"pose-estimation---ultralytics-yolo11",children:"Pose Estimation - Ultralytics YOLO11"})}),"\n",(0,t.jsxs)(n.p,{children:["This example demonstrates how to run the Ultralytics YOLO11 pose estimation model on the BPU to perform human keypoint detection and visualization. It supports model preprocessing, inference execution, and post-processing (including keypoint decoding, bounding box drawing, and keypoint annotation). The example code is located in the ",(0,t.jsx)(n.code,{children:"/app/cdev_demo/bpu/04_pose_sample/01_ultralytics_yolo11_pose/"})," directory."]}),"\n",(0,t.jsx)(n.h2,{id:"model-description",children:"Model Description"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Overview"}),":"]}),"\n",(0,t.jsx)(n.p,{children:"Ultralytics YOLO11 Pose is an efficient, lightweight human keypoint detection model capable of simultaneous object detection and pose estimation (multi-keypoint prediction). It integrates Distribution Focal Loss (DFL) to enhance the localization accuracy of bounding boxes and keypoints, making it suitable for real-time multi-person pose estimation tasks."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"HBM Model Name"}),": yolo11n_pose_nashe_640x640_nv12.hbm"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Input Format"}),": NV12 format image (separated Y and UV planes), with resolution 640\xd7640"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Outputs"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Bounding box coordinates (xyxy) for each detected person"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Keypoint locations (K\xd72, x/y coordinates)"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Confidence scores for each keypoint"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Supports COCO human keypoint format (commonly 17 keypoints)"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"functionality-description",children:"Functionality Description"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Model Loading"})}),"\n",(0,t.jsx)(n.p,{children:"Loads the specified Ultralytics YOLO11 pose estimation model and automatically parses its metadata."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Input Preprocessing"})}),"\n",(0,t.jsx)(n.p,{children:"Resizes the input BGR image to 640\xd7640 and converts it to NV12 format (separated Y and UV planes) for model inference."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Inference Execution"})}),"\n",(0,t.jsxs)(n.p,{children:["Calls the ",(0,t.jsx)(n.code,{children:".infer()"})," interface to perform inference."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Result Post-processing"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Decodes bounding boxes from multi-scale outputs (using DFL bin decoding);"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Decodes keypoint positions and confidence scores (K\xd72 + K);"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Applies NMS to remove redundant detection boxes;"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Maps keypoint coordinates and bounding boxes back to the original image dimensions;"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Provides threshold control to display only high-confidence keypoints;"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Supports image visualization, including drawing detection boxes and keypoints."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"environment-dependencies",children:"Environment Dependencies"}),"\n",(0,t.jsx)(n.p,{children:"Before compiling and running, ensure the following dependencies are installed:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sudo apt update\nsudo apt install libgflags-dev\n"})}),"\n",(0,t.jsx)(n.h2,{id:"directory-structure",children:"Directory Structure"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:".\n|-- CMakeLists.txt                     # CMake build script: target/dependency/include path configuration\n|-- README.md                          # Usage instructions (this file)\n|-- inc\n|   `-- ultralytics_yolo11_pose.hpp    # Model wrapper header: declarations for load/preprocess/infer/postprocess interfaces\n`-- src\n    |-- main.cc                        # Program entry point: parses arguments \u2192 full pipeline \u2192 saves visualization results\n    `-- ultralytics_yolo11_pose.cc     # Model implementation: decoding, NMS, keypoint post-processing, and coordinate restoration\n"})}),"\n",(0,t.jsx)(n.h2,{id:"build-project",children:"Build Project"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Configuration and Compilation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"mkdir build && cd build\ncmake ..\nmake -j$(nproc)\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"model-download",children:"Model Download"}),"\n",(0,t.jsx)(n.p,{children:"If the model is not found during program execution, download it using the following command:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"wget https://archive.d-robotics.cc/downloads/rdk_model_zoo/rdk_s100/ultralytics_YOLO/yolo11n_pose_nashe_640x640_nv12.hbm\n"})}),"\n",(0,t.jsx)(n.h2,{id:"parameter-description",children:"Parameter Description"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Parameter"}),(0,t.jsx)(n.th,{children:"Description"}),(0,t.jsx)(n.th,{children:"Default Value"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"--model_path"})}),(0,t.jsxs)(n.td,{children:["Path to the model file (",(0,t.jsx)(n.code,{children:".hbm"}),")"]}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"/opt/hobot/model/s100/basic/yolo11n_pose_nashe_640x640_nv12.hbm"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"--test_img"})}),(0,t.jsx)(n.td,{children:"Path to the input test image"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"/app/res/assets/bus.jpg"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"--label_file"})}),(0,t.jsx)(n.td,{children:"Class label file (one class name per line)"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"/app/res/labels/coco_classes.names"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"--score_thres"})}),(0,t.jsx)(n.td,{children:"Confidence threshold (detections below this are filtered)"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"0.25"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"--nms_thres"})}),(0,t.jsx)(n.td,{children:"IoU threshold for class-wise NMS deduplication"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"0.7"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"--kpt_conf_thres"})}),(0,t.jsx)(n.td,{children:"Keypoint visualization confidence threshold (points below this are hidden)"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"0.5"})})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"quick-run",children:"Quick Run"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Run the Model"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Ensure you are in the ",(0,t.jsx)(n.code,{children:"build"})," directory."]}),"\n",(0,t.jsxs)(n.li,{children:["Run with default parameters:","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"./ultralytics_yolo11_pose\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Run with custom parameters:","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"./ultralytics_yolo11_pose \\\n--model_path /opt/hobot/model/s100/basic/yolo11n_pose_nashe_640x640_nv12.hbm \\\n--test_img   /app/res/assets/bus.jpg \\\n--label_file /app/res/labels/coco_classes.names \\\n--score_thres 0.25 \\\n--nms_thres   0.7 \\\n--kpt_conf_thres 0.5\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"View Results"})}),"\n",(0,t.jsxs)(n.p,{children:["Upon successful execution, results are overlaid on the original image and saved as ",(0,t.jsx)(n.code,{children:"build/result.jpg"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"[Saved] Result saved to: result.jpg\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"notes",children:"Notes"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["The output result is saved as ",(0,t.jsx)(n.code,{children:"result.jpg"}),", which users can inspect directly."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"For more information about deployment options or model support, please refer to the official documentation or contact platform technical support."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"license",children:"License"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-license",children:"Copyright (C) 2025, XiangshunZhao D-Robotics.\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Affero General Public License as\npublished by the Free Software Foundation, either version 3 of the\nLicense, or (at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Affero General Public License for more details.\n\nYou should have received a copy of the GNU Affero General Public License\nalong with this program.  If not, see <https://www.gnu.org/licenses/>.\n"})})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(a,{...e})}):a(e)}}}]);