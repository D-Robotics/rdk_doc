"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[15583],{28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>s});var a=i(96540);const t={},o=a.createContext(t);function r(e){const n=a.useContext(o);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),a.createElement(o.Provider,{value:n},e.children)}},43844:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"Application_case/amr","title":"6.2 AMR","description":"Autonomous Mobile Robot (AMR) is a type of robot that can autonomously navigate and perform tasks in an environment. AMR is different from Automated Guided Vehicles (AGVs), which rely on tracks or pre-defined routes and typically require operator supervision. AMR uses various technologies such as multi-sensor fusion, artificial intelligence, and machine learning to understand the environment and navigate within it, without being limited by wired power sources. Due to its highly flexible mobility and intelligent navigation system, AMR has been widely used in industrial automation, logistics, healthcare, and other fields.","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/06_Application_case/amr.md","sourceDirName":"06_Application_case","slug":"/Application_case/amr","permalink":"/rdk_doc/en/Application_case/amr","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1755152490000,"sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"6.1 Line Following","permalink":"/rdk_doc/en/Application_case/line_follower"},"next":{"title":"7. \u8fdb\u9636\u5f00\u53d1","permalink":"/rdk_doc/en/Advanced_development"}}');var t=i(74848),o=i(28453);const r={sidebar_position:2},s="6.2 AMR",l={},c=[{value:"1.  Example Introduction",id:"1--example-introduction",level:2},{value:"1.1 Appearance",id:"11-appearance",level:3},{value:"1.2 Functional framework",id:"12-functional-framework",level:3},{value:"2. Assembly Example",id:"2-assembly-example",level:2},{value:"2.1 Component List",id:"21-component-list",level:3},{value:"2.2 Assembly steps explanation",id:"22-assembly-steps-explanation",level:3},{value:"2.2.1 Assemble upper shell",id:"221-assemble-upper-shell",level:4},{value:"2.2.2 Wiring of RDK X5",id:"222-wiring-of-rdk-x5",level:4},{value:"2.2.3 Assemble cover board",id:"223-assemble-cover-board",level:4},{value:"2.2.4 Lidar access",id:"224-lidar-access",level:4},{value:"2.2.5 Install camera",id:"225-install-camera",level:4},{value:"1. Install TOF camera",id:"1-install-tof-camera",level:5},{value:"2. Install Binocular camera",id:"2-install-binocular-camera",level:5},{value:"2.2.6 Power supply instructions",id:"226-power-supply-instructions",level:4},{value:"2.2.7 Installation of other components",id:"227-installation-of-other-components",level:4},{value:"3. Preparation of operating environment",id:"3-preparation-of-operating-environment",level:2},{value:"3.1 Sensors check",id:"31-sensors-check",level:3},{value:"3.1.1 Binocular camera",id:"311-binocular-camera",level:4},{value:"3.1.2 Lidar",id:"312-lidar",level:4},{value:"3.1.3 Chassis",id:"313-chassis",level:4},{value:"3.1.4 IMU",id:"314-imu",level:4},{value:"3.2 Function installation",id:"32-function-installation",level:3},{value:"3.3 Source code acquisition (without installation package function, requires source code compilation)",id:"33-source-code-acquisition-without-installation-package-function-requires-source-code-compilation",level:3},{value:"3.4 Compile code",id:"34-compile-code",level:3},{value:"3.5 Sensor calibrate",id:"35-sensor-calibrate",level:3},{value:"3.5.1 Preparation before sensor calibration",id:"351-preparation-before-sensor-calibration",level:4},{value:"1. Calibration environment preparation (calibration tool is kalibr, here we provide dokcer containing kalibr and other calibration scripts)",id:"1-calibration-environment-preparation-calibration-tool-is-kalibr-here-we-provide-dokcer-containing-kalibr-and-other-calibration-scripts",level:5},{value:"i. Download Docker files(Baidu Netdisk)",id:"i-download-docker-filesbaidu-netdisk",level:6},{value:"ii. Create environment",id:"ii-create-environment",level:6},{value:"iii.Docker supports interface display",id:"iiidocker-supports-interface-display",level:6},{value:"2. Prepare calibration board and configure parameters",id:"2-prepare-calibration-board-and-configure-parameters",level:5},{value:"3.5.2 Binocular camera internal parameter calibration (using binocular depth algorithm)",id:"352-binocular-camera-internal-parameter-calibration-using-binocular-depth-algorithm",level:4},{value:"1. Calibration data collection",id:"1-calibration-data-collection",level:5},{value:"i. Activate the binocular camera",id:"i-activate-the-binocular-camera",level:6},{value:"ii. Ros1 and Ros2 message conversion and saving of image dataset (Ubuntu 20.04 system, Ros1 and Ros2 installed)",id:"ii-ros1-and-ros2-message-conversion-and-saving-of-image-dataset-ubuntu-2004-system-ros1-and-ros2-installed",level:6},{value:"iii. Call the calibration program for calibration (specify the path of the recorded bag package and the path of the calibration configuration file)",id:"iii-call-the-calibration-program-for-calibration-specify-the-path-of-the-recorded-bag-package-and-the-path-of-the-calibration-configuration-file",level:6},{value:"iv. After calibration is completed, the calibration file is as follows",id:"iv-after-calibration-is-completed-the-calibration-file-is-as-follows",level:6},{value:"3.5.3 Monocular camera internal parameter calibration (used for external parameter calibration of IMU and RGB CAM in the future. If the module already provides internal parameters, ignore this step)",id:"353-monocular-camera-internal-parameter-calibration-used-for-external-parameter-calibration-of-imu-and-rgb-cam-in-the-future-if-the-module-already-provides-internal-parameters-ignore-this-step",level:4},{value:"1. Run the camera (please execute the command according to the actual module used)",id:"1-run-the-camera-please-execute-the-command-according-to-the-actual-module-used",level:5},{value:"2. Run the bag recorded as ros1 (the conversion between ros1 and ros2 can refer to binocular intrinsic calibration)",id:"2-run-the-bag-recorded-as-ros1-the-conversion-between-ros1-and-ros2-can-refer-to-binocular-intrinsic-calibration",level:5},{value:"3. Run calibration instructions (in Docker image)",id:"3-run-calibration-instructions-in-docker-image",level:5},{value:"4. After calibration is completed, the content is as follows (distortion_comffs: distortion coefficients intrinsic: internal parameters)",id:"4-after-calibration-is-completed-the-content-is-as-follows-distortion_comffs-distortion-coefficients-intrinsic-internal-parameters",level:5},{value:"3.5.4 IMU parameter calibration (used for external parameter calibration of IMU and RGB CAM)",id:"354-imu-parameter-calibration-used-for-external-parameter-calibration-of-imu-and-rgb-cam",level:4},{value:"1. Data acquisition",id:"1-data-acquisition",level:5},{value:"i. Run imu",id:"i-run-imu",level:6},{value:"ii. Ros1 and Ros2 message conversion and saving of image dataset (Ubuntu 20.04 system, Ros1 and Ros2 installed)",id:"ii-ros1-and-ros2-message-conversion-and-saving-of-image-dataset-ubuntu-2004-system-ros1-and-ros2-installed-1",level:6},{value:"iii. Modify script file parameters (in Docker environment)",id:"iii-modify-script-file-parameters-in-docker-environment",level:6},{value:"iv. Run calibration script (calibration results will be printed after imu bag data playback is complete)",id:"iv-run-calibration-script-calibration-results-will-be-printed-after-imu-bag-data-playback-is-complete",level:6},{value:"v.Play IMU&#39;s bag data",id:"vplay-imus-bag-data",level:6},{value:"vi. Modify the parameter configuration file of IMU",id:"vi-modify-the-parameter-configuration-file-of-imu",level:6},{value:"3.5.5 Rgb_cam imu extrinsic calibration (used to obtain the transformation of tof_cam imu)",id:"355-rgb_cam-imu-extrinsic-calibration-used-to-obtain-the-transformation-of-tof_cam-imu",level:4},{value:"1. Activates binocular cameras",id:"1-activates-binocular-cameras",level:5},{value:"2. Start imu",id:"2-start-imu",level:5},{value:"3. Ros1 and Ros2 message conversion and saving of image dataset (Ubuntu 20.04 system, Ros1 and Ros2 installed)",id:"3-ros1-and-ros2-message-conversion-and-saving-of-image-dataset-ubuntu-2004-system-ros1-and-ros2-installed",level:5},{value:"4. Run the calibration program",id:"4-run-the-calibration-program",level:5},{value:"3.5.6 Other",id:"356-other",level:4},{value:"1.  The rgb-cam-tof_cam transformation is provided by the module, and the tof_cam-bask-lonk transformation is provided by the equipment assembly drawing.",id:"1--the-rgb-cam-tof_cam-transformation-is-provided-by-the-module-and-the-tof_cam-bask-lonk-transformation-is-provided-by-the-equipment-assembly-drawing",level:5},{value:"2.  The transformation of tof_cam imu (used for tofSLAM) can be obtained by multiplying the transformation matrices of rgb-cam imu and rgb-cam tof_cam",id:"2--the-transformation-of-tof_cam-imu-used-for-tofslam-can-be-obtained-by-multiplying-the-transformation-matrices-of-rgb-cam-imu-and-rgb-cam-tof_cam",level:5},{value:"3.  The transformation of bask_link imu (for tofSLAM) can be obtained by multiplying the transformation matrices of tof_cam imu and tof_cam bask_link",id:"3--the-transformation-of-bask_link-imu-for-tofslam-can-be-obtained-by-multiplying-the-transformation-matrices-of-tof_cam-imu-and-tof_cam-bask_link",level:5},{value:"3.6 Change configuration file (for tofSLAM)",id:"36-change-configuration-file-for-tofslam",level:3},{value:"4. Functional Experience",id:"4-functional-experience",level:2},{value:"4.1 TofSLAM creates 3D maps",id:"41-tofslam-creates-3d-maps",level:3},{value:"4.2 TofSLAM-positioning",id:"42-tofslam-positioning",level:3},{value:"4.3 Convert 3D point cloud map to 2D grid map",id:"43-convert-3d-point-cloud-map-to-2d-grid-map",level:3},{value:"1. Source code download and compilation",id:"1-source-code-download-and-compilation",level:4},{value:"2. Operation function",id:"2-operation-function",level:4},{value:"3. Save map",id:"3-save-map",level:4},{value:"4.4 Fixed-point navigation",id:"44-fixed-point-navigation",level:3},{value:"1. Activate TOF camera, LiDAR, chassis, IMU",id:"1-activate-tof-camera-lidar-chassis-imu",level:4},{value:"2. Activate Nav2 navigation function",id:"2-activate-nav2-navigation-function",level:4},{value:"3. Activate the fixed-point navigation function (ensure that the complete AprilTag can be seen in the camera image after activation)",id:"3-activate-the-fixed-point-navigation-function-ensure-that-the-complete-apriltag-can-be-seen-in-the-camera-image-after-activation",level:4},{value:"4. Open rviz on PC to view navigation effects",id:"4-open-rviz-on-pc-to-view-navigation-effects",level:4},{value:"4.5 Object detection and segmentation",id:"45-object-detection-and-segmentation",level:3},{value:"1. Copy the launch script to the board and change the corresponding parameters:",id:"1-copy-the-launch-script-to-the-board-and-change-the-corresponding-parameters",level:4},{value:"2. Replace path_of_launchFILE in the command with your own launch file path and run the command",id:"2-replace-path_of_launchfile-in-the-command-with-your-own-launch-file-path-and-run-the-command",level:4},{value:"3. Open a browser on a PC within the same local area network and enter &quot;board ip: 8000&quot; in the website to view the recognition effect",id:"3-open-a-browser-on-a-pc-within-the-same-local-area-network-and-enter-board-ip-8000-in-the-website-to-view-the-recognition-effect",level:4},{value:"5.  Code Introduction",id:"5--code-introduction",level:2},{value:"5.1 tofSLAM",id:"51-tofslam",level:3},{value:"5.1.1 Code repository:",id:"511-code-repository",level:4},{value:"5.1.2 Engineering framework logic:",id:"512-engineering-framework-logic",level:4},{value:"5.1.3 detailed description:",id:"513-detailed-description",level:4},{value:"1. ESKF\uff08Error State Kalman Filter\uff09",id:"1-eskferror-state-kalman-filter",level:5},{value:"2. IMU static initialization",id:"2-imu-static-initialization",level:5},{value:"3. Map format: Ha style voxel map",id:"3-map-format-ha-style-voxel-map",level:5},{value:"4.  Point Cloud ICP",id:"4--point-cloud-icp",level:5},{value:"1.  Key point extraction: Use gridsample method to downsample the point cloud of each frame and extract key points.",id:"1--key-point-extraction-use-gridsample-method-to-downsample-the-point-cloud-of-each-frame-and-extract-key-points",level:6},{value:"5. CT_SCP algorithm\uff08CT-ICP: Real-time Elastic LiDAR Odometry with Loop Closure\uff09",id:"5-ct_scp-algorithmct-icp-real-time-elastic-lidar-odometry-with-loop-closure",level:5},{value:"1. Front end pose estimation",id:"1-front-end-pose-estimation",level:6},{value:"2. Backend Loop Optimization",id:"2-backend-loop-optimization",level:6},{value:"5.2 Binocular depth",id:"52-binocular-depth",level:3},{value:"5.2.1 Repositories:",id:"521-repositories",level:4},{value:"5.2.2 Engineering Logic Framework:",id:"522-engineering-logic-framework",level:4},{value:"5.2.3 detailed description:",id:"523-detailed-description",level:4},{value:"1. Detailed principle of binocular depth perception:",id:"1-detailed-principle-of-binocular-depth-perception",level:5},{value:"2. Steps of binocular stereo matching algorithm:",id:"2-steps-of-binocular-stereo-matching-algorithm",level:5},{value:"3. Model Introduction",id:"3-model-introduction",level:5},{value:"5.3 Object detection and segmentation",id:"53-object-detection-and-segmentation",level:3},{value:"5.3.1 Repositories:",id:"531-repositories",level:4},{value:"5.3.2 Code Engineering Logic Framework:",id:"532-code-engineering-logic-framework",level:4},{value:"5.3.3 detailed description:",id:"533-detailed-description",level:4},{value:"5.4 Navigation and task scheduling",id:"54-navigation-and-task-scheduling",level:3},{value:"5.4.1 Repositories:",id:"541-repositories",level:4},{value:"5.4.2 Engineering Logic Framework:",id:"542-engineering-logic-framework",level:4},{value:"5.4.3 detailed description:",id:"543-detailed-description",level:4},{value:"6. Annex",id:"6-annex",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",h6:"h6",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"62-amr",children:"6.2 AMR"})}),"\n",(0,t.jsx)(n.p,{children:"Autonomous Mobile Robot (AMR) is a type of robot that can autonomously navigate and perform tasks in an environment. AMR is different from Automated Guided Vehicles (AGVs), which rely on tracks or pre-defined routes and typically require operator supervision. AMR uses various technologies such as multi-sensor fusion, artificial intelligence, and machine learning to understand the environment and navigate within it, without being limited by wired power sources. Due to its highly flexible mobility and intelligent navigation system, AMR has been widely used in industrial automation, logistics, healthcare, and other fields."}),"\n",(0,t.jsx)(n.h2,{id:"1--example-introduction",children:"1.  Example Introduction"}),"\n",(0,t.jsx)(n.h3,{id:"11-appearance",children:"1.1 Appearance"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/appearance.png",alt:""})}),"\n",(0,t.jsx)(n.h3,{id:"12-functional-framework",children:"1.2 Functional framework"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/amr_framework_en.jpg",alt:""})}),"\n",(0,t.jsx)(n.h2,{id:"2-assembly-example",children:"2. Assembly Example"}),"\n",(0,t.jsx)(n.h3,{id:"21-component-list",children:"2.1 Component List"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Name"}),(0,t.jsx)(n.th,{children:"Number"}),(0,t.jsx)(n.th,{children:"notes"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"RDK X5"}),(0,t.jsx)(n.td,{children:"1"}),(0,t.jsx)(n.td,{children:"D-robotics"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"AMR chassis\uff08Steering wheel\uff09+ Screws and Parts"}),(0,t.jsx)(n.td,{children:"1"}),(0,t.jsx)(n.td,{children:"YUHESEN"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Single line LiDAR"}),(0,t.jsx)(n.td,{children:"1"}),(0,t.jsx)(n.td,{children:"KRUISEE"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"TOFcamera"}),(0,t.jsx)(n.td,{children:"1"}),(0,t.jsx)(n.td,{children:"DEPTRUM"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Binocular camera\uff08230ai\uff09"}),(0,t.jsx)(n.td,{children:"1"}),(0,t.jsx)(n.td,{children:"D-robotics"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"IMU\uff08BMI088\uff09"}),(0,t.jsx)(n.td,{children:"1"}),(0,t.jsx)(n.td,{children:"D-robotics"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Network cable(0.3m)"}),(0,t.jsx)(n.td,{children:"1"}),(0,t.jsx)(n.td,{})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"12v to 5V4A(MAX 5A)"}),(0,t.jsx)(n.td,{children:"1"}),(0,t.jsx)(n.td,{})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"TOF camera stand\uff083D Printing\uff09"}),(0,t.jsx)(n.td,{children:"1"}),(0,t.jsx)(n.td,{})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Binocular camera stand\uff083D Printing\uff09"}),(0,t.jsx)(n.td,{children:"1"}),(0,t.jsx)(n.td,{})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Cover board\uff083D Printing\uff09"}),(0,t.jsx)(n.td,{children:"1"}),(0,t.jsx)(n.td,{})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"M2*16 Screws + M2 nuts"}),(0,t.jsx)(n.td,{children:"1"}),(0,t.jsx)(n.td,{})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"22-assembly-steps-explanation",children:"2.2 Assembly steps explanation"}),"\n",(0,t.jsx)(n.h4,{id:"221-assemble-upper-shell",children:"2.2.1 Assemble upper shell"}),"\n",(0,t.jsx)(n.p,{children:"The native chassis cannot install cameras or store various cables, so a separate installation is needed to connect to the chassis. Simply align the two holes at the bottom of the installation with the parts on the chassis mounting rail, and tighten the screws"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/upper_shell.jpg",alt:""})}),"\n",(0,t.jsx)(n.h4,{id:"222-wiring-of-rdk-x5",children:"2.2.2 Wiring of RDK X5"}),"\n",(0,t.jsx)(n.p,{children:"All sensors and chassis are directly connected to RDK X5. It should be noted that the binocular camera uses two 22 pin ribbon cables on the same surface. Please install them in the direction shown in the following figure"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/RDK-X5_connection.jpg",alt:""})}),"\n",(0,t.jsx)(n.h4,{id:"223-assemble-cover-board",children:"2.2.3 Assemble cover board"}),"\n",(0,t.jsx)(n.p,{children:"The cover plate is a 3D printed part (see the attached drawing at the end of the text), mainly used for installing RDK X5 and storing other cables. The four mounting holes in the yellow frame are used to install RDK X5, located on the back of the upper cover plate. Align the installation holes at the red box position with the parts on the chassis mounting rail, and tighten the screws."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/cover_board.jpg",alt:""})}),"\n",(0,t.jsx)(n.h4,{id:"224-lidar-access",children:"2.2.4 Lidar access"}),"\n",(0,t.jsx)(n.p,{children:"First, install the LiDAR onto the carrier board, screw it onto the back of the carrier board, and then connect the wiring through the holes on the overload board (shown in the yellow box on the right) to the power manager. The power manager is installed on the back of the carrier board, and the screws are installed from the front of the carrier board (as shown in the red box on the left). After installation, the network cable and 12V power supply are connected, and the network cable and power cord pass through the holes on the top (as shown in the red box on the right)."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/lidar.jpg",alt:""})}),"\n",(0,t.jsx)(n.h4,{id:"225-install-camera",children:"2.2.5 Install camera"}),"\n",(0,t.jsx)(n.h5,{id:"1-install-tof-camera",children:"1. Install TOF camera"}),"\n",(0,t.jsx)(n.p,{children:"Firstly, connect the TOF camera, 3D printed part, and sheet metal bracket together, paying attention to the groove of the 3D printed part facing downwards. After the connection is completed, insert the entire device into the groove above the upper part, and pass the power and signal wires through the internal holes of the upper part (yellow box in Figure 3). Align the screw holes of the sheet metal parts (red box in Figure 1) with the internal screw holes of the upper assembly (red box in Figure 3), and tighten the screws."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/tof_cam.jpg",alt:""})}),"\n",(0,t.jsx)(n.h5,{id:"2-install-binocular-camera",children:"2. Install Binocular camera"}),"\n",(0,t.jsx)(n.p,{children:"First, connect the binocular camera to the 3D printed part, and then connect it to the sheet metal bracket at the rear of the chassis"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/mipi_cam.jpg",alt:""})}),"\n",(0,t.jsx)(n.h4,{id:"226-power-supply-instructions",children:"2.2.6 Power supply instructions"}),"\n",(0,t.jsx)(n.p,{children:"A 12V power supply is provided on the chassis. 12V is provided to the radar, TOF camera, and transformer, and the transformer outputs 5V to the RDK X5"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/power.jpg",alt:""})}),"\n",(0,t.jsx)(n.h4,{id:"227-installation-of-other-components",children:"2.2.7 Installation of other components"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/other.jpg",alt:""})}),"\n",(0,t.jsx)(n.h2,{id:"3-preparation-of-operating-environment",children:"3. Preparation of operating environment"}),"\n",(0,t.jsx)(n.admonition,{title:"attention",type:"info",children:(0,t.jsx)(n.p,{children:"Please obtain the code or function package for TOF camera, LiDAR, chassis, and IMU based on the actual device model. Only the source code for the sweet potato accessory IMU is provided here."})}),"\n",(0,t.jsx)(n.h3,{id:"31-sensors-check",children:"3.1 Sensors check"}),"\n",(0,t.jsx)(n.h4,{id:"311-binocular-camera",children:"3.1.1 Binocular camera"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"#Check the i2c device\nroot@ubuntu:~# i2cdetect -y -r 4\n     0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f\n00:                         -- -- -- -- -- -- -- --\n10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n30: -- -- 32 -- -- -- -- -- -- -- -- -- -- -- -- --\n40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n50: 50 -- -- -- -- -- -- -- 58 -- -- -- -- -- -- --\n60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n70: -- -- -- -- -- -- -- --\n\nroot@ubuntu:~# i2cdetect -y -r 6\n     0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f\n00:                         -- -- -- -- -- -- -- --\n10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n30: 30 -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n50: 50 -- -- -- -- -- -- -- 58 -- -- -- -- -- -- --\n60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n70: -- -- -- -- -- -- -- --\n"})}),"\n",(0,t.jsx)(n.h4,{id:"312-lidar",children:"3.1.2 Lidar"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"#Ensure that the gateway and mask of the Lidar and board are consistent, and confirm if they can be pinged\nping ip of Lidar\n"})}),"\n",(0,t.jsx)(n.h4,{id:"313-chassis",children:"3.1.3 Chassis"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"#Start the CAN device, please set it according to the specific CAN device number and baud rate\nip link set can1 up type can bitrate 500000\n\n#Checking the equipment will result in the following outputs\uff1acan1: <NOARP,UP,LOWER_UP> mtu 16 qdisc mq state UP mode DEFAULT group default qlen 10link/can \nip link show can1\n\n#Check if CAN data can be obtained, please set according to the specific CAN device number\ncandump can1\n"})}),"\n",(0,t.jsx)(n.h4,{id:"314-imu",children:"3.1.4 IMU"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"#Check the i2c device\nroot@ubuntu:~# i2cdetect -y -r 5\n     0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f\n00:                         -- -- -- -- -- -- -- --\n10: -- -- -- -- -- -- -- -- -- 19 -- -- -- -- -- --\n20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n60: -- -- -- -- -- -- -- -- -- 69 -- -- -- -- -- --\n70: -- -- -- -- -- -- -- --\n\n# Obtain IMU data (will print three numbers and change with IMU pose)\ncat /sys/devices/virtual/input/input2/acc_bal\ncat /sys/devices/virtual/input/input2/gry_bal\n"})}),"\n",(0,t.jsx)(n.h3,{id:"32-function-installation",children:"3.2 Function installation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"sudo apt -y tros-hobot-nav2 libsuitesparse-dev libblas-dev liblapack-dev libeigen3-dev libopencv-dev libspdlog-dev libconsole-bridge-dev libpcl-dev libgoogle-glog-dev libtf2-dev  ros-humble-cv-bridge  ros-humble-nav-msgs ros-humble-image-transport ros-humble-tf2-ros ros-humble-pcl-conversions ros-humble-navigation2 libceres-dev\n"})}),"\n",(0,t.jsx)(n.h3,{id:"33-source-code-acquisition-without-installation-package-function-requires-source-code-compilation",children:"3.3 Source code acquisition (without installation package function, requires source code compilation)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"mkdir -p ~/amr_ws/src && cd src\n#tofSLAM \ngit clone https://github.com/wunuo1/Tofslam.git -b humble\n\n#point cloud filter\ngit clone https://github.com/wunuo1/voxel_filter.git\n\n#Specify location navigation\ngit clone https://github.com/wunuo1/pose_setter.git\n\n#\u53cc\u76ee\u6df1\u5ea6\ngit clone https://github.com/D-Robotics/hobot_stereonet.git\n"})}),"\n",(0,t.jsx)(n.h3,{id:"34-compile-code",children:"3.4 Compile code"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"#It is recommended to enable swap memory before compilation\nsudo fallocate -l 4G /swapfile\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile\nswapon /swapfile\n\n#Before compilation, the g2o feature package needs to be compiled, and the code can be obtained from the attachment at the end of the article\n#The compilation method is as follows\ncd g2o\nmkdir build && cd build\ncmake ..\nmake\nmake install\n\n#After compiling g2o, you can compile various feature packages of ros2\ncd ~/amr_ws\nsource /opt/tros/humble/setup.bash\ncolcon build\n"})}),"\n",(0,t.jsx)(n.h3,{id:"35-sensor-calibrate",children:"3.5 Sensor calibrate"}),"\n",(0,t.jsx)(n.h4,{id:"351-preparation-before-sensor-calibration",children:"3.5.1 Preparation before sensor calibration"}),"\n",(0,t.jsx)(n.h5,{id:"1-calibration-environment-preparation-calibration-tool-is-kalibr-here-we-provide-dokcer-containing-kalibr-and-other-calibration-scripts",children:"1. Calibration environment preparation (calibration tool is kalibr, here we provide dokcer containing kalibr and other calibration scripts)"}),"\n",(0,t.jsx)(n.h6,{id:"i-download-docker-filesbaidu-netdisk",children:"i. Download Docker files(Baidu Netdisk)"}),"\n",(0,t.jsxs)(n.p,{children:["Link: ",(0,t.jsx)(n.a,{href:"https://pan.baidu.com/s/1oXegKORgWi8Kzf4kdQXL2g?pwd=ur2z",children:"https://pan.baidu.com/s/1oXegKORgWi8Kzf4kdQXL2g?pwd=ur2z"})]}),"\n",(0,t.jsx)(n.p,{children:"Extracted code\uff1aur2z"}),"\n",(0,t.jsx)(n.h6,{id:"ii-create-environment",children:"ii. Create environment"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"#Load Docker\ndocker load -i ubuntu18.04_calibration.tar\n\n#Check if Docker loads successfully (images with TAG as calibration appear)\ndocker images\n\n#Generate a contact from an image\ndocker run -it --privileged -v /home/nuo.wu/share_dir:/share_dir -e DISPLAY=$DISPLAY -e GDK_SCALE -e GDK_DPI_SCALE --net=host ubuntu18.04:calibration /bin/bash\n"})}),"\n",(0,t.jsx)(n.h6,{id:"iiidocker-supports-interface-display",children:"iii.Docker supports interface display"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"# Run on the host side\nxhost +local:docker\n# Run the following command on the host side\necho $DISPLAY\n# Run the following command in Docker\necho $DISPLAY\n# Check if the outputs of the two are the same\n# If the outputs of the two are different, and the result displayed on the host side is 0, then run the following command in Docker\nexport DISPLAY=:0\n"})}),"\n",(0,t.jsx)(n.h5,{id:"2-prepare-calibration-board-and-configure-parameters",children:"2. Prepare calibration board and configure parameters"}),"\n",(0,t.jsx)(n.p,{children:"Both checkerboard and aprilgrid are acceptable (see attachment for aprilgrid file). The size of the checkerboard should be at least 10cm in length, and to avoid errors in extracting or connecting the corners of the checkerboard during calibration, the rows and columns of the checkerboard should be different. The Aprilgrid calibration board is relatively convenient for data acquisition and easy to operate, but it requires high image quality. Therefore, at this stage, it is still recommended to use a checkerboard. The outer dimensions of the calibration board are greater than 1m."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/cal_board.jpg",alt:""})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"#Set aprilgrid.yaml file\ntarget_type: 'aprilgrid' #gridtype\ntagCols: 6               #number of apriltags\ntagRows: 6               #number of apriltags\ntagSize: 0.024           #size of apriltag, edge to edge [m]\ntagSpacing: 0.3083          #ratio of space between tags to tagSize\ncodeOffset: 0            #code offset for the first tag in the aprilboard\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"#Set the parameters of the chessboard calibration board, where targetCols and targetRows are the number of inliers in each column and row, respectively. If the number of squares is 9 columns x8 columns, then the number of inliers is 8 columns x7 columns.\ntarget_type: 'checkboard' \ntargetCols: 8 \ntargetRows: 7  \nrawSpacingMeters: 0.1        \ncolSpacingMeters: 0.1    \n"})}),"\n",(0,t.jsx)(n.h4,{id:"352-binocular-camera-internal-parameter-calibration-using-binocular-depth-algorithm",children:"3.5.2 Binocular camera internal parameter calibration (using binocular depth algorithm)"}),"\n",(0,t.jsx)(n.h5,{id:"1-calibration-data-collection",children:"1. Calibration data collection"}),"\n",(0,t.jsx)(n.p,{children:"Move the camera or calibration board at different distances to ensure that the calibration board covers the FOV range of the image as much as possible. During the movement, do not move the checkerboard calibration board out of the frame. Slowly and steadily move the camera or calibration board to avoid image blurring caused by motion."}),"\n",(0,t.jsx)(n.h6,{id:"i-activate-the-binocular-camera",children:"i. Activate the binocular camera"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:'source /opt/tros/humble/setup.bash\nros2 run mipi_cam mipi_cam --ros-args -p device_mode:="dual" -p out_format:="nv12" -p dual_combine:=2 -p framerate:=10.0 --log-level warn\n'})}),"\n",(0,t.jsx)(n.h6,{id:"ii-ros1-and-ros2-message-conversion-and-saving-of-image-dataset-ubuntu-2004-system-ros1-and-ros2-installed",children:"ii. Ros1 and Ros2 message conversion and saving of image dataset (Ubuntu 20.04 system, Ros1 and Ros2 installed)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"# Install ros1_bridge\nsudo apt update\nsudo apt install ros-humble-ros1-bridge\n\n#Start Terminal\nsource /opt/ros/noetic/setup.bash\nroscore\n\n#Start a new terminal\nsource /opt/ros/noetic/setup.bash\nsource /opt/ros/humble/setup.bash\nros2 run ros1_bridge dynamic_bridge --bridge-all-topics=false --bridge-topic sensor_msgs/msg/Image /image_combine_raw\n\n#Start a new terminal to record image data (move the camera to different positions)\nsource /opt/ros/noetic/setup.bash\nrosbag record image_combine_raw\n"})}),"\n",(0,t.jsx)(n.h6,{id:"iii-call-the-calibration-program-for-calibration-specify-the-path-of-the-recorded-bag-package-and-the-path-of-the-calibration-configuration-file",children:"iii. Call the calibration program for calibration (specify the path of the recorded bag package and the path of the calibration configuration file)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"$1=path_to_bag\n$2=path_to_calib_yaml(checkbord.yaml or aprilgrid.yaml)\nrosrun kalibr kalibr_calibrate_cameras --bag $1 --topics /camera/left/image_raw /camera/right/image_raw --models pinhole-radtan pinhole-radtan --target $2 --show-extraction\n"})}),"\n",(0,t.jsx)(n.h6,{id:"iv-after-calibration-is-completed-the-calibration-file-is-as-follows",children:"iv. After calibration is completed, the calibration file is as follows"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"cam0:\n  cam_overlaps: [1]\n  camera_model: pinhole\n  distortion_coeffs: [-0.30512368344314034, 0.06869074995752597, -0.0007768971335288749, -0.0018077365618742228]\n  distortion_model: radtan\n  intrinsics: [874.7756769752957, 876.9016366753913, 988.9147013767086, 574.9147170323885]\n  resolution: [1920, 1080]\n  rostopic: /camera/left/image_raw\ncam1:\n  T_cn_cnm1:\n  - [0.9998783828690954, -0.0007967376839525958, 0.015575130180209843, -0.06826569753821157]\n  - [0.0008025928358794327, 0.9999996095896897, -0.0003696825565397285, 8.695548926243514e-05]\n  - [-0.01557482955949466, 0.0003821380847083855, 0.9998786319622305, 0.0011080079883866683]\n  - [0.0, 0.0, 0.0, 1.0]\n  cam_overlaps: [0]\n  camera_model: pinhole\n  distortion_coeffs: [-0.30647450034075296, 0.06904546877578269, -0.0015178028297652521, -0.00016199661118514476]\n  distortion_model: radtan\n  intrinsics: [875.6405166670517, 877.1839943792446, 963.0392879775055, 519.3086364230766]\n  resolution: [1920, 1080]\n  rostopic: /camera/right/image_raw\n"})}),"\n",(0,t.jsx)(n.h4,{id:"353-monocular-camera-internal-parameter-calibration-used-for-external-parameter-calibration-of-imu-and-rgb-cam-in-the-future-if-the-module-already-provides-internal-parameters-ignore-this-step",children:"3.5.3 Monocular camera internal parameter calibration (used for external parameter calibration of IMU and RGB CAM in the future. If the module already provides internal parameters, ignore this step)"}),"\n",(0,t.jsx)(n.h5,{id:"1-run-the-camera-please-execute-the-command-according-to-the-actual-module-used",children:"1. Run the camera (please execute the command according to the actual module used)"}),"\n",(0,t.jsx)(n.h5,{id:"2-run-the-bag-recorded-as-ros1-the-conversion-between-ros1-and-ros2-can-refer-to-binocular-intrinsic-calibration",children:"2. Run the bag recorded as ros1 (the conversion between ros1 and ros2 can refer to binocular intrinsic calibration)"}),"\n",(0,t.jsx)(n.h5,{id:"3-run-calibration-instructions-in-docker-image",children:"3. Run calibration instructions (in Docker image)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"#cam.bag records the bag file/image camera topic pinhole radtan camera model checkborad.yaml calibration board description file\nrosrun kalibr kalibr_calibrate_cameras --bag cam.bag --topic /image --model pinhole-radtan --target checkboard.yaml\n"})}),"\n",(0,t.jsx)(n.h5,{id:"4-after-calibration-is-completed-the-content-is-as-follows-distortion_comffs-distortion-coefficients-intrinsic-internal-parameters",children:"4. After calibration is completed, the content is as follows (distortion_comffs: distortion coefficients intrinsic: internal parameters)"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/mipi_int.jpg",alt:""})}),"\n",(0,t.jsx)(n.h4,{id:"354-imu-parameter-calibration-used-for-external-parameter-calibration-of-imu-and-rgb-cam",children:"3.5.4 IMU parameter calibration (used for external parameter calibration of IMU and RGB CAM)"}),"\n",(0,t.jsx)(n.h5,{id:"1-data-acquisition",children:"1. Data acquisition"}),"\n",(0,t.jsx)(n.h6,{id:"i-run-imu",children:"i. Run imu"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"source ~/amr_ws/install/setup.bash \nros2 launch imu_sensor imu_sensor.launch.py\n"})}),"\n",(0,t.jsx)(n.h6,{id:"ii-ros1-and-ros2-message-conversion-and-saving-of-image-dataset-ubuntu-2004-system-ros1-and-ros2-installed-1",children:"ii. Ros1 and Ros2 message conversion and saving of image dataset (Ubuntu 20.04 system, Ros1 and Ros2 installed)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"#Start Terminal\nsource /opt/ros/noetic/setup.bash\nroscore\n\n#Start a new Terminal\nsource /opt/ros/noetic/setup.bash\nsource /opt/ros/humble/setup.bash\nros2 run ros1_bridge dynamic_bridge --bridge-all-topics=false --bridge-topic sensor_msgs/msg/Imu /imu_data\n\n#Start a new terminal to record IMU data (IMU needs to be left idle for more than 30 minutes to store data)\nsource /opt/ros/noetic/setup.bash\nrosbag record imu_data\n"})}),"\n",(0,t.jsx)(n.h6,{id:"iii-modify-script-file-parameters-in-docker-environment",children:"iii. Modify script file parameters (in Docker environment)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"vim /root/catkin_ws/src/imu_utils/launch/oal.launch\n\n#imu_topic         IMU's topic\n#data_save_path    Save path of calibration file\n#max_time_min      The duration of collecting IMU data\n"})}),"\n",(0,t.jsx)(n.h6,{id:"iv-run-calibration-script-calibration-results-will-be-printed-after-imu-bag-data-playback-is-complete",children:"iv. Run calibration script (calibration results will be printed after imu bag data playback is complete)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"roslaunch imu_utils oal.lauch\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/imu_output.jpg",alt:""})}),"\n",(0,t.jsx)(n.h6,{id:"vplay-imus-bag-data",children:"v.Play IMU's bag data"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"rosbag play imu.bag\n"})}),"\n",(0,t.jsx)(n.h6,{id:"vi-modify-the-parameter-configuration-file-of-imu",children:"vi. Modify the parameter configuration file of IMU"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/cal_product.jpg",alt:""})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"# Copy a new copy of the BMI088_imu_ param. yaml file generated in the previous text as a backup for future use. Execute the following command:\ncp -r BMI088_imu_param.yaml imu.yaml\n\nvim imu.yaml\n"})}),"\n",(0,t.jsxs)(n.p,{children:["After opening the configuration file, as shown in the following figure:\n",(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/imu_config.jpg",alt:""})]}),"\n",(0,t.jsx)(n.p,{children:"Rewrite the content of the original BMI088_imu_maram.ml file and modify it to the following format:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"#Accelerometers\naccelerometer_noise_density: 3.0219972277096316e-02   #Noise density (continuous-time): Corresponding to Acc ->avg axis ->acc_n in the screenshot above\naccelerometer_random_walk:   1.9955401630133244e-04   #Bias random walk: Corresponding to Acc ->avg axis ->accw_ in the screenshot above\n \n#Gyroscopes\ngyroscope_noise_density:     2.8131368978892658e-03   #Noise density (continuous-time): Corresponding to Gyr ->avg axis ->gyr-n in the screenshot above\ngyroscope_random_walk:       1.9705283783356781e-05   #Bias random walk: Corresponding to Gyr ->avg axis ->gyr-w in the screenshot above\n \nrostopic:                    /imu_data     #The IMU ROS topic corresponds to the/imu_data in the topic of the data bag generated in rosbag format in the previous section\nupdate_rate:                 400.0      #Hz (for discretization of the values above) corresponds to the actual IMU frequency used\n"})}),"\n",(0,t.jsx)(n.h4,{id:"355-rgb_cam-imu-extrinsic-calibration-used-to-obtain-the-transformation-of-tof_cam-imu",children:"3.5.5 Rgb_cam imu extrinsic calibration (used to obtain the transformation of tof_cam imu)"}),"\n",(0,t.jsx)(n.h5,{id:"1-activates-binocular-cameras",children:"1. Activates binocular cameras"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:'source /opt/tros/humble/setup.bash\nros2 run mipi_cam mipi_cam --ros-args -p device_mode:="dual" -p out_format:="nv12" -p dual_combine:=2 -p framerate:=10.0 --log-level warn\n'})}),"\n",(0,t.jsx)(n.h5,{id:"2-start-imu",children:"2. Start imu"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"source ~/amr_ws/install/setup.bash \nros2 launch imu_sensor imu_sensor.launch.py\n"})}),"\n",(0,t.jsx)(n.h5,{id:"3-ros1-and-ros2-message-conversion-and-saving-of-image-dataset-ubuntu-2004-system-ros1-and-ros2-installed",children:"3. Ros1 and Ros2 message conversion and saving of image dataset (Ubuntu 20.04 system, Ros1 and Ros2 installed)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"#Install ros1_bridge\nsudo apt update\nsudo apt install ros-humble-ros1-bridge\n\n#Start terminal\nsource /opt/ros/noetic/setup.bash\nroscore\n\n#Start a new terminal\nsource /opt/ros/noetic/setup.bash\nsource /opt/ros/humble/setup.bash\nros2 run ros1_bridge dynamic_bridge --bridge-all-topics\n\n#Start a new terminal to record image data (move the camera to different positions)\nsource /opt/ros/noetic/setup.bash\nrosbag record /image_combine_raw /imu_data\n"})}),"\n",(0,t.jsx)(n.h5,{id:"4-run-the-calibration-program",children:"4. Run the calibration program"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"rosrun kalibr kalibr_calibrate_imu_camera --bag cam_imu.bag --target april.yaml --cam cam.yaml --imu imu.yaml\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/imu_cam_cal.jpg",alt:""})}),"\n",(0,t.jsx)(n.p,{children:"After calibration, check the calibration results (when dJ converges and drops below 0.1, it indicates accurate calibration)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"# storage-results-imucam.txt \nvim storage-results-imucam.txt\n"})}),"\n",(0,t.jsxs)(n.p,{children:["The external reference results are as follows\uff1a\n",(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/imu_cam_trans.jpg",alt:""})]}),"\n",(0,t.jsx)(n.h4,{id:"356-other",children:"3.5.6 Other"}),"\n",(0,t.jsx)(n.h5,{id:"1--the-rgb-cam-tof_cam-transformation-is-provided-by-the-module-and-the-tof_cam-bask-lonk-transformation-is-provided-by-the-equipment-assembly-drawing",children:"1.  The rgb-cam-tof_cam transformation is provided by the module, and the tof_cam-bask-lonk transformation is provided by the equipment assembly drawing."}),"\n",(0,t.jsx)(n.h5,{id:"2--the-transformation-of-tof_cam-imu-used-for-tofslam-can-be-obtained-by-multiplying-the-transformation-matrices-of-rgb-cam-imu-and-rgb-cam-tof_cam",children:"2.  The transformation of tof_cam imu (used for tofSLAM) can be obtained by multiplying the transformation matrices of rgb-cam imu and rgb-cam tof_cam"}),"\n",(0,t.jsx)(n.h5,{id:"3--the-transformation-of-bask_link-imu-for-tofslam-can-be-obtained-by-multiplying-the-transformation-matrices-of-tof_cam-imu-and-tof_cam-bask_link",children:"3.  The transformation of bask_link imu (for tofSLAM) can be obtained by multiplying the transformation matrices of tof_cam imu and tof_cam bask_link"}),"\n",(0,t.jsx)(n.h3,{id:"36-change-configuration-file-for-tofslam",children:"3.6 Change configuration file (for tofSLAM)"}),"\n",(0,t.jsx)(n.p,{children:"Modify the mapping.yaml file in the config folder and pay attention to the parameters with explanations\uff0cthe output file is obtained from the attachment at the end of the article"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"YAML: 1.0\n\npreprocess:\n  point_filter_num: 3  #Point cloud downsampling\n  lidar_type: 7  # 1-AVIA 2-velodyne 3-ouster  4-robosense 5-pandar 6-dtof 7-mtof\n  blind: 0.01\n\n\ncommon:\n  imu_topic: /imu #imu message name \n  lid_topic: /nebula/mtof_points2 #point cloud or lidar message name \n  odom_topic: /odom  # odometer message name \n  img_topic: /camera/color/image_raw/compressed #rgb image message, can be compressed or original image\n  Localization_mode: False #Whether to locate, otherwise it is mapping\n  output_path: /home/slam_ws_2/output #Path to store mapping products\n  model_path: /home/slam_ws_2/output/hfnet.onnx #Specify the hfnet model path\n  imuhz: 400 #IMU's frequency\n \nmapping:\n  extrinsic_est_en: true\n  #The change from TOF to IMU can be obtained by referring to the calibration instructions\n  tof2imu_extrinsic_T: [ -0.0039799203111821468, -0.027189542185995823831, -0.14001955020902342461]\n  tof2imu_extrinsic_R: [-0.993063627638373553564 ,0.052518963674774226572 ,-0.01498737732389914978, \n                        -0.033814436045461613632 ,-0.016796566829113524293 ,-0.99928717438014507495, \n                        -0.00801808196977665167 ,-0.99847864521832871386  ,0.03465425204866725379]\n  \n  #The change from bsak_link to IMU can be obtained by referring to the calibration instructions\n  robot2imu_extrinsic_T: [0.12588064308116462841, 0.008162559660163853708, 0.20705414746130172184]\n  robot2imu_extrinsic_R: [0.052268869631951098023, -0.93852017660344945467, 0.033846560965168889575, \n                          0.93808656435923075909, -0.051737213472478319612, 0.015014368436287678477, \n                          0.033019704102458273174        , 0.01675474093936925155, 0.99931446977489903968]\n\n\ndelay_time: 0.3\n\nodometry:\n  wheelmode: 1 #0-relativepose 1-velocity\n  max_trans_diff: 0.1\n  max_drift_num: 15\n  surf_res: 0.1 #0.4\n  log_print: true\n  max_num_iteration: 5\n  # ct_icp\n  icpmodel: CT_POINT_TO_PLANE  # CT_POINT_TO_PLANE  #CT_POINT_TO_PLANE                    # Options: [CT_POINT_TO_PLANE, POINT_TO_PLANE]\n  size_voxel_map: 0.1 #0.4                         # The voxel size of in the voxel map\n  min_distance_points: 0.05\n  max_num_points_in_voxel: 20                 # The maximum number of points per voxel of the map\n  max_distance: 50.0                        # The threshold of the distance to suppress voxels from the map\n  weight_alpha: 0.9\n  weight_neighborhood: 0.1\n  max_dist_to_plane_icp: 0.1 #0.3\n  init_num_frames: 20\n  voxel_neighborhood: 1\n  max_number_neighbors: 20\n  threshold_voxel_occupancy: 1\n  estimate_normal_from_neighborhood: true\n  min_number_neighbors: 20                    # The minimum number of neighbor points to define a valid neighborhood\n  power_planarity: 2.0\n  num_closest_neighbors: 1\n\n  sampling_rate: 1.0\n  ratio_of_nonground: 2\n  # max_num_residuals: 1000\n  max_num_residuals: 2000\n  min_num_residuals: 100\n  motion_compensation: CONSTANT_VELOCITY #NONE #CONSTANT_VELOCITY  #CONTINUOUS #NONE, CONSTANT_VELOCITY, ITERATIVE, CONTINUOUS\n  beta_location_consistency: 1.0\n  beta_orientation_consistency: 1.0\n  beta_constant_velocity: 1.0\n  beta_small_velocity: 0.0\n\n  thres_translation_norm: 0.03\n  thres_orientation_norm: 0.05\n\n  use_ground_constraint: 0\n\n#The following are the positioning function parameters\nreloc:\n  mapfile: /home/slam_ws_2/output/final-voxel.pcd #PCD point cloud map path, the product of completed mapping\n  reloc_mode: 1  #0-scancontext 1-hfnet  2-no   #Relocation detection method, 0 is point cloud detection, 1 is feature point detection\n  scancontext_dbfile: /home/slam_ws_2/output/ScanContext.bin #Point cloud detection model path\n  hfnet_dbfile: /home/slam_ws_2/output/HFNet.bin #Feature point detection model path\n  icp_threshold: 0.03\n"})}),"\n",(0,t.jsx)(n.h2,{id:"4-functional-experience",children:"4. Functional Experience"}),"\n",(0,t.jsx)(n.h3,{id:"41-tofslam-creates-3d-maps",children:"4.1 TofSLAM creates 3D maps"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"#Ensure the completion of all items mentioned in the preparation of the operating environment\n#Please ensure that the camera can see the complete apriltag at the beginning of the mapping process, and that all topic parameters match. After running, the map coordinate system will be transformed to apriltag in the output folder\nsource ~/amr_ws/install/setup.bash\nros2 run demo demo --ros-args -p build_map:=true\n\n#The Localization mode parameter in the config configuration file is set to False, and various topic parameters match\nsource ~/amr_ws/install/setup.bash\nros2 run ct_lio ct_lio_eskf\n"})}),"\n",(0,t.jsxs)(n.p,{children:["After startup, it is necessary to let the IMU initialize successfully for 3-4 seconds. Then, the mobile robot can be used for mapping. After the mapping is completed, the program should be closed. After completion, as shown in the following figure:\n",(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/buld_map_output.jpg",alt:""})]}),"\n",(0,t.jsxs)(n.p,{children:["The path specified in the mapping. yaml file will generate the following products\n",(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/build_map_product.jpg",alt:""})]}),"\n",(0,t.jsxs)(n.p,{children:["Use the pcl_viewer tool to view point cloud maps:\n",(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/pcl_viewer.jpg",alt:""})]}),"\n",(0,t.jsxs)(n.p,{children:["Architectural effect video: ",(0,t.jsx)(n.a,{href:"https://www.youtube.com/watch?v=TKRYnEfsEoQ&list=PLSxjn4YS2IuFUWcLGj2_uuCfLYnNYw6Ld&index=20",children:"https://www.youtube.com/watch?v=TKRYnEfsEoQ&list=PLSxjn4YS2IuFUWcLGj2_uuCfLYnNYw6Ld&index=20"})]}),"\n",(0,t.jsx)(n.h3,{id:"42-tofslam-positioning",children:"4.2 TofSLAM-positioning"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"#Ensure that all items mentioned in the preparation of the running environment are completed, the Localization_made parameter in the config configuration file is set to True, and all types of topic parameters match\nsource ~/amr_ws/install/setup.bash\nros2 run ct_lio ct_lio_eskf\n"})}),"\n",(0,t.jsxs)(n.p,{children:["After startup, if the display frame number remains at 1, it means that the relocation has not been successful yet\n",(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/reloc_fail.jpg",alt:""}),"\nWhen the frame number is greater than 1, it indicates successful relocation, and rviz2 can be opened to view the path\n",(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/reloc_success.png",alt:""}),"\n",(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/path.png",alt:""})]}),"\n",(0,t.jsx)(n.h3,{id:"43-convert-3d-point-cloud-map-to-2d-grid-map",children:"4.3 Convert 3D point cloud map to 2D grid map"}),"\n",(0,t.jsx)(n.p,{children:"This feature pack is an open-source feature pack for ros1. Please use it in an environment where ros1 is already installed. The usage method is as follows:"}),"\n",(0,t.jsx)(n.h4,{id:"1-source-code-download-and-compilation",children:"1. Source code download and compilation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"mkdir -p ~/pcd3d-2d/src && cd ~/pcd3d-2d/src\ngit clone https://github.com/Hinson-A/pcd2pgm_package\ncd ..\nsource /opt/ros/noetic/setup.bash\ncatkin_make\n"})}),"\n",(0,t.jsx)(n.h4,{id:"2-operation-function",children:"2. Operation function"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"source /opt/ros/noetic/setup.bash\nsource ~/pcd3d-2d/devel/setup.bash\n\n#Replace the pcd file path in the launch file with the actual path\nroslaunch pcd2pgm run.launch\n"})}),"\n",(0,t.jsx)(n.h4,{id:"3-save-map",children:"3. Save map"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"#Open a new terminal (requiring installation of ros nometic map server)\nsource /opt/ros/noetic/setup.bash\nrosrun map_server map_saver\n"})}),"\n",(0,t.jsxs)(n.p,{children:["After execution, map.pgm and map.yaml files will be generated in the current path. These two files can be used for Nav2, but when using them, the files in the maps folder of hobot_nav2 need to be replaced. The general path is/opt/dros/humble/share/hobot_nav2/maps. The point cloud map and raster map effects are as follows:\n",(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/map.jpg",alt:""})]}),"\n",(0,t.jsx)(n.h3,{id:"44-fixed-point-navigation",children:"4.4 Fixed-point navigation"}),"\n",(0,t.jsx)(n.h4,{id:"1-activate-tof-camera-lidar-chassis-imu",children:"1. Activate TOF camera, LiDAR, chassis, IMU"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"#Please start each sensor according to the actual device operation instructions. Only the IMU start instruction is provided here\nsource /opt/tros/humble/setup.bash\nros2 launch imu_sensor imu_sensor.launch.py\n\n#Open a new terminal and start point cloud filtering\nros2 run point_cloud_processing point_cloud_sparsification\n"})}),"\n",(0,t.jsx)(n.h4,{id:"2-activate-nav2-navigation-function",children:"2. Activate Nav2 navigation function"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"source /opt/tros/humble/setup.bash\nros2 launch hobot_nav2 hobot_nav2_bringup.launch.py\n"})}),"\n",(0,t.jsx)(n.h4,{id:"3-activate-the-fixed-point-navigation-function-ensure-that-the-complete-apriltag-can-be-seen-in-the-camera-image-after-activation",children:"3. Activate the fixed-point navigation function (ensure that the complete AprilTag can be seen in the camera image after activation)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"source ~/amr_ws/install/setup.bash\nros2 run demo demo\n"})}),"\n",(0,t.jsx)(n.h4,{id:"4-open-rviz-on-pc-to-view-navigation-effects",children:"4. Open rviz on PC to view navigation effects"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"ros2 launch nav2_bringup rviz_launch.py\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Video: ",(0,t.jsx)(n.a,{href:"https://www.youtube.com/watch?v=IYsS2bADpeQ&list=PLSxjn4YS2IuFUWcLGj2_uuCfLYnNYw6Ld&index=21",children:"https://www.youtube.com/watch?v=IYsS2bADpeQ&list=PLSxjn4YS2IuFUWcLGj2_uuCfLYnNYw6Ld&index=21"})]}),"\n",(0,t.jsx)(n.h3,{id:"45-object-detection-and-segmentation",children:"4.5 Object detection and segmentation"}),"\n",(0,t.jsx)(n.h4,{id:"1-copy-the-launch-script-to-the-board-and-change-the-corresponding-parameters",children:"1. Copy the launch script to the board and change the corresponding parameters:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"import os\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource, AnyLaunchDescriptionSource\nfrom ament_index_python import get_package_share_directory\n\ndef generate_launch_description():\n    #Encode the BGR8 images released by the TOF camera into JPEG format for web display\n    bgr2jpeg =  Node(\n        package='hobot_codec',\n        executable='hobot_codec_republish',\n        output='screen',\n        parameters=[{\n            'in_mode': 'ros',\n            'in_format': 'bgr8',\n            'out_mode': 'ros',\n            'out_format': 'jpeg',\n            'sub_topic': '/nebula200/stof_rgb/image_raw',\n            'dump_output': False\n        }],\n        arguments=['--ros-args', '--log-level', 'warn']\n    )\n    #Decoding JPEG images into NV12 for YOLOV8 inference\n    jpeg2nv12 =  Node(\n        package='hobot_codec',\n        executable='hobot_codec_republish',\n        output='screen',\n        parameters=[{\n            'in_mode': 'ros',\n            'in_format': 'jpeg',\n            'out_mode': 'ros',\n            'out_format': 'nv12',\n            'channel': 1,\n            'sub_topic': '/image_raw/compressed',\n            'dump_output': False,\n            'pub_topic': '/image'\n        }],\n        arguments=['--ros-args', '--log-level', 'warn']\n    )\n    #Run YOLOV8 to detect segmentation\n    yolov8 =  Node(\n        package='dnn_node_example',\n        executable='example',\n        output='screen',\n        parameters=[{\n            'feed_type': 1,\n            'is_shared_mem_sub': 0,\n            'config_file': 'config/yolov8segworkconfig.json'\n        }],\n        arguments=['--ros-args', '--log-level', 'warn']\n    )\n    #Activate the web display function\n    web_node = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource(\n            os.path.join(\n                get_package_share_directory('websocket'),\n                'launch/websocket.launch.py')),\n        launch_arguments={\n            'websocket_image_topic': '/image_raw/compressed',\n            'websocket_image_type': 'mjpeg',\n            'websocket_smart_topic': '/hobot_dnn_detection'\n        }.items()\n    )\n\n    return LaunchDescription([\n        bgr2jpeg,\n        jpeg2nv12,\n        yolov8,\n        web_node\n    ])\n"})}),"\n",(0,t.jsx)(n.h4,{id:"2-replace-path_of_launchfile-in-the-command-with-your-own-launch-file-path-and-run-the-command",children:"2. Replace path_of_launchFILE in the command with your own launch file path and run the command"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"source /opt/tros/humble/setup.bash\nros2 launch <path_of_launch_file>\n"})}),"\n",(0,t.jsx)(n.h4,{id:"3-open-a-browser-on-a-pc-within-the-same-local-area-network-and-enter-board-ip-8000-in-the-website-to-view-the-recognition-effect",children:'3. Open a browser on a PC within the same local area network and enter "board ip: 8000" in the website to view the recognition effect'}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/yolov8seg_web.jpg",alt:""})}),"\n",(0,t.jsx)(n.h2,{id:"5--code-introduction",children:"5.  Code Introduction"}),"\n",(0,t.jsx)(n.h3,{id:"51-tofslam",children:"5.1 tofSLAM"}),"\n",(0,t.jsx)(n.h4,{id:"511-code-repository",children:"5.1.1 Code repository:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://github.com/wunuo1/TofSLAM_ros2",children:"https://github.com/wunuo1/TofSLAM_ros2"})}),"\n",(0,t.jsx)(n.h4,{id:"512-engineering-framework-logic",children:"5.1.2 Engineering framework logic:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/tofslam_framework_en.jpg",alt:""})}),"\n",(0,t.jsx)(n.h4,{id:"513-detailed-description",children:"5.1.3 detailed description:"}),"\n",(0,t.jsx)(n.h5,{id:"1-eskferror-state-kalman-filter",children:"1. ESKF\uff08Error State Kalman Filter\uff09"}),"\n",(0,t.jsx)(n.p,{children:"The error state Kalman filter is an algorithm for sensor fusion. It is based on the classical Kalman Filter (KF) principle and has been improved to better handle error propagation in nonlinear systems"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.em,{children:"state quantity"}),"\uff1ap\u3001R\u3001v\u3001bg\u3001ba\u3001g"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.em,{children:"prediction model"}),"\uff1aIMU\n",(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/imu_model.png",alt:""})]}),"\n",(0,t.jsx)(n.p,{children:"*Observation (update) model 1 *: Chassis wheel speed&chassis IMU pose calculated through EKF - loose coupling (R, p update)"}),"\n",(0,t.jsxs)(n.p,{children:["*Observation (update) model 2 *: pose obtained from point cloud ICP calculation - loose coupling (R, p update)\n",(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/model2.png",alt:""})]}),"\n",(0,t.jsx)(n.h5,{id:"2-imu-static-initialization",children:"2. IMU static initialization"}),"\n",(0,t.jsx)(n.p,{children:"IMU initialization remains static, estimating the direction of gravity, gyroscope bias, and accelerometer bias based on the accelerometer. Set the initial pose as the origin and the initial velocity as 0"}),"\n",(0,t.jsx)(n.h5,{id:"3-map-format-ha-style-voxel-map",children:"3. Map format: Ha style voxel map"}),"\n",(0,t.jsx)(n.p,{children:"Use a Ha series table to store voxel maps, with several points stored in each voxel (0.1 \xd7 0.1 \xd7 0.1) and a certain distance (0.05) maintained between each point."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/voxel.png",alt:""})}),"\n",(0,t.jsx)(n.h5,{id:"4--point-cloud-icp",children:"4.  Point Cloud ICP"}),"\n",(0,t.jsx)(n.h6,{id:"1--key-point-extraction-use-gridsample-method-to-downsample-the-point-cloud-of-each-frame-and-extract-key-points",children:"1.  Key point extraction: Use gridsample method to downsample the point cloud of each frame and extract key points."}),"\n",(0,t.jsx)(n.p,{children:"For each key point:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Find nearest neighbors: Using voxelmap, search for the N nearest points qi in adjacent voxels as neighbor points."}),"\n",(0,t.jsx)(n.li,{children:"Calculate the normal n, covariance matrix, and smoothness weight a of neighboring points"}),"\n",(0,t.jsxs)(n.li,{children:["Calculate the distance between key points and the plane where neighboring points are located, and add ICP optimization for point pairs with distances less than the threshold.\n",(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/point_icp.jpg",alt:""})]}),"\n"]}),"\n",(0,t.jsx)(n.h5,{id:"5-ct_scp-algorithmct-icp-real-time-elastic-lidar-odometry-with-loop-closure",children:"5. CT_SCP algorithm\uff08CT-ICP: Real-time Elastic LiDAR Odometry with Loop Closure\uff09"}),"\n",(0,t.jsx)(n.h6,{id:"1-front-end-pose-estimation",children:"1. Front end pose estimation"}),"\n",(0,t.jsxs)(n.p,{children:["The overall framework of the front-end part of the CTICP algorithm uses two poses to describe each frame:\n",(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/ct_icp_pose.jpg",alt:""}),"\nThere is no need to separately remove motion distortion for each frame of point cloud in advance, but rather to directly remove distortion during the optimization process. This algorithm is more robust for high-speed motion."]}),"\n",(0,t.jsxs)(n.p,{children:["Optimization equation:\n",(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/equation.jpg",alt:""})]}),"\n",(0,t.jsx)(n.h6,{id:"2-backend-loop-optimization",children:"2. Backend Loop Optimization"}),"\n",(0,t.jsxs)(n.p,{children:["Scan Nmap frames each time to form a point cloud, and insert each point into a 2D elevation grid. Then generate a 2D elevation map from the 2D elevation grid, with each pixel corresponding to the point with the largest Z-axis. Extract features from 2D elevation maps, and store the extracted features in memory along with the 2D elevation grid along with keyframes. When the elevation map features are extracted each time and matched with the features in memory; For successfully matched elevation maps, first estimate the 2D transformation matrix using RANSAC, and verify the matching reliability based on the number of inliers; Using the 2D transformation matrix as the initial value, perform ICP on the point cloud in the 2D elevation grid, and finally add an edge to the successfully matched keyframes. Use g2o to optimize the pose map. (This method is only applicable to planar motion)\n",(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/loop_optimization.jpg",alt:""})]}),"\n",(0,t.jsx)(n.h3,{id:"52-binocular-depth",children:"5.2 Binocular depth"}),"\n",(0,t.jsx)(n.h4,{id:"521-repositories",children:"5.2.1 Repositories:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://github.com/D-Robotics/hobot_stereonet.git",children:"https://github.com/D-Robotics/hobot_stereonet.git"})}),"\n",(0,t.jsx)(n.h4,{id:"522-engineering-logic-framework",children:"5.2.2 Engineering Logic Framework:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/stereonet_framework_en.jpg",alt:""})}),"\n",(0,t.jsx)(n.h4,{id:"523-detailed-description",children:"5.2.3 detailed description:"}),"\n",(0,t.jsx)(n.h5,{id:"1-detailed-principle-of-binocular-depth-perception",children:"1. Detailed principle of binocular depth perception:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/perception_principle.jpg",alt:""})}),"\n",(0,t.jsx)(n.h5,{id:"2-steps-of-binocular-stereo-matching-algorithm",children:"2. Steps of binocular stereo matching algorithm:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/stereonet_step.jpg",alt:""})}),"\n",(0,t.jsx)(n.h5,{id:"3-model-introduction",children:"3. Model Introduction"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Backbone: MixVarGENet+UNet. Efficient backbone+UNet recovery details optimized for X5;"}),"\n",(0,t.jsx)(n.li,{children:"Cost Volume: Groupwise Correlation Cost Volume. Calculate the correlation between left and right image features to construct a cost volume;"}),"\n",(0,t.jsx)(n.li,{children:"Cost Aggregation: UNet. Aggregate cost volume through UNet to achieve more refined integration of cost volume;"}),"\n",(0,t.jsx)(n.li,{children:"Refinement: GRU. The use of GRU module brings performance improvement in edge details."}),"\n",(0,t.jsxs)(n.li,{children:["Spatial Upsampling: Using Conv instead of Unfold operation for tensor segmentation, generating a full resolution disparity map through weighted combination\n",(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/stereonet_model.jpg",alt:""})]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"53-object-detection-and-segmentation",children:"5.3 Object detection and segmentation"}),"\n",(0,t.jsx)(n.h4,{id:"531-repositories",children:"5.3.1 Repositories:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://github.com/D-Robotics/hobot_dnn/tree/",children:"https://github.com/D-Robotics/hobot_dnn/tree/"})}),"\n",(0,t.jsx)(n.h4,{id:"532-code-engineering-logic-framework",children:"5.3.2 Code Engineering Logic Framework:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/detection_framework_en.jpg",alt:""})}),"\n",(0,t.jsx)(n.h4,{id:"533-detailed-description",children:"5.3.3 detailed description:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Use the official version YOLOV8 SEG model, source code repository link ",(0,t.jsx)(n.a,{href:"https://github.com/ultralytics/ultralytics",children:"https://github.com/ultralytics/ultralytics"})]}),"\n",(0,t.jsx)(n.li,{children:"The encoding and decoding functions are accelerated by the hardware unit on RDK X5, significantly reducing CPU usage while improving format conversion efficiency"}),"\n",(0,t.jsx)(n.li,{children:"The project uses the hobot-dnn_deample with the code tros, which supports inference for multiple models in addition to yolov8-seg"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"54-navigation-and-task-scheduling",children:"5.4 Navigation and task scheduling"}),"\n",(0,t.jsx)(n.h4,{id:"541-repositories",children:"5.4.1 Repositories:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://github.com/wunuo1/pose_setter.git",children:"https://github.com/wunuo1/pose_setter.git"})}),"\n",(0,t.jsx)(n.h4,{id:"542-engineering-logic-framework",children:"5.4.2 Engineering Logic Framework:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/06_Application_case/amr/pose_setter_en.jpg",alt:""})}),"\n",(0,t.jsx)(n.h4,{id:"543-detailed-description",children:"5.4.3 detailed description:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"The program performs AprilTag detection at startup. After successful detection, it will cache ten frames of transformations and take the average value. Obtain the transformation from RGB to AprilTag, read the transformation from map to AprilTag recorded during mapping, and calculate the transformation from RGB to map. In the early calibration process, the transformation from RGB to robot can already be calculated, so the transformation from robot to map can be obtained to publish the initial position of the robot in the map coordinate system."}),"\n",(0,t.jsx)(n.li,{children:"After the initial position is published, the activation thread starts requesting navigation to the target position. If the request is rejected, it indicates that the posture initialization has failed and the initial position is republished."}),"\n",(0,t.jsx)(n.li,{children:"Request navigation to multiple target locations. Only after the previous target location is successfully navigated, will the thread be activated for the next request"}),"\n",(0,t.jsx)(n.li,{children:"After the initial position, AprilTag detection will be performed and the previous steps will be repeated in a loop"}),"\n",(0,t.jsx)(n.li,{children:"In the Nav2 navigation plugin, point cloud information has been added to the obstacle layer to avoid obstacles with heights lower than the LiDAR. However, due to the noise in the native point cloud data and the large amount of point cloud data, point cloud filtering nodes have been added to sparsify and filter the point cloud."}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"6-annex",children:"6. Annex"}),"\n",(0,t.jsxs)(n.p,{children:["Link\uff1a",(0,t.jsx)(n.a,{href:"https://pan.baidu.com/s/14A4mqJvqzfp-MZaQkGpvLg?pwd=8VpL",children:"https://pan.baidu.com/s/14A4mqJvqzfp-MZaQkGpvLg?pwd=8VpL"}),"\nExtracted code\uff1a8VpL"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);