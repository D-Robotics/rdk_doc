<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-Advanced_development/toolchain_development/intermediate/ptq_process" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">Principles and Steps of PTQ | RDK DOC</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://developer.d-robotics.cc/rdk_doc/en/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://developer.d-robotics.cc/rdk_doc/en/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://developer.d-robotics.cc/rdk_doc/en/Advanced_development/toolchain_development/intermediate/ptq_process"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="zh_Hans"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Principles and Steps of PTQ | RDK DOC"><meta data-rh="true" name="description" content="Introduction"><meta data-rh="true" property="og:description" content="Introduction"><link data-rh="true" rel="icon" href="/rdk_doc/en/img/logo.png"><link data-rh="true" rel="canonical" href="https://developer.d-robotics.cc/rdk_doc/en/Advanced_development/toolchain_development/intermediate/ptq_process"><link data-rh="true" rel="alternate" href="https://developer.d-robotics.cc/rdk_doc/Advanced_development/toolchain_development/intermediate/ptq_process" hreflang="zh-Hans"><link data-rh="true" rel="alternate" href="https://developer.d-robotics.cc/rdk_doc/en/Advanced_development/toolchain_development/intermediate/ptq_process" hreflang="en"><link data-rh="true" rel="alternate" href="https://developer.d-robotics.cc/rdk_doc/Advanced_development/toolchain_development/intermediate/ptq_process" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"7. Advanced Development","item":"https://developer.d-robotics.cc/rdk_doc/en/Advanced_development"},{"@type":"ListItem","position":2,"name":"7.4 Algorithm Toolchain Development Guide","item":"https://developer.d-robotics.cc/rdk_doc/en/04_toolchain_development"},{"@type":"ListItem","position":3,"name":"7.4.2 Advanced Guide","item":"https://developer.d-robotics.cc/rdk_doc/en/Advanced_development/toolchain_development/intermediate/"},{"@type":"ListItem","position":4,"name":"Principles and Steps of PTQ","item":"https://developer.d-robotics.cc/rdk_doc/en/Advanced_development/toolchain_development/intermediate/ptq_process"}]}</script><script src="https://hm.baidu.com/hm.js?24dd63cad43b63889ea6bede5fd1ab9e" async></script>
<script src="/rdk_doc/js/dify-config.js"></script>
<script src="https://rdk.d-robotics.cc/embed.min.js" id="MltLQTHPb5EeP7uz" defer="defer"></script><link rel="stylesheet" href="/rdk_doc/en/assets/css/styles.e31f7a90.css">
<script src="/rdk_doc/en/assets/js/runtime~main.8763d9fb.js" defer="defer"></script>
<script src="/rdk_doc/en/assets/js/main.ea148884.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a href="https://d-robotics.cc/" target="_blank" rel="noopener noreferrer" class="navbar__brand"><div class="navbar__logo"><img src="/rdk_doc/en/img/logo.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/rdk_doc/en/img/logo.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">D-Robotics</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/rdk_doc/en/RDK">RDK X3 / X5</a><a class="navbar__item navbar__link" href="/rdk_doc/en/rdk_s/RDK">RDK S100</a><a href="https://developer.d-robotics.cc/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Community<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/D-Robotics" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>EN</a><ul class="dropdown__menu"><li><a href="/rdk_doc/Advanced_development/toolchain_development/intermediate/ptq_process" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-Hans">CN</a></li><li><a href="/rdk_doc/en/Advanced_development/toolchain_development/intermediate/ptq_process" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">EN</a></li></ul></div><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_YFbd" value=""><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rdk_doc/en/RDK">D-Robotics RDK Suite</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/rdk_doc/en/Quick_start">1. Quick Start</a><button aria-label="Expand sidebar category &#x27;1. Quick Start&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/rdk_doc/en/System_configuration">2. System Configuration</a><button aria-label="Expand sidebar category &#x27;2. System Configuration&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/rdk_doc/en/Basic_Application">3. Basic Application Development</a><button aria-label="Expand sidebar category &#x27;3. Basic Application Development&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/rdk_doc/en/Basic_Development">4. Algorithm Application Development</a><button aria-label="Expand sidebar category &#x27;4. Algorithm Application Development&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/rdk_doc/en/Robot_development">5. Robotics Application</a><button aria-label="Expand sidebar category &#x27;5. Robotics Application&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/rdk_doc/en/Application_case">6. Application Development Guide</a><button aria-label="Expand sidebar category &#x27;6. Application Development Guide&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/rdk_doc/en/Advanced_development">7. Advanced Development</a><button aria-label="Collapse sidebar category &#x27;7. Advanced Development&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/rdk_doc/en/hardware_development">7.1 Hardware Development Guide</a><button aria-label="Expand sidebar category &#x27;7.1 Hardware Development Guide&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/rdk_doc/en/linux_development">7.2. Linux Development Guide</a><button aria-label="Expand sidebar category &#x27;7.2. Linux Development Guide&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/rdk_doc/en/03_multimedia_development">7.3 RDK X3 Multimedia Development Guide</a><button aria-label="Expand sidebar category &#x27;7.3 RDK X3 Multimedia Development Guide&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/rdk_doc/en/04_toolchain_development">7.4 Algorithm Toolchain Development Guide</a><button aria-label="Collapse sidebar category &#x27;7.4 Algorithm Toolchain Development Guide&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/rdk_doc/en/Advanced_development/toolchain_development/overview">7.4.1 Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/rdk_doc/en/Advanced_development/toolchain_development/intermediate/">7.4.2 Advanced Guide</a><button aria-label="Collapse sidebar category &#x27;7.4.2 Advanced Guide&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/rdk_doc/en/Advanced_development/toolchain_development/intermediate/environment_config">Environment Installation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/rdk_doc/en/Advanced_development/toolchain_development/intermediate/ptq_process">Principles and Steps of PTQ</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/rdk_doc/en/Advanced_development/toolchain_development/intermediate/supported_op_list">supported_op_list</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/rdk_doc/en/Advanced_development/toolchain_development/intermediate/runtime_sample">On-board Model Application Development Guide</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/rdk_doc/en/Advanced_development/toolchain_development/expert/">7.4.3 Expert Guide</a><button aria-label="Expand sidebar category &#x27;7.4.3 Expert Guide&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/rdk_doc/en/FAQ">8. FAQs</a><button aria-label="Expand sidebar category &#x27;8. FAQs&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/rdk_doc/en/Appendix">9. Appendix</a><button aria-label="Expand sidebar category &#x27;9. Appendix&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/rdk_doc/en/Release_Note/release_note">10. Version release</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/rdk_doc/en/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/rdk_doc/en/Advanced_development"><span>7. Advanced Development</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/rdk_doc/en/04_toolchain_development"><span>7.4 Algorithm Toolchain Development Guide</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/rdk_doc/en/Advanced_development/toolchain_development/intermediate/"><span>7.4.2 Advanced Guide</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Principles and Steps of PTQ</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Principles and Steps of PTQ</h1></header>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="PTQ_introduction">Introduction<a href="#PTQ_introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction">​</a></h3>
<p>Model conversion refers to the process of converting the original floating-point model to the Heterogeneous Line model. The original floating-point model (also referred to as the floating-point model in some places in this article) refers to the model you trained using DL frameworks such as TensorFlow/PyTorch, with a computational precision of float32. The Heterogeneous Line model is a model format that is suitable for running on the Heterogeneous Line processor.
This chapter will repeatedly use these two model terms. To avoid misunderstandings, please understand this concept before reading further.</p>
<p>The complete development process of the model with the Heterogeneous Line algorithm toolchain requires five important stages: <strong>Floating-point Model Preparation</strong>, <strong>Model Verification</strong>, <strong>Model Conversion</strong>, <strong>Performance Evaluation</strong>, and <strong>Accuracy Evaluation</strong>, as shown in the following diagram:</p>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/model_conversion_flowchart.png" alt="model_conversion_flowchart" class="img_ev3q"></p>
<p><strong>Floating-point Model Preparation</strong> This stage is used to ensure that the format of the original floating-point model is supported by the Heterogeneous Line model conversion tool. The original floating-point model is obtained from the available model trained by DL frameworks such as TensorFlow/PyTorch. For specific floating-point model requirements and recommendations, please refer to the <a href="#model_preparation"><strong>Floating-point Model Preparation</strong></a> section.</p>
<p><strong>Model Verification</strong> This stage is used to verify whether the original floating-point model meets the requirements of the Heterogeneous Line algorithm toolchain. Heterogeneous Line provides the <code>hb_mapper checker</code> tool for checking floating-point models. For specific usage, please refer to the <a href="#model_check"><strong>Model Verification</strong></a> section.</p>
<p><strong>Model Conversion</strong> This stage is used to perform the conversion from the floating-point model to the Heterogeneous Line heterogeneous model. After this stage, you will obtain a model that can run on the Heterogeneous Line processor. Heterogeneous Line provides the <code>hb_mapper makertbin</code> conversion tool to complete key steps such as model optimization, quantization, and compilation. For specific usage, please refer to the <a href="#model_conversion"><strong>Model Conversion</strong></a> section.</p>
<p><strong>Performance Evaluation</strong> This stage is mainly used to evaluate the inference performance of the Heterogeneous Line heterogeneous model. Heterogeneous Line provides tools for model performance evaluation, which you can use to verify whether the model performance meets the application requirements. For specific usage instructions, please refer to the <a href="#performance_evaluation"><strong>Model Performance Analysis and Tuning</strong></a> section.</p>
<p><strong>Accuracy Evaluation</strong> This stage is mainly used to evaluate the inference accuracy of the Heterogeneous Line heterogeneous model. Heterogeneous Line provides tools for model accuracy evaluation. For specific usage instructions, please refer to the <a href="#accuracy_evaluation"><strong>Model Accuracy Analysis and Tuning</strong></a> section.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="model_preparation">Model Preparation<a href="#model_preparation" class="hash-link" aria-label="Direct link to Model Preparation" title="Direct link to Model Preparation">​</a></h3>
<p>The floating-point model obtained from publicly available DL frameworks is the input for the Heterogeneous Line model conversion tool. Currently, the conversion tool supports the following DL frameworks:</p>
<table><thead><tr><th><strong>Framework</strong></th><th>Caffe</th><th>PyTorch</th><th>TensorFlow</th><th>MXNet</th><th>PaddlePaddle</th></tr></thead><tbody><tr><td><strong>Heterogeneous Line Toolchain</strong></td><td>Supported</td><td>Supported (via ONNX conversion)</td><td>Supported (via ONNX conversion)</td><td>Supported (via ONNX conversion)</td><td>Supported (via ONNX conversion)</td></tr></tbody></table>
<p>Among these frameworks, the caffemodel exported by the Caffe framework is directly supported. PyTorch, TensorFlow, MXNet, and other DL frameworks are indirectly supported through conversion to the ONNX format.</p>
<p>For the conversion from different frameworks to ONNX, there are currently corresponding standard solutions, as follows:</p>
<ul>
<li>
<p>Pytorch2Onnx: PyTorch official API supports exporting models directly as ONNX models. Refer to the link:
<a href="https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html" target="_blank" rel="noopener noreferrer">https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html</a>.</p>
</li>
<li>
<p>Tensorflow2Onnx: Conversion based on the onnx/tensorflow-onnx in the ONNX community. Refer to the link:
<a href="https://github.com/onnx/tensorflow-onnx" target="_blank" rel="noopener noreferrer">https://github.com/onnx/tensorflow-onnx</a>.</p>
</li>
<li>
<p>MXNet2Onnx: MXNet official API supports exporting models directly as ONNX models. Refer to the link:- More ONNX conversion support for other frameworks, please refer to the link: <a href="https://github.com/onnx/tutorials#converting-to-onnx-format" target="_blank" rel="noopener noreferrer">https://github.com/onnx/tutorials#converting-to-onnx-format</a>.</p>
</li>
</ul>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Tips</div><div class="admonitionContent_BuS1"><p>We also provide tutorials on how to export ONNX and visualize models for Pytorch, PaddlePaddle, and TensorFlow2 frameworks. Please refer to:</p><ul>
<li>
<p><a href="https://developer.d-robotics.cc/forumDetail/146177165367615499" target="_blank" rel="noopener noreferrer"><strong>Pytorch Export ONNX and Model Visualization Tutorial</strong></a>;</p>
</li>
<li>
<p><a href="https://developer.d-robotics.cc/forumDetail/146177165367615500" target="_blank" rel="noopener noreferrer"><strong>PaddlePaddle Export ONNX and Model Visualization Tutorial</strong></a>;</p>
</li>
<li>
<p><a href="https://developer.d-robotics.cc/forumDetail/146177165367615501" target="_blank" rel="noopener noreferrer"><strong>TensorFlow2 Export ONNX and Model Visualization Tutorial</strong></a>;</p>
</li>
</ul></div></div>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Caution</div><div class="admonitionContent_BuS1"><ul>
<li>
<p>Operators used in floating-point models need to comply with the operator constraint conditions of the D-Robotics algorithm toolchain. Please refer to the <a href="/rdk_doc/en/Advanced_development/toolchain_development/intermediate/supported_op_list"><strong>Supported Operator List</strong></a> section for details.</p>
</li>
<li>
<p>Currently, the conversion tool only supports the conversion of models with output count less than or equal to 32.</p>
</li>
<li>
<p>Supports quantization of <code>caffe 1.0</code> version floating-point models and <code>ir_version ≤ 7</code>, <code>opset=10</code>, <code>opset=11</code> versions of ONNX floating-point models into fixed-point models supported by Horizon. For the mapping between the IR version of the ONNX model and the ONNX version, please refer to the <a href="https://github.com/onnx/onnx/blob/main/docs/Versioning.md" target="_blank" rel="noopener noreferrer"><strong>ONNX official documentation</strong></a>.</p>
</li>
<li>
<p>Model input dimensions only support <code>fixed 4 dimensions</code> input NCHW or NHWC (N dimension can only be 1), for example: 1x3x224x224 or 1x224x224x3. Dynamic dimensions and non-4D inputs are not supported.</p>
</li>
<li>
<p>Do not include <code>post-processing operators</code> in floating-point models, such as NMS operators.</p>
</li>
</ul></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="model_check">Model Validation<a href="#model_check" class="hash-link" aria-label="Direct link to Model Validation" title="Direct link to Model Validation">​</a></h3>
<p>Before formally converting the model, please use the <code>hb_mapper checker</code> tool to validate the model and ensure that it complies with the constraints supported by the D-Robotics processor.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Tips</div><div class="admonitionContent_BuS1"><p>It is recommended to refer to the script methods <code>01_check_X3.sh</code> or <code>01_check_Ultra.sh</code> in the model conversion <code>horizon_model_convert_sample</code> example package of D-Robotics for examples of caffe, onnx, and other models.</p></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="validate-the-model-using-the-hb_mapper-checker-tool">Validate the model using the <code>hb_mapper checker</code> tool<a href="#validate-the-model-using-the-hb_mapper-checker-tool" class="hash-link" aria-label="Direct link to validate-the-model-using-the-hb_mapper-checker-tool" title="Direct link to validate-the-model-using-the-hb_mapper-checker-tool">​</a></h4>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">The usage of the hb_mapper checker tool is as follows:</span><br></span></code></pre></div></div>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  hb_mapper checker --model-type $`{`model_type`}` \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    --march $`{`march`}` \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    --proto $`{`proto`}` \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    --model $`{`caffe_model/onnx_model`}` \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    --input-shape $`{`input_node`}` $`{`input_shape`}` \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    --output $`{`output`}`</span><br></span></code></pre></div></div>
<p>hb_mapper checker parameters explanation:</p>
<p>--model-type<br>
Specifies the model type of the input for checking, currently only supports setting <code>caffe</code> or <code>onnx</code>.</p>
<p>--march
Specifies the D-Robotics processor type to be adapted, can be set to <code>bernoulli2</code> or <code>bayes</code>; set to <code>bernoulli2</code> for RDK X3 and <code>bayes</code> for RDK Ultra.</p>
<p>--proto<br>
This parameter is only useful when <code>model-type</code> is set to <code>caffe</code>, and its value is the prototxt file name of the Caffe model.</p>
<p>--model<br>
When <code>model-type</code> is specified as <code>caffe</code>, its value is the caffemodel file name of the Caffe model.
When <code>model-type</code> is specified as <code>onnx</code>, its value is the name of the ONNX model file.</p>
<p>--input-shape<br>
Optional parameter, explicitly specifies the input shape of the model.
Its value is <code>{`input_name`}` `{`NxHxWxC/NxCxHxW`}</code>, with a space between <code>input_name</code> and the shape.
For example, if the model input is named <code>data1</code> and the input shape is <code>[1,224,224,3]</code>,
then the configuration should be <code>--input-shape data1 1x224x224x3</code>.
If the configured shape here is inconsistent with the shape information inside the model, the shape configured here takes precedence.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Remark</div><div class="admonitionContent_BuS1"><p>Note that <code>--input-shape</code> only accepts one name-shape combination. If your model has multiple input nodes,
you can configure the <code>--input-shape</code> parameter multiple times in the command.</p></div></div>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>caution</div><div class="admonitionContent_BuS1"><p>The --output parameter has been deprecated. The log information is stored in <code>hb_mapper_checker.log</code> by default.</p></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="handling-exceptions-in-checking">Handling Exceptions in Checking<a href="#handling-exceptions-in-checking" class="hash-link" aria-label="Direct link to Handling Exceptions in Checking" title="Direct link to Handling Exceptions in Checking">​</a></h4>
<p>If the model checking step terminates abnormally or error messages are displayed, it means that the model verification fails. Please refer to the error information printed on the terminal or the <code>hb_mapper_checker.log</code> log file generated in the current path for error information and modification suggestions.</p>
<p>For example: The following configuration contains an unrecognized operator type <code>Accuracy</code>:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  layer `{`</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    name: &quot;data&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    type: &quot;Input&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    top: &quot;data&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    input_param `{` shape: `{` dim: 1 dim: 3 dim: 224 dim: 224 `}` `}`</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  `}`</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  layer `{`</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    name: &quot;Convolution1&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    type: &quot;Convolution&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    bottom: &quot;data&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    top: &quot;Convolution1&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    convolution_param `{`</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      num_output: 128</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      bias_term: false</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      pad: 0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      kernel_size: 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      group: 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      stride: 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      weight_filler `{`</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        type: &quot;msra&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      `}`</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    `}`</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  `}`</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  layer `{`</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    name: &quot;accuracy&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    type: &quot;Accuracy&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    bottom: &quot;Convolution3&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    top: &quot;accuracy&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    include `{`</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      phase: TEST</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    `}`</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  `}`</span><br></span></code></pre></div></div>
<p>After using <code>hb_mapper checker</code> to check this model, you will get the following information in the <code>hb_mapper_checker.log</code>:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  ValueError: Not support layer name=accuracy type=Accuracy</span><br></span></code></pre></div></div>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><ul>
<li>If the model check step is terminated abnormally or there is an error message, it means that the model verification fails. Please confirm the error message and modification suggestions according to the terminal print or the generated <code>hb_mapper_checker.log</code> log file in the current path. You can find the solution to the error in the &quot;Model Quantization Errors and Solutions&quot; section. If the above steps still cannot resolve the problem, please contact the D-Robotics technical support team or submit your question in the <a href="https://developer.d-robotics.cc/" target="_blank" rel="noopener noreferrer">D-Robotics Official Developer Community</a>. We will provide support within 24 hours.</li>
</ul></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="check_result">Interpretation of the check results<a href="#check_result" class="hash-link" aria-label="Direct link to Interpretation of the check results" title="Direct link to Interpretation of the check results">​</a></h4>
<p>If there is no ERROR, then the check is successful. The <code>hb_mapper checker</code> tool will directly output the following information:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  ==============================================</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Node         ON   Subgraph  Type</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ----------</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  conv1        BPU  id(0)     HzSQuantizedConv</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  conv2_1/dw   BPU  id(0)     HzSQuantizedConv</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  conv2_1/sep  BPU  id(0)     HzSQuantizedConv</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  conv2_2/dw   BPU  id(0)     HzSQuantizedConv</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  conv2_2/sep  BPU  id(0)     HzSQuantizedConvconv3_1/dw BPU id(0) HzSQuantizedConv</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">conv3_1/sep BPU id(0) HzSQuantizedConv</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">...</span><br></span></code></pre></div></div>
<p>The result of each line represents the checking status of a model node, with four columns: Node, ON, Subgraph, and Type. They represent the node name, the hardware on which the node is executed, the subgraph to which the node belongs, and the D-Robotics operator name to which the node is mapped. If the model contains CPU operators in the network structure, the hb_mapper checker tool will split the part before and after the CPU operator into two subgraphs.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="optimization-guide-for-checking-results">Optimization Guide for Checking Results<a href="#optimization-guide-for-checking-results" class="hash-link" aria-label="Direct link to Optimization Guide for Checking Results" title="Direct link to Optimization Guide for Checking Results">​</a></h4>
<p>Ideally, all operators in the model&#x27;s network structure should run on the BPU, which means there is only one subgraph. If there are CPU operators causing multiple subgraphs to be split, the &quot;hb_mapper checker&quot; tool will provide the specific reasons for the appearance of the CPU operators. Below are examples of model verification on RDK X3 and RDK Ultra.</p>
<ul>
<li>The Caffe model running on &quot;RDK X3&quot; has a structure of Reshape + Pow + Reshape. According to the operator constraint list on &quot;RDK X3&quot;, we can see that the Reshape operator is currently running on the CPU, and the shape of Pow is also non-4D, which does not meet the constraints of the X3 BPU operator.</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/model_reshape.png" alt="model_reshape" class="img_ev3q"></p>
<p>Therefore, the final checking result of the model will also show segmentation, as follows:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">2022-05-25 15:16:14,667 INFO The converted model node information:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">====================================================================================</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Node                                  ON   Subgraph  Type</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-------------</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">conv68                                BPU  id(0)     HzSQuantizedConv</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sigmoid16                             BPU  id(0)     HzLut</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">axpy_prod16                           BPU  id(0)     HzSQuantizedMul</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">UNIT_CONV_FOR_eltwise_layer16_add_1   BPU  id(0)     HzSQuantizedConv</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">prelu49                               BPU  id(0)     HzPRelu</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">fc1                                   BPU  id(0)     HzSQuantizedConv</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">fc1_reshape_0                         CPU  --        Reshape</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">fc_output/square                      CPU  --        Pow</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">fc_output/sum_pre_reshape             CPU  --        Reshape</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">fc_output/sum                         BPU  id(1)     HzSQuantizedConv</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">fc_output/sum_reshape_0               CPU  --        Reshape</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">fc_output/sqrt                        CPU  --        Pow</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">fc_output/expand_pre_reshape          CPU  --        Reshape</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">fc_output/expand                      BPU  id(2)     HzSQuantizedConv</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">fc1_reshape_1                         CPU  --        Reshape</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">fc_output/expand_reshape_0            CPU  --        Reshape</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">fc_output/op                          CPU  --        Mul</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre></div></div>
<ul>
<li>The ONNX model running on &quot;RDK Ultra&quot; has a structure of Mul + Add + Mul. According to the operator constraint list on &quot;RDK Ultra&quot;, we can see that Mul and Add operators are supported on five dimensions for BPU execution, but they need to meet the constraints of the Ultra BPU operators; otherwise, they will fall back to CPU computation.</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/model_reshape_ONNX.png" alt="model_reshape" class="img_ev3q"></p>
<p>Therefore, the final checking result of the model will also show segmentation, as follows:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  ====================================================================================</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Node                                    ON   Subgraph  Type</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  -------------------------------------------------------------------------------------</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Reshape_199                             BPU  id(0)     Reshape</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Transpose_200                           BPU  id(0)     Transpose</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Sigmoid_201                             BPU  id(0)     HzLut</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Split_202                               BPU  id(0)     Split</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Mul_204                                 CPU  --        Mul</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Add_206                                 CPU  --        Add</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Mul_208                                 CPU  --        Mul</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Mul_210                                 CPU  --        Mul</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Pow_211                                 BPU  id(1)     HzLut</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Mul_213                                 CPU  --        Mul</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Concat_214                              CPU  --        Concat</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Reshape_215                             CPU  --        Reshape</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Conv_216                                BPU  id(0)     HzSQuantizedConv</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Reshape_217                             BPU  id(0)     Reshape</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Transpose_218                           BPU  id(0)     Transpose</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Sigmoid_219                             BPU  id(0)     HzLut</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Split_220                               BPU  id(0)     Split</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Mul_222                                 CPU  --        Mul</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Add_224                                 CPU  --        Add</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Mul_226                                 CPU  --        Mul</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Mul_228                                 CPU  --        Mul</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Pow_229                                 BPU  id(2)     HzLut</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Mul_231                                 CPU  --        Mul</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Concat_232                              CPU  --        Concat</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Reshape_233                             CPU  --        Reshape</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Conv_234                                BPU  id(0)     HzSQuantizedConv</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Reshape_235                             BPU  id(0)     Reshape</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Transpose_236                           BPU  id(0)     Transpose</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Sigmoid_237                             BPU  id(0)     HzLut</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Split_238                               BPU  id(0)     Split</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Mul_240                                 CPU  --        Mul</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Add_242                                 CPU  --        Add</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Mul_244                                 CPU  --        Mul</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Mul_246                                 CPU  --        Mul</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Pow_247                                 BPU  id(3)     HzLut</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Mul_249                                 CPU  --        Mul</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Concat_250                              CPU  --        Concat</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Reshape_251                             CPU  --        Reshape</span><br></span></code></pre></div></div>
<p>According to the hints provided by hb_mapper checker, generally, the operators running on BPU will have better performance. In this case, you can remove the CPU operators like pow and reshape from the model and calculate the corresponding functions in the post-processing to reduce the number of subgraphs.</p>
<p>However, multiple subgraphs will not affect the overall conversion process but will affect the performance of the model to a large extent. It is recommended to adjust the model operators to run on BPU as much as possible. You can refer to the BPU operator support list in the D-Robotics Processor Operator Support List to replace the CPU operators with BPU operators with the same functions or move the CPU operators in the model to the pre- and post-processing of the model for CPU calculations.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="model-conversion">Model conversion<a href="#model-conversion" class="hash-link" aria-label="Direct link to Model conversion" title="Direct link to Model conversion">​</a></h3>
<p>The model conversion phase will convert the floating-point model to a D-Robotics heterogeneous model, and after this phase, you will have a model that can run on the D-Robotics processor.</p>
<p>Before performing the conversion, please make sure that the validation model process has been successfully passed.</p>
<p>The model conversion is completed using the <code>hb_mapper makertbin</code> tool, during which important processes such as model optimization and calibration quantization will be performed. Calibration requires preparing calibration data according to the model preprocessing requirements.</p>
<p>In order to facilitate your comprehensive understanding of model conversion, this section will introduce calibration data preparation, conversion tool usage, conversion internal process interpretation, conversion result interpretation, and conversion output interpretation in order.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="preparing-calibration-data">Preparing Calibration Data<a href="#preparing-calibration-data" class="hash-link" aria-label="Direct link to Preparing Calibration Data" title="Direct link to Preparing Calibration Data">​</a></h4>
<p>During the model conversion, the calibration stage will require about 100 calibration input samples, and each sample is an independent data file. To ensure the accuracy of the converted model, we hope that these calibration samples come from the training set or validation set you used to train the model, and not use very rare abnormal samples, such as solid color images, images without any detection or classification objects, etc.</p>
<p>In the conversion configuration file, the &quot;preprocess_on&quot; parameter corresponds to two different preprocessing sample requirements under the enabled and disabled states, respectively. (For detailed configuration of parameters, please refer to the relevant instructions in the calibration parameter group below)</p>
<p>When &quot;preprocess_on&quot; is disabled, you need to perform the same preprocessing of the samples taken from the training set/validation set as the model inference before the calibration. The calibrated samples after processing will have the same data type (&quot;input_type_train&quot;), size (&quot;input_shape&quot;), and layout (&quot;input_layout_train&quot;) as the original model.</p>
<p>For models with featuremap inputs, you can use the &quot;numpy.tofile&quot; command to save the data as a float32 format binary file. The tool chain will read it based on the &quot;numpy.fromfile&quot; command during calibration.</p>
<p>For example, for the original floating-point model trained using ImageNet for classification, which has only one input node, the input information is described as follows:</p>
<ul>
<li>Input type: BGR</li>
<li>Input layout: NCHW</li>
<li>Input size: 1x3x224x224</li>
</ul>
<p>The data preprocessing for model inference using the validation set is as follows:</p>
<ol>
<li>Scale the image to maintain the aspect ratio, with the shorter side scaled to 256.</li>
<li>Use the &quot;center_crop&quot; method to obtain a 224x224 size image.</li>
<li>Subtract the mean by channel.</li>
<li>Multiply the data by a scale factor.</li>
</ol>
<p>The sample processing code for the example model mentioned above is as follows:</p>
<p>To avoid excessive code length, the implementation code of various simple transformers is not included. For specific usage, please refer to the chapter &quot;Transformer Usage&quot; in the <a href="/rdk_doc/en/FAQ/toolchain#transposetransformer"><strong>Toolchain</strong></a> section.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Tips</div><div class="admonitionContent_BuS1"><p>It is recommended to refer to the preprocessing steps of the sample models in the D-Robotics model conversion &quot;horizon_model_convert_sample&quot; sample package, such as caffe and onnx models: &quot;02_preprocess.sh&quot; and &quot;preprocess.py&quot;.</p></div></div>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># This example uses skimage, there may be differences if using opencv</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># It is worth noting that the transformers do not reflect the mean subtraction and scale multiplication operations</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># The mean and scale operations have been integrated into the model, please refer to the norm_type/mean_value/scale_value configuration below</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">data_transformer</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    transformers </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Scale the long side and short side to maintain the aspect ratio, with the shorter side scaled to 256</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ShortSideResizeTransformer</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">short_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">256</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Use CenterCrop to obtain a 224x224 image</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    CenterCropTransformer</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">crop_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">224</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Convert NHWC layout from skimage to NCHW layout required by the model</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">HWC2CHWTransformer</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Convert channel order from RGB to BGR required by the model</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">RGB2BGRTransformer</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Adjust the value range from [0.0, 1.0] to the value range required by the model</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">ScaleTransformer</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">scale_value</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">255</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> transformers</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># src_image represents the original image in the calibration set</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># dst_file represents the file name for storing the final calibration sample data</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">convert_image</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">src_image</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dst_file</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> transformers</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    image </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> skimage</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">img_as_float</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">skimage</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">io</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">imread</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">src_file</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> trans </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> transformers</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        image </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> trans</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">image</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># The input_type_train specified by the model is UINT8 for BGR value type</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    image </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> image</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">astype</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">uint8</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Store the calibration sample data in binary format to the data file</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    image</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">tofile</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">dst_file</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> __name__ </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;__main__&#x27;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># The following represents the original calibration image set (pseudo code)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    src_images </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">&#x27;ILSVRC2012_val_00000001.JPEG&#x27;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># The following represents the final calibration file name (no restrictions on the file extension) (pseudo code)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># calibration_data_bgr_f32 is cal_data_dir specified in your configuration file</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    dst_files </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">&#x27;./calibration_data_bgr_f32/ILSVRC2012_val_00000001.bgr&#x27;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    transformers </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> data_transformer</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> src_image</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dst_file </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">zip</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">src_images</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dst_files</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        convert_image</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">src_image</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dst_file</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> transformers</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>When <code>preprocess_on</code> is enabled, the calibration samples can be in any image format supported by skimage. The conversion tool will resize these images to the size required by the input node of the model, and use the result as the input for calibration. This operation is simple but does not guarantee the accuracy of quantization, so we strongly recommend that you use the method of disabling <code>preprocess_on</code>.</p><div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><p>Please note that the input_shape parameter in the YAML file specifies the input data size of the original float model. If it is a dynamic input model, you can set the input size after conversion using this parameter, and the size of the calibration data should be the same as input_shape.</p><p>For example: If the shape of the input node of the original float model is ?x3x224x224 (where &quot;?&quot; indicates a placeholder, i.e., the first dimension of the model is a dynamic input), and the input_shape in the conversion configuration file is set to 8x3x224x224, then the size of each calibration data that the user needs to prepare is 8x3x224x224.
(Note that models with input shapes where the first dimension is not 1 do not support modifying the model batch information through the input_batch parameter.)</p></div></div></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="makertbin">Convert the Model Using the hb_mapper makertbin Tool<a href="#makertbin" class="hash-link" aria-label="Direct link to Convert the Model Using the hb_mapper makertbin Tool" title="Direct link to Convert the Model Using the hb_mapper makertbin Tool">​</a></h4>
<p>hb_mapper makertbin provides two modes, with and without the <code>fast-perf</code> mode enabled.When the &quot;fast-perf&quot; mode is enabled, it will generate a bin model that can run at the highest performance on the board during the conversion process. The tool mainly performs the following operations:</p>
<ul>
<li>
<p>Run BPU executable operators on the BPU as much as possible (if using &quot;RDK Ultra&quot;, you can specify the operators running on the BPU through the node_info parameter in the yaml file. &quot;RDK X3&quot; is automatically optimized and cannot specify operators through the yaml configuration file).</p>
</li>
<li>
<p>Delete CPU operators at the beginning and end of the model that cannot be deleted, including: Quantize/Dequantize, Transpose, Cast, Reshape, etc.</p>
</li>
<li>
<p>Compile the model with the highest performance optimization level O3.</p>
</li>
</ul>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Tips</div><div class="admonitionContent_BuS1"><p>It is recommended to refer to the script methods of the example models in the D-Robotics Model Conversion &quot;horizon_model_convert_sample&quot; package, such as caffe and onnx example models: &quot;03_build_X3.sh&quot; or &quot;03_build_Ultra.sh&quot;.</p></div></div>
<p>The usage of the hb_mapper makertbin command is as follows:</p>
<p>Without enabling the &quot;fast-perf&quot; mode:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  hb_mapper makertbin --config ${config_file}  \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                      --model-type  ${model_type}</span><br></span></code></pre></div></div>
<p>With the &quot;fast-perf&quot; mode enabled:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  hb_mapper makertbin --fast-perf --model ${caffe_model/onnx_model`}` --model-type $`{`model_type`}` \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                      --proto ${caffe_proto} \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                      --march ${march}</span><br></span></code></pre></div></div>
<p>Explanation of hb_mapper makertbin parameters:</p>
<p>--help<br>
Display help information and exit.</p>
<p>-c, --config<br>
The configuration file for model compilation, in yaml format. The file name uses the .yaml suffix. The complete configuration file template is as follows.</p>
<p>--model-type<br>
Used to specify the model type for conversion input, currently supports setting &quot;caffe&quot; or &quot;onnx&quot;.</p>
<p>--fast-perf<br>
Enable the fast-perf mode. After enabling this mode, it will generate a bin model that can run at the highest performance on the board during the conversion process, making it convenient for you to evaluate the model&#x27;s performance.</p>
<p>If you enable the fast-perf mode, you also need to configure the following:</p>
<p>--model<br>
Caffe or ONNX floating-point model file.--proto
Used to specify the prototxt file of the Caffe model.</p>
<p>--march
Microarchitecture of BPU. Set it to &quot;bernoulli2&quot; if using &quot;RDK X3&quot;, and set it to &quot;bayes&quot; if using &quot;RDK Ultra&quot;.</p>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><ul>
<li>
<p>For &quot;RDK X3 yaml configuration file&quot;, fill in the template file <a href="/rdk_doc/en/FAQ/toolchain#rdk_x3_caffe_yaml_template"><strong>RDK X3 Caffe model quantization yaml template</strong></a> or <a href="/rdk_doc/en/FAQ/toolchain#rdk_x3_onnx_yaml_template"><strong>RDK X3 ONNX model quantization yaml template</strong></a> directly.</p>
</li>
<li>
<p>For &quot;RDK Ultra yaml configuration file&quot;, fill in the template file <a href="/rdk_doc/en/FAQ/toolchain#rdk_ultra_caffe_yaml_template"><strong>RDK Ultra Caffe model quantization yaml template</strong></a> or <a href="/rdk_doc/en/FAQ/toolchain#rdk_ultra_onnx_yaml_template"><strong>RDK Ultra ONNX model quantization yaml template</strong></a> directly.</p>
</li>
<li>
<p>If the hb_mapper makertbin step terminates abnormally or shows an error message, it means that the model conversion has failed. Please check the error message and modification suggestions in the terminal printout or in the <code>hb_mapper_makertbin.log</code> log file generated in the current path. You can find the solution for the error in the <a href="/rdk_doc/en/FAQ/toolchain#model_convert_errors_and_solutions"><strong>Model Quantization Errors and Solutions</strong></a> section. If the problem cannot be solved after these steps, please contact the D-Robotics technical support team or submit your question in the <a href="https://developer.d-robotics.cc/" target="_blank" rel="noopener noreferrer"><strong>D-Robotics Official Technical Community</strong></a>. We will provide support within 24 hours.</p>
</li>
</ul></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="yaml_config">Explanation of Parameters in Model Conversion YAML Configuration<a href="#yaml_config" class="hash-link" aria-label="Direct link to Explanation of Parameters in Model Conversion YAML Configuration" title="Direct link to Explanation of Parameters in Model Conversion YAML Configuration">​</a></h4>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><p>Either a Caffe model or an ONNX model can be used, that is, either <code>caffe_model</code> + <code>prototxt</code> or <code>onnx_model</code> can be chosen.
In other words, either a Caffe model or an ONNX model can be used.</p></div></div>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  # Model parameter group</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  model_parameters:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # Original Caffe floating-point model description file</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    prototxt: &#x27;***.prototxt&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # Original Caffe floating-point model data model file</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    caffe_model: &#x27;****.caffemodel&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # Original ONNX floating-point model file</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    onnx_model: &#x27;****.onnx&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # Target processor architecture for conversion, keep the default value, bernoulli2 for D-Robotics RDK X3 and bayes for RDK Ultra</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    march: &#x27;bernoulli2&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # Prefix of the model file for execution on the board after conversion</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    output_model_file_prefix: &#x27;mobilenetv1&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # Directory for storing the output of the model conversion</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    working_dir: &#x27;./model_output_dir&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # Specify whether the converted hybrid heterogeneous model retains the ability to output intermediate results of each layer, keep the default value</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    layer_out_dump: False</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # Specify the output nodes of the model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    output_nodes: `{`OP_name`}`</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # Batch deletion of nodes of a certain typeremove_node_type: Dequantize</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Remove specified node by name</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  remove_node_name: `{`OP_name`}`</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Input parameters group</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">input_parameters:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Name of the input node in the original floating-point model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  input_name: &quot;data&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Data format of the input to the original floating-point model (same number and order as input_name)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  input_type_train: &#x27;bgr&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Data layout of the input to the original floating-point model (same number and order as input_name)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  input_layout_train: &#x27;NCHW&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Shape of the input to the original floating-point model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  input_shape: &#x27;1x3x224x224&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Batch size given to the network during execution, default value is 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  input_batch: 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Pre-processing method applied to the input data in the model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  norm_type: &#x27;data_mean_and_scale&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Mean value subtracted from the image in the pre-processing method, if channel means, values must be separated by spaces</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  mean_value: &#x27;103.94 116.78 123.68&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Scale value applied to the image in the pre-processing method, if channel scales, values must be separated by spaces</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  scale_value: &#x27;0.017&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Data format that the transformed hybrid heterogeneous model needs to adapt to (same number and order as input_name)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  input_type_rt: &#x27;yuv444&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Special format of the input data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  input_space_and_range: &#x27;regular&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Data layout that the transformed hybrid heterogeneous model needs to adapt to (same number and order as input_name), not required if input_type_rt is nv12</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  input_layout_rt: &#x27;NHWC&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Calibration parameters group</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">calibration_parameters:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Directory where the calibration samples for model calibration are stored</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  cal_data_dir: &#x27;./calibration_data&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Data storage type of the binary files for calibration data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  cal_data_type: &#x27;float32&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Enable automatic processing of calibration image samples (using skimage read and resize to input node size)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  #preprocess_on: False# Algorithm type for calibration, with default calibration algorithm as the first priority</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  calibration_type: &#x27;default&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Parameters for max calibration mode</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # max_percentile: 1.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Force the OP to run on CPU, generally not needed, can be enabled during model accuracy tuning phase for precision optimization</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # run_on_cpu:  {OP_name}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Force the OP to run on BPU, generally not needed, can be enabled during model performance tuning phase for performance optimization</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # run_on_bpu:  {OP_name}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Specify whether to calibrate for each channel</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # per_channel: False</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Specify the data precision for output nodes</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # optimization: set_model_output_int8</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Compiler parameter group</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">compiler_parameters:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Compilation strategy selection</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  compile_mode: &#x27;latency&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Whether to enable debug information for compilation, keep the default False</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  debug: False</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Number of cores for model execution</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  core_num: 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Optimization level for model compilation, keep the default O3</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  optimize_level: &#x27;O3&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Specify the input data source as &#x27;pyramid&#x27; for the input named &#x27;data&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  #input_source: {&quot;data&quot;: &quot;pyramid&quot;}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Specify the maximum continuous execution time for each function call in the model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  #max_time_per_fc: 1000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Specify the number of processes during model compilation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  #jobs: 8</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # This parameter group does not need to be configured, only enabled when there are custom CPU operators</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  #custom_op: </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Calibration method for custom OP, recommend using registration method</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  #custom_op_method: register</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Implementation files for custom OP, multiple files can be separated with &quot;;&quot;, this file can be generated from a template, see the custom OP documentation for details</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  #op_register_files: sample_custom.py# The folder where the custom OP implementation file is located, please use a relative path</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  #custom_op_dir: ./custom_op</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre></div></div>
<p>The configuration file mainly includes four parameter groups: model parameter group, input information parameter group, calibration parameter group, and compilation parameter group.</p>
<p>In your configuration file, all four parameter groups need to exist, and specific parameters can be optional or mandatory. Optional parameters can be omitted.</p>
<p>The specific format for setting parameters is: <code>param_name:  &#x27;param_value&#x27;</code> ;
If there are multiple values for a parameter, separate each value with the <code>&#x27;;&#x27;</code> symbol: <code>param_name:  &#x27;param_value1; param_value2; param_value3&#x27;</code> ;
For specific configuration methods, please refer to: <code>run_on_cpu: &#x27;conv_0; conv_1; conv12&#x27;</code> .</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Tip</div><div class="admonitionContent_BuS1"><ul>
<li>When the model is a multi-input model, it is recommended to explicitly specify optional parameters such as <code>input_name</code> and <code>input_shape</code> to avoid errors in parameter correspondence order.</li>
<li>When configuring the <code>march</code> as bayes, which means performing RDK Ultra model conversion, if you configure the optimization level <code>optimize_level</code> as O3, hb_mapper makerbin will automatically provide caching capabilities. That is, when you use hb_mapper makerbin to compile the model for the first time, it will automatically create a cache file. In subsequent compilations with the same working directory, this file will be automatically called, reducing your compilation time.</li>
</ul></div></div>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Caution</div><div class="admonitionContent_BuS1"><ul>
<li>Please note that if you set <code>input_type_rt</code> to <code>nv12</code> or <code>yuv444</code>, the input size of the model cannot have odd numbers.</li>
<li>Please note that currently RDK X3 does not support the combination of <code>input_type_rt</code> as <code>yuv444</code> and <code>input_layout_rt</code> as <code>NCHW</code>.</li>
<li>After the model conversion is successful, if an OP that meets the constraints of BPU operators still runs on the CPU, the main reason is that the OP belongs to the passive quantization OP. For information about passive quantization, please read the section <a href="https://developer.d-robotics.cc/forumDetail/118364000835765793" target="_blank" rel="noopener noreferrer"><strong>Active and Passive Quantization Logic in the Algorithm Toolchain</strong></a>.</li>
</ul></div></div>
<p>The following is a description of the specific parameter information. There will be many parameters, and we will introduce them in the order of the parameter groups mentioned above.</p>
<ul>
<li>
<h6 class="anchor anchorWithStickyNavbar_LWe7" id="model-parameter-group">Model Parameter Group<a href="#model-parameter-group" class="hash-link" aria-label="Direct link to Model Parameter Group" title="Direct link to Model Parameter Group">​</a></h6>
</li>
</ul>
<table><thead><tr><th>Parameter Name</th><th>Description</th><th>Value Range</th><th>Optional/Required</th></tr></thead><tbody><tr><td><code>prototxt</code></td><td><strong>Purpose:</strong> Specifies the filename of the Caffe float model prototxt file.<br><strong>Description:</strong> Mandatory for <code>hb_mapper makertbin</code> with <code>model-type</code> set to <code>caffe</code>.</td><td>N/A</td><td>Optional</td></tr><tr><td><code>caffe_model</code></td><td><strong>Purpose:</strong> Specifies the filename of the Caffe float model caffemodel file.<br><strong>Description:</strong> Mandatory for <code>hb_mapper makertbin</code> with <code>model-type</code> set to <code>caffe</code>.</td><td>N/A</td><td>Optional</td></tr><tr><td><code>onnx_model</code></td><td><strong>Purpose:</strong> Specifies the filename of the ONNX float model onnx file.<br><strong>Description:</strong> Mandatory for <code>hb_mapper makertbin</code> with <code>model-type</code> set to <code>onnx</code>.</td><td>N/A</td><td>Optional</td></tr><tr><td><code>march</code></td><td><strong>Purpose:</strong> Specifies the platform architecture supported by the mixed-heterogeneous model to be produced.<br><strong>Description:</strong> Two available options correspond to the BPU micro-framework for RDK X3 and RDK Ultra. Choose based on your platform.</td><td><code>bernoulli2</code> or <code>bayes</code></td><td>Required</td></tr><tr><td><code>output_model_file_prefix</code></td><td><strong>Purpose:</strong> Specifies the prefix for the converted mixed-heterogeneous model&#x27;s output file name.<br><strong>Description:</strong> Prefix for the output integer model file name.</td><td>N/A</td><td>Required</td></tr><tr><td><code>working_dir</code></td><td><strong>Purpose:</strong> Specifies the directory where the model conversion output will be stored.<br><strong>Description:</strong> If the directory does not exist, the tool will automatically create it.</td><td>N/A</td><td>Optional (default: <code>model_output</code>)</td></tr><tr><td><code>layer_out_dump</code></td><td><strong>Purpose:</strong> Enables the ability to retain intermediate layer values in the mixed-heterogeneous model.<br><strong>Description:</strong> Intermediate layer values are used for debugging purposes. Disable this in normal scenarios.</td><td><code>True</code> or <code>False</code></td><td>Optional (default: <code>False</code>)</td></tr><tr><td><code>output_nodes</code></td><td><strong>Purpose:</strong> Specifies the model&#x27;s output nodes.<br><strong>Description:</strong> Generally, the conversion tool automatically identifies the model&#x27;s output nodes. This parameter is used to support specifying some intermediate layers as outputs. Provide specific node names, following the same format as the <code>param_value</code> description. Note that setting this parameter prevents the tool from automatically detecting outputs; the nodes you specify become the entire output.</td><td>N/A</td><td>Optional</td></tr><tr><td><code>remove_node_type</code></td><td><strong>Purpose:</strong> Sets the type of nodes to remove.<br><strong>Description:</strong> Hidden parameter, not setting or leaving blank won&#x27;t affect the model conversion process. This parameter is used to support specifying node types to delete. Removed nodes must appear at the beginning or end of the model, connected to inputs or outputs. Caution: Nodes will be deleted in order, dynamically updating the model structure. The tool checks if a node is at an input or output before deletion. Order matters.</td><td>&quot;Quantize&quot;, &quot;Transpose&quot;, &quot;Dequantize&quot;, &quot;Cast&quot;, &quot;Reshape&quot;. Separate by &quot;;&quot;.</td><td>Optional</td></tr><tr><td><code>remove_node_name</code></td><td><strong>Purpose:</strong> Sets the name of nodes to remove.<br><strong>Description:</strong> Hidden parameter, not setting or leaving blank won&#x27;t affect the model conversion process. This parameter is used to support specifying node names to delete. Removed nodes must appear at the beginning or end of the model, connected to inputs or outputs. Caution: Nodes will be deleted in order, dynamically updating the model structure. The tool checks if a node is at an input or output before deletion. Order matters.</td><td>N/A</td><td>Optional</td></tr><tr><td><code>set_node_data_type</code></td><td><strong>Purpose:</strong> Configures the output data type of a specific op as int16, only supported for <strong>RDK Ultra configuration!</strong><br><strong>Description:</strong> In the model conversion process, most ops default to int8 for input and output data types. This parameter allows you to specify the output data type of a specific op as int16 under certain constraints. See the <a href="#int16_config">int16 configuration details</a> for more information.<br><strong>Note:</strong> This functionality has been merged into the <code>node_info</code> parameter, which will be deprecated in future versions.</td><td>Supported operators listed in the <a href="/rdk_doc/en/Advanced_development/toolchain_development/intermediate/supported_op_list">model operator support list</a> for RDK Ultra.</td><td>Optional</td></tr><tr><td><code>debug_mode</code></td><td><strong>Purpose:</strong> Saves calibration data for precision debugging analysis.<br><strong>Description:</strong> This parameter saves calibration data for precision debugging analysis in .npy format. This data can be directly loaded into the model for inference. If not set, you can save the data yourself and use the precision debugging tool for analysis.</td><td><code>&quot;dump_calibration_data&quot;</code></td><td>Optional</td></tr><tr><td><code>node_info</code></td><td><strong>Purpose:</strong> Supports configuring the input and output data types of specific ops as int16, and forces certain ops to run on CPU or BPU. Only supported for <strong>RDK Ultra configuration!</strong><br><strong>Description:</strong> To reduce YAML parameters, we&#x27;ve combined the capabilities of <code>set_node_data_type</code>, <code>run_on_cpu</code>, and <code>run_on_bpu</code> into this parameter and expanded it to support configuring the input data type of specific ops as int16.<br><strong>Usage of <code>node_info</code>:</strong><br>- Run an op on BPU/CPU (example with BPU):<br>node_info: <code>{</code><br>&quot;node_name&quot;: <code>{</code><br>&quot;ON&quot;: &quot;BPU&quot;,<br><code>}</code><br><code>}</code><br>- Run an op on BPU and configure its input and output data types:<br>node_info: <code>{</code><br>&quot;node_name&quot;: <code>{</code><br>&quot;ON&quot;: &quot;BPU&quot;,<br>&quot;InputType&quot;: &quot;int16&quot;,<br>&quot;OutputType&quot;: &quot;int16&quot;<br><code>}</code><br><code>}</code><br>- Run on multiple operators:<br> node_info:<br><code>{&quot;/model.0/conv/Conv&quot;: {&quot;ON&quot;: &quot;BPU&quot;,&quot;InputType&quot;: &quot;int16&quot;,&quot;OutputType&quot;: &quot;int16&quot;},</code><br><code>&quot;/model.0/act/Mul&quot;: {&quot;ON&quot;: &quot;BPU&quot;,&quot;InputType&quot;: &quot;int16&quot;,&quot;OutputType&quot;: &quot;int16&quot;},</code><br><code>&quot;/model.2/Concat&quot;: {&quot;ON&quot;: &quot;BPU&quot;,&quot;InputType&quot;: &quot;int16&quot;,&quot;OutputType&quot;: &quot;int16&quot;}}</code><br>* <code>InputType</code>: &#x27;int16&#x27; applies to all inputs. For specifying a particular input&#x27;s data type, append a number, e.g., <code>&#x27;InputType0&#x27;: &#x27;int16&#x27;</code> for the first input, <code>&#x27;InputType1&#x27;: &#x27;int16&#x27;</code> for the second input.<br>* <code>OutputType</code> doesn&#x27;t support specifying a particular output, applying to all outputs. It doesn&#x27;t support individual types like <code>OutputType0</code> or <code>OutputType1</code>.<br><strong>Value Range:</strong> Refer to the <a href="/rdk_doc/en/Advanced_development/toolchain_development/intermediate/supported_op_list">model operator support list</a> for RDK Ultra for supported int16 ops and those that can run on CPU or BPU.<br><strong>Default Configuration:</strong> None</td><td>Optional</td><td></td></tr></tbody></table>
<ul>
<li>
<h6 class="anchor anchorWithStickyNavbar_LWe7" id="input-parameters-group">Input Parameters Group<a href="#input-parameters-group" class="hash-link" aria-label="Direct link to Input Parameters Group" title="Direct link to Input Parameters Group">​</a></h6>
</li>
</ul>
<table><thead><tr><th>Parameter Name</th><th>Description</th><th>Value Range</th><th>Optional/Required</th></tr></thead><tbody><tr><td><code>input_name</code></td><td><strong>Purpose</strong>: Specifies the input node name of the original floating-point model.<br><strong>Description</strong>: Not required when the floating-point model has only one input node. Must be configured for models with multiple input nodes to ensure accurate order of type and calibration data inputs. Multiple values can be set as described for <code>param_value</code>.</td><td>N/A</td><td>Optional</td></tr><tr><td><code>input_type_train</code></td><td><strong>Purpose</strong>: Specifies the input data type of the original floating-point model.<br><strong>Description</strong>: Each input node must have a configured data type. For models with multiple input nodes, the order should match that in <code>input_name</code>. Multiple values can be set as described for <code>param_value</code>. Data types available: refer to the explanation in the &quot;Conversion Internal Process&quot; section.</td><td>Values: <code>rgb</code>, <code>bgr</code>, <code>yuv444</code>, <code>gray</code>, <code>featuremap</code></td><td>Required</td></tr><tr><td><code>input_layout_train</code></td><td><strong>Purpose</strong>: Specifies the input data layout of the original floating-point model.<br><strong>Description</strong>: Each input node requires a specific layout, which must match the model&#x27;s original layout. Order should align with <code>input_name</code>. Multiple values can be set as described for <code>param_value</code>. Learn more about layouts in the &quot;Conversion Internal Process&quot; section.</td><td>Values: NHWC, NCHW</td><td>Required</td></tr><tr><td><code>input_type_rt</code></td><td><strong>Purpose</strong>: The input format needed for the converted heterogeneous model.<br><strong>Description</strong>: Specifies the desired input format, not necessarily matching the original model&#x27;s format, but important for the platform to feed data into the model. One type per input node, with order matching <code>input_name</code>. Multiple values can be set as described for <code>param_value</code>. Data types available: refer to the explanation in the &quot;Conversion Internal Process&quot; section.</td><td>Values: <code>rgb</code>, <code>bgr</code>, <code>yuv444</code>, <code>nv12</code>, <code>gray</code>, <code>featuremap</code></td><td>Required</td></tr><tr><td><code>input_layout_rt</code></td><td><strong>Purpose</strong>: The input data layout for the converted heterogeneous model.<br><strong>Description</strong>: Specifies the desired input layout for each node, which can differ from the original model. For NV12 input_type_rt, this parameter is unnecessary. Order should match <code>input_name</code>. Multiple values can be set as described for <code>param_value</code>. Learn more about layouts in the &quot;Conversion Internal Process&quot; section.</td><td>Values: NCHW, NHWC</td><td>Optional (if <code>input_type_rt</code> is NV12)</td></tr><tr><td><code>input_space_and_range</code></td><td><strong>Purpose</strong>: Specifies the special format of input data, particularly for ISP outputs in yuv420 format.<br><strong>Description</strong>: Used for adapting to different ISP formats, valid only if <code>input_type_rt</code> is set to <code>nv12</code>. Choices: <code>regular</code> for common yuv420, <code>bt601_video</code> for another video standard. Keep as <code>regular</code> unless specifically needed.</td><td>Values: <code>regular</code>, <code>bt601_video</code></td><td>Optional</td></tr><tr><td><code>input_shape</code></td><td><strong>Purpose</strong>: Specifies the dimensions of the input data for the original floating-point model.<br><strong>Description</strong>: Dimensions should be separated by <code>x</code>, e.g., <code>1x3x224x224</code>. Can be omitted for single-input models with the tool automatically reading size information from the model file. Order should match <code>input_name</code>. Multiple values can be set as described for <code>param_value</code>.</td><td>N/A</td><td>Optional</td></tr><tr><td><code>input_batch</code></td><td><strong>Purpose</strong>: The number of batches for the converted heterogeneous model to adapt to.<br><strong>Description</strong>: The batch size for the converted heterogeneous bin model, not affecting the ONNX model&#x27;s batch size. Defaults to 1 if not specified. Only applicable for single-input models where the first dimension of <code>input_shape</code> is 1.</td><td>Range: <code>1-128</code></td><td>Optional</td></tr><tr><td><code>norm_type</code></td><td><strong>Purpose</strong>: The preprocessing method added to the model&#x27;s input data.<br><strong>Description</strong>: <code>no_preprocess</code> means no preprocessing; <code>data_mean</code> for mean subtraction; <code>data_scale</code> for scaling; <code>data_mean_and_scale</code> for both mean subtraction and scaling. Must be consistent with the order of <code>input_name</code>. Multiple values can be set as described for <code>param_value</code>. See the &quot;Conversion Internal Process&quot; section for impact.</td><td>Values: <code>data_mean_and_scale</code>, <code>data_mean</code>, <code>data_scale</code>, <code>no_preprocess</code></td><td>Required</td></tr><tr><td><code>mean_value</code></td><td><strong>Purpose</strong>: The image mean value for the preprocessing method.<br><strong>Description</strong>: Required if <code>norm_type</code> includes <code>data_mean_and_scale</code> or <code>data_mean</code>. Two configuration options: a single value for all channels or channel-specific values (separated by spaces). Channel count should match <code>norm_type</code> nodes. Set to <code>&#x27;None&#x27;</code> for nodes without mean processing. Multiple values can be set as described for <code>param_value</code>.</td><td>N/A</td><td>Optional</td></tr><tr><td><code>scale_value</code></td><td><strong>Purpose</strong>: The scale coefficient for the preprocessing method.<br><strong>Description</strong>: Required if <code>norm_type</code> includes <code>data_mean_and_scale</code> or <code>data_scale</code>. Similar to <code>mean_value</code>, two configurations are allowed: a single value for all channels or channel-specific values (separated by spaces). Channel count should match <code>norm_type</code> nodes. Set to <code>&#x27;None&#x27;</code> for nodes without scale processing. Multiple values can be set as described for <code>param_value</code>.</td><td>N/A</td><td>Optional</td></tr></tbody></table>
<ul>
<li>
<h6 class="anchor anchorWithStickyNavbar_LWe7" id="calibration-parameter-group">Calibration Parameter Group<a href="#calibration-parameter-group" class="hash-link" aria-label="Direct link to Calibration Parameter Group" title="Direct link to Calibration Parameter Group">​</a></h6>
</li>
</ul>
<table><thead><tr><th>Parameter Name</th><th>Description</th><th>Value Range</th><th>Optional/Required</th></tr></thead><tbody><tr><td><code>cal_data_dir</code></td><td>Specifies the directory containing calibration samples for model calibration. <br><strong>Description</strong>: The data in this directory should adhere to the input configuration requirements. Please refer to the section on <a href="https://..." target="_blank" rel="noopener noreferrer">Preparing Calibration Data</a> for more details. When configuring multiple input nodes, the order of the specified nodes must strictly match that in <code>input_name</code>. Multiple value configurations can be done as described earlier for <code>param_value</code>. For <code>calibration_type</code> of <code>load</code>, <code>skip</code>, this parameter is not needed. Note: To facilitate your use, if no <code>cal_data_type</code> configuration is found, we will infer the data type based on the file extension. If the file extension ends with <code>_f32</code>, it will be considered float32; otherwise, uint8. However, we strongly recommend constraining the data type using the <code>cal_data_type</code> parameter.</td><td>N/A</td><td>Optional</td></tr><tr><td><code>cal_data_type</code></td><td>Specifies the binary file data storage type for calibration data.<br><strong>Description</strong>: The data storage type used by the model during calibration. If not specified, the tool will determine the type based on the file name suffix.</td><td><code>float32</code>, <code>uint8</code></td><td>Optional</td></tr><tr><td><code>preprocess_on</code></td><td>Enables automatic preprocessing of image calibration samples.<br><strong>Description</strong>: This option is only applicable to models with 4D image inputs. Do not enable this for non-4D models. When enabled, the tool reads jpg/bmp/png files in the <code>cal_data_dir</code> and resizes them to the required dimensions for input nodes. It is recommended to keep this parameter disabled to ensure calibration accuracy. Refer to the <a href="https://..." target="_blank" rel="noopener noreferrer">Preparing Calibration Data</a> section for more information on its impact.</td><td><code>True</code>, <code>False</code></td><td>Optional</td></tr><tr><td><code>calibration_type</code></td><td>Calibration algorithm type to use.<br><strong>Description</strong>: Both <code>kl</code> and <code>max</code> are public calibration quantization algorithms, whose basic principles can be found in online resources. When using <code>load</code>, the QAT model must be exported using a plugin. <code>mix</code> is an integrated search strategy that automatically determines sensitive quantization nodes and selects the best method at the node granularity, ultimately constructing a calibration combination that leverages the advantages of multiple methods. <code>default</code> is an automated search strategy that attempts to find a relatively better combination of calibration parameters from a series. We suggest starting with <code>default</code>. If the final accuracy does not meet expectations, refer to the <a href="https://..." target="_blank" rel="noopener noreferrer">Precision Tuning</a> section for suggested parameter adjustments. If you just want to verify the model performance without accuracy requirements, try the <code>skip</code> mode, which uses random numbers for calibration and does not require calibration data, suitable for initial model structure validation. Note: Using the <code>skip</code> mode results in models calibrated with random numbers, which are not suitable for accuracy validation.</td><td><code>default</code>, <code>mix</code>, <code>kl</code>, <code>max</code>, <code>load</code>, <code>skip</code></td><td>Required</td></tr><tr><td><code>max_percentile</code></td><td>Parameter for the <code>max</code> calibration method, used to adjust the cutoff point for <code>max</code> calibration.<br><strong>Description</strong>: Only valid when <code>calibration_type</code> is set to <code>max</code>. Common options include: 0.99999/0.99995/0.99990/0.99950/0.99900. Start with <code>calibration_type</code> set to <code>default</code>, and adjust this parameter if the final accuracy is unsatisfactory, as advised in the <a href="https://..." target="_blank" rel="noopener noreferrer">Precision Tuning</a> section.</td><td><code>0.0</code> - <code>1.0</code></td><td>Optional</td></tr><tr><td><code>per_channel</code></td><td>Controls whether to calibrate each channel individually within a featuremap.<br><strong>Description</strong>: Effective when <code>calibration_type</code> is not set to <code>default</code>. Start with <code>default</code> and adjust this parameter if necessary, as suggested in the <a href="https://..." target="_blank" rel="noopener noreferrer">Precision Tuning</a> section.</td><td><code>True</code>, <code>False</code></td><td>Optional</td></tr><tr><td><code>run_on_cpu</code></td><td>Forces operators to run on CPU.<br><strong>Description</strong>: Although CPU performance is inferior to BPU, it provides float precision calculations. Specify this parameter if you&#x27;re certain that some operators need to run on CPU. Set values to specific node names in your model, following the same configuration method as described earlier for <code>param_value</code>. <strong>Note</strong>: In RDK Ultra, this parameter functionality has been merged into the <code>node_info</code> parameter and is planned to be deprecated in future versions. It continues to be available in RDK X3.</td><td>N/A</td><td>Optional</td></tr><tr><td><code>run_on_bpu</code></td><td>Forces an operator to run on BPU.<br><strong>Description</strong>: To maintain the accuracy of the quantized model, occasionally, the conversion tool may place some operators that can run on BPU on CPU. If you have higher performance requirements and are willing to accept slightly more quantization loss, you can explicitly specify that an operator runs on BPU. Set values to specific node names in your model, following the same configuration method as described earlier for <code>param_value</code>. <strong>Note</strong>: In RDK Ultra, this parameter functionality has been merged into the <code>node_info</code> parameter and is planned to be deprecated in future versions. It continues to be available in RDK X3.</td><td>N/A</td><td>Optional</td></tr><tr><td><code>optimization</code></td><td>Sets the model output format to int8 or int16.<br><strong>Description</strong>: If set to <code>set_model_output_int8</code>, the model will output in low-precision int8 format; if set to <code>set_model_output_int16</code>, the model will output in low-precision int16 format. <strong>Note</strong>: RDK X3 only supports <code>set_model_output_int8</code>, while RDK Ultra supports both <code>set_model_output_int8</code> and <code>set_model_output_int16</code>.</td><td><code>set_model_output_int8</code>, <code>set_model_output_int16</code></td><td>Optional</td></tr></tbody></table>
<ul>
<li>
<h6 class="anchor anchorWithStickyNavbar_LWe7" id="compiler_parameters">Compiler Parameters <code>{</code>#compiler_parameters<code>}</code><a href="#compiler_parameters" class="hash-link" aria-label="Direct link to compiler_parameters" title="Direct link to compiler_parameters">​</a></h6>
</li>
</ul>
<table><thead><tr><th>Parameter Name</th><th>Description</th><th>Value Range</th><th>Optional/Required</th></tr></thead><tbody><tr><td><code>compile_mode</code></td><td><strong>Purpose</strong>: Select the compilation strategy.<br><strong>Description</strong>: Choose between <code>latency</code> for inference time optimization or <code>bandwidth</code> for DDR access bandwidth optimization. For models without significant bandwidth exceedance, use the <code>latency</code> strategy is recommended.</td><td><strong>Value Range</strong>: <code>latency</code>, <code>bandwidth</code>.<br> <strong>Default</strong>: <code>latency</code>.</td><td>Required</td></tr><tr><td><code>debug</code></td><td><strong>Purpose</strong>: Enable debug information in the compilation process.<br><strong>Description</strong>: Enabling this parameter saves performance analysis results in the model, allowing you to view layer-wise BPU operator performance (including compute, compute time, and data movement time) in the generated static performance assessment files. It is recommended to keep it disabled by default.</td><td><strong>Value Range</strong>: <code>True</code>, <code>False</code>.<br> <strong>Default</strong>: <code>False</code>.</td><td>Optional</td></tr><tr><td><code>core_num</code></td><td><strong>Purpose</strong>: Number of cores for model execution.<br><strong>Description</strong>: D-Robotics Platform supports using multiple AI accelerator cores simultaneously for inference tasks. Multiple cores are beneficial for larger input sizes, with double-core speed typically around 1.5 times that of single-core. If your model has large inputs and追求极致速度, set <code>core_num=2</code>. <strong>Note</strong>: This option is not supported for RDK Ultra, please do not configure.</td><td><strong>Value Range</strong>: <code>1</code>, <code>2</code>.<br> <strong>Default</strong>: <code>1</code>.</td><td>Optional</td></tr><tr><td><code>optimize_level</code></td><td><strong>Purpose</strong>: Model compilation optimization level.<br><strong>Description</strong>: The optimization levels range from <code>O0</code> (no optimization, fastest compile) to <code>O3</code> (higher optimization, slower compile). Normal performance models should use <code>O3</code> for optimal performance. Lower levels can be used for faster development or debugging processes.</td><td><strong>Value Range</strong>: <code>O0</code>, <code>O1</code>, <code>O2</code>, <code>O3</code>.<br> <strong>Default</strong>: None.</td><td>Required</td></tr><tr><td><code>input_source</code></td><td><strong>Purpose</strong>: Set the source of input data for the on-board bin model.<br><strong>Description</strong>: This parameter is for engineering environment compatibility. Configure after model validation. Options include <code>ddr</code> (memory), <code>pyramid</code>, and <code>resizer</code>. Note: If set to <code>resizer</code>, the model&#x27;s h*w should be less than 18432. In an engineering environment, adapting <code>pyramid</code> and <code>resizer</code> sources requires specific configuration, e.g., if the model input name is <code>data</code> and the source is memory (ddr), set as <code>{`&quot;data&quot;: &quot;ddr&quot;`}</code>.</td><td><strong>Value Range</strong>: <code>ddr</code>, <code>pyramid</code>, <code>resizer</code><br> <strong>Default</strong>: None (auto-selected based on <code>input_type_rt</code>).</td><td>Optional</td></tr><tr><td><code>max_time_per_fc</code></td><td><strong>Purpose</strong>: Maximum continuous execution time per function call (in us).<br><strong>Description</strong>: In the compiled data instruction model, each inference on BPU is represented by one or more function calls (BPU execution granularity). A value of 0 means no limit. This parameter limits the max execution time per function call, allowing the model to be interrupted if necessary. See the section on Model Priority Control for details. - This parameter is for implementing model preemption; ignore if not needed.<br> - Model preemption is only supported on development boards, not PC simulators.</td><td><strong>Value Range</strong>: <code>0</code> or <code>1000-4294967295</code>.<br> <strong>Default</strong>: <code>0</code>.</td><td>Optional</td></tr><tr><td><code>jobs</code></td><td><strong>Purpose</strong>: Number of processes for compiling the bin model.<br><strong>Description</strong>: Sets the number of processes during bin model compilation, potentially improving compile speed.</td><td><strong>Value Range</strong>: Up to the maximum number of supported cores on the machine.<br> <strong>Default</strong>: None.</td><td>Optional</td></tr></tbody></table>
<ul>
<li>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="custom-operator-parameter-group">Custom Operator Parameter Group<a href="#custom-operator-parameter-group" class="hash-link" aria-label="Direct link to Custom Operator Parameter Group" title="Direct link to Custom Operator Parameter Group">​</a></h5>
</li>
</ul>
<table><thead><tr><th>Parameter Name</th><th>Description of Configuration</th><th>Range of Values</th><th>Optional/Mandatory</th></tr></thead><tbody><tr><td><code>custom_op_method</code></td><td><strong>Purpose</strong>: Select strategy for custom operator.<br><strong>Description</strong>: Currently, only the &#x27;register&#x27; strategy is supported.</td><td><strong>Range</strong>: <code>register</code>.<br> <strong>Default</strong>: None.</td><td>Optional</td></tr><tr><td><code>op_register_files</code></td><td><strong>Purpose</strong>: Names of Python files implementing the custom operator(s).<br><strong>Description</strong>: Multiple files can be separated by <code>;</code>.</td><td><strong>Range</strong>: None.<br> <strong>Default</strong>: None.</td><td>Optional</td></tr><tr><td><code>custom_op_dir</code></td><td><strong>Purpose</strong>: Path to the directory containing the Python files for the custom operator(s).<br><strong>Description</strong>: Please use relative path when setting the path.</td><td><strong>Range</strong>: None.<br> <strong>Default</strong>: None.</td><td>Optional</td></tr></tbody></table>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="int16_config">RDK Ultra int16 Configuration Instructions <code>{</code>#int16_config<code>}</code><a href="#int16_config" class="hash-link" aria-label="Direct link to int16_config" title="Direct link to int16_config">​</a></h5>
<p>In the process of model conversion, most operators in the model will be quantized to int8 for computation. By configuring the &quot;node_info&quot; parameter, you can specify in detail that the input/output data type of a specific op is int16 for computation (the specific supported operator range can be referred to the RDK Ultra operator support list in the &quot;Supported Operator List&quot; chapter).
The basic principle is as follows:</p>
<p>After you configure the input/output data type of a certain op as int16, the model conversion will automatically update and check the int16 configuration of the op&#x27;s input/output context. For example, when configuring the input/output data type of op_1 as int16, it actually implicitly specifies that the previous/next op of op_1 needs to support int16 computation.
For unsupported scenarios, the model conversion tool will print a log to indicate that the int16 configuration combination is temporarily unsupported and will fallback to int8 computation.</p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="pre_process">Pre-processing HzPreprocess Operator Instructions <code>{</code>#pre_process<code>}</code><a href="#pre_process" class="hash-link" aria-label="Direct link to pre_process" title="Direct link to pre_process">​</a></h5>
<p>The pre-processing HzPreprocess operator is a pre-processing operator node inserted after the model input node during the model conversion process of D-Robotics Model Conversion Tool. It is used to normalize the input data of the model. This section mainly introduces the parameters &quot;norm_type&quot;, &quot;mean_value&quot;, &quot;scale_value&quot;, and the explanation of the HzPreprocess operator node generated by the model pre-processing.</p>
<p><strong>norm_type Parameter Explanation</strong></p>
<ul>
<li>
<p>Parameter Function: This parameter is used to add the input data pre-processing method to the model.</p>
</li>
<li>
<p>Parameter Value Range and Explanation:</p>
<ul>
<li>&quot;no_preprocess&quot; indicates no data pre-processing is added.</li>
<li>&quot;data_mean&quot; indicates subtraction of mean value pre-processing.is provided.</li>
<li>&quot;data_scale&quot; indicates multiplication by scale factor pre-processing.</li>
<li>&quot;data_mean_and_scale&quot; indicates subtraction of mean value followed by multiplication by scale factor pre-processing.</li>
</ul>
</li>
</ul>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><p>When there are multiple input nodes, the order of the set nodes must strictly match the order in &quot;input_name&quot;.</p></div></div>
<p><strong>mean_value Parameter Explanation</strong></p>
<ul>
<li>
<p>Parameter Function: This parameter represents the mean value subtracted from the image for the specified pre-processing method.</p>
</li>
<li>
<p>Usage: This parameter needs to be configured when &quot;norm_type&quot; is set to &quot;data_mean_and_scale&quot; or &quot;data_mean&quot;.</p>
</li>
<li>
<p>Parameter Explanation:</p>
<ul>
<li>When there is only one input node, only one value needs to be configured, indicating that all channels will subtract this mean value.</li>
<li>When there are multiple nodes, provide values that match the number of channels (these values are separated by spaces), indicating that each channel will subtract a different mean value.</li>
</ul>
</li>
</ul>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><ol>
<li>The number of configured input nodes must match the number of nodes configured in &quot;norm_type&quot;.</li>
<li>If there is a node that does not require mean processing, configure it as &quot;None&quot;.</li>
</ol></div></div>
<p><strong>scale_value Parameter Explanation</strong></p>
<ul>
<li>
<p>Parameter Function: This parameter represents the scale factor for the specified pre-processing method.</p>
</li>
<li>
<p>Usage: This parameter needs to be configured when &quot;norm_type&quot; is set to &quot;data_mean_and_scale&quot; or &quot;data_scale&quot;.- Parameter description:</p>
<ul>
<li>When there is only one input node, only one value needs to be configured, which represents the scaling factor for all channels.</li>
<li>When there are multiple nodes, provide the same number of values as the number of channels (these values are separated by spaces), which represents different scaling factors for each channel.</li>
</ul>
</li>
</ul>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><ol>
<li>The number of configured input nodes must be consistent with the number of nodes configured for <code>norm_type</code>.</li>
<li>If there is a node that does not require <code>scale</code> processing, configure it as <code>&#x27;None&#x27;</code>.</li>
</ol></div></div>
<p><strong>Formula and example explanation</strong></p>
<ul>
<li>Formula for data normalization during model training</li>
</ul>
<p>The mean and scale parameters in the YAML file need to be calculated based on the mean and std during training.</p>
<p>The calculation formula for data normalization in the preprocessing node (i.e. in the HzPreprocess node) is <code>norm_data = (data - mean) * scale</code>.</p>
<p>Taking yolov3 as an example, the preprocessing code during training is as follows:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">base_transform</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">image</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> size</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> mean</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> std</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    x </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> cv2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">resize</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">image</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">size</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> size</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">astype</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">float32</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    x </span><span class="token operator" style="color:#393A34">/=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">255</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    x </span><span class="token operator" style="color:#393A34">-=</span><span class="token plain"> mean</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    x </span><span class="token operator" style="color:#393A34">/=</span><span class="token plain"> std</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> x</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">class</span><span class="token plain"> </span><span class="token class-name">BaseTransform</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> size</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> mean</span><span class="token operator" style="color:#393A34">=</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">0.406</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.456</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.485</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> std</span><span class="token operator" style="color:#393A34">=</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">0.225</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.224</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.229</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">size </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> size</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">mean </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">array</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">mean</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dtype</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">float32</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">std </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">array</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">std</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dtype</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">float32</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<p>The formula becomes: <code>norm_data = (\frac</code>data<code>{`255`}` - mean) * \frac`{`1`}</code>std<!-- -->``,</p>
<p>Rewritten as the calculation method in the HzPreprocess node: <code>norm_data = (\frac</code>data<code>{`255`}` - mean) * \frac`{`1`}</code>std<code> = (data - 255mean) * \frac</code>1<code>{`255std`}</code>,</p>
<p>Therefore: <code>mean_yaml = 255 mean, scale_yaml = \frac</code>1<code>{`255std`}</code>.</p>
<ul>
<li>Formula during model inference</li>
</ul>
<p>By configuring the parameters in the YAML configuration file, whether to add the HzPreprocess node is determined.
When configuring mean/scale, when performing model conversion, a HzPreprocess node will be added to the input, which can be understood as performing a convolution operation on the input data.</p>
<p>The calculation formula in the HzPreprocess node is: <code>((input (range [-128,127]) + 128) - mean) * scale</code>, where <code>weight = scale</code>, <code>bias = (128 - mean) * scale</code>.</p>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><ol>
<li>After adding mean/scale in the YAML, there is no need to include MeanTransformer and ScaleTransformer in the preprocessing.</li>
<li>Adding mean/scale in the YAML will place the parameters within the HzPreprocess node, which is a BPU (Base Processing Unit) node.</li>
</ol></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="conversion-internal-process-interpretation">Conversion Internal Process Interpretation<a href="#conversion-internal-process-interpretation" class="hash-link" aria-label="Direct link to Conversion Internal Process Interpretation" title="Direct link to Conversion Internal Process Interpretation">​</a></h4>
<p>During the model conversion stage, the floating-point model is transformed into the D-Robotics mixed heterogeneous model. In order to efficiently run this heterogeneous model on embedded devices, the model conversion focuses on solving two key issues: <strong>input data processing</strong> and <strong>model optimization compilation</strong>. This section will discuss these two key issues in detail.</p>
<p><strong>Input Data Processing</strong>: The D-Robotics X3 processor provides hardware-level support for certain types of model input pathways. For example, in the case of the video pathway, the video processing subsystem provides functions such as image cropping, scaling, and other image quality optimization for image acquisition. The output of these subsystems is in the YUV420 NV12 format, while the algorithm models are typically trained on more common image formats such as BGR/RGB.</p>
<p>D-Robotics provides the following solutions for this situation:</p>
<ol>
<li>
<p>Each converted model provides two types of descriptions: one for describing the input data of the original floating-point model (<code>input_type_train</code> and <code>input_layout_train</code>), and the other for describing the input data of the processor we need to interface with (<code>input_type_rt</code> and <code>input_layout_rt</code>).</p>
</li>
<li>
<p>Mean/scale operations on image data are also common, but these operations are not suitable for the data formats supported by processors such as YUV420 NV12. Therefore, we have embedded these common image preprocessing operations into the model.</p>
</li>
</ol>
<p>After processing through the above two methods, the input part of the heterogeneous model <code>***.bin</code> generated during the model conversion stage will look like the following:</p>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/input_data_process.png" alt="input_data_process" class="img_ev3q"></p>
<p>The data layouts shown in the above figure include NCHW and NHWC, where N represents the number, C represents the channel, H represents the height, and W represents the width. The two different layouts reflect different memory access characteristics. NHWC is more commonly used in TensorFlow models, while NCHW is used in Caffe. The D-Robotics processor does not restrict the use of data layouts, but there are two requirements: first, <code>input_layout_train</code> must be consistent with the data layout of the original model; second, prepare the data with the data layout consistent with <code>input_layout_rt</code> on the processor, as the correct data layout is the basis for successful data parsing.</p>
<p>The model conversion tool will automatically add data conversion nodes based on the data formats specified by <code>input_type_rt</code> and <code>input_type_train</code>. According to D-Robotics&#x27;s actual usage experience, not all possible type combinations are needed. To prevent misuse, we only provide a few fixed type combinations, as shown in the table below:</p>
<table><thead><tr><th><code>input_type_train</code> \ <code>input_type_rt</code></th><th>nv12</th><th>yuv444</th><th>rgb</th><th>bgr</th><th>gray</th><th>featuremap</th></tr></thead><tbody><tr><td>yuv444</td><td>Y</td><td>Y</td><td>N</td><td>N</td><td>N</td><td>N</td></tr><tr><td>rgb</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>N</td><td>N</td></tr><tr><td>bgr</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>N</td><td>N</td></tr><tr><td>gray</td><td>N</td><td>N</td><td>N</td><td>N</td><td>Y</td><td>N</td></tr><tr><td>featuremap</td><td>N</td><td>N</td><td>N</td><td>N</td><td>N</td><td>Y</td></tr></tbody></table>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><p>The first row in the table represents the types supported by <code>input_type_rt</code>, and the first column represents the types supported by <code>input_type_train</code>. The <strong>Y/N</strong> indicates whether the conversion from <code>input_type_rt</code> to <code>input_type_train</code> is supported. In the final produced bin model after model conversion, the conversion from <code>input_type_rt</code> to <code>input_type_train</code> is an internal process, so you only need to pay attention to the data format of <code>input_type_rt</code>. Understanding the requirements of each <code>input_type_rt</code> is important for preparing inference data for embedded applications. The following are explanations of each format of <code>input_type_rt</code>:</p><ul>
<li>
<p>RGB, BGR, and gray are common image formats, and each value is represented by UINT8.</p>
</li>
<li>
<p>YUV444 is a common image format, and each value is represented by UINT8.</p>
</li>
<li>
<p>NV12 is a common YUV420 image format, and each value is represented by UINT8.</p>
</li>
<li>
<p>A special case of nv12 is when &quot;input_space_and_range&quot; is set to &quot;bt601_video&quot; (refer to the previous description of the &quot;input_space_and_range&quot; parameter). In contrast to the regular nv12 case, the value range in this case changes from [0,255] to [16,235], and each value is still represented by UINT8.</p>
</li>
<li>
<p>The data format type for the input feature map of the model only requires the data to be four-dimensional, and each value is represented by float32. For example, models processing radar and audio often use this format.</p>
</li>
</ul></div></div>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Tip</div><div class="admonitionContent_BuS1"><p>The calibration data only needs to be processed until input_type_train, and be careful not to perform duplicate normalization operations.</p><p>The &quot;input_type_rt&quot; and &quot;input_type_train&quot; are fixed in the algorithm toolchain&#x27;s processing flow. If you are certain that no conversion is needed, you can set both &quot;input_type&quot; configurations to be the same. This way, &quot;input_type&quot; will be treated as a pass-through without affecting the actual execution performance of the model.</p><p>Similarly, the data preprocessing is also fixed in the flow. If you don&#x27;t need any preprocessing, you can disable this feature by configuring norm_type, without affecting the actual execution performance of the model.</p></div></div>
<p>The <strong>model optimization compilation</strong> completes several important stages, including model parsing, model optimization, model calibration and quantization, and model compilation. The internal workflow is shown in the following diagram:</p>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/model_optimization.png" alt="model_optimization" class="img_ev3q"></p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><ol>
<li>&quot;input_type_rt*&quot; represents the intermediate format of input_type_rt.</li>
<li>The X3 processor architecture only supports inference with &quot;NHWC&quot; data. Please use the visualization tool Netron to view the data layout of the input nodes in the &quot;quantized_model.onnx&quot; and decide whether to add &quot;layout conversion&quot; in the preprocessing.</li>
</ol></div></div>
<p>In the <strong>model parsing stage</strong>, for a Caffe floating-point model, it will be transformed into an ONNX floating-point model. The original floating-point model will determine whether to include a data preprocessing node based on the configuration parameters in the transformation configuration YAML file. This stage produces an &quot;original_float_model.onnx&quot;. This ONNX model still has a calculation precision of float32, but it includes a data preprocessing node in the input part.</p>
<p>Ideally, this preprocessing node should complete the complete conversion from &quot;input_type_rt&quot; to &quot;input_type_train&quot;. In reality, the entire type conversion process will be completed in collaboration with the D-Robotics processor hardware. The ONNX model does not include the hardware conversion. Therefore, the real input type of ONNX will use an intermediate type, which is the result type of the hardware processing of &quot;input_type_rt&quot;. The data layout (NCHW/NHWC) will remain consistent with the input layout of the original floating-point model. Each &quot;input_type_rt&quot; has a specific corresponding intermediate type, as shown in the following table:</p>
<table><thead><tr><th><strong>nv12</strong></th><th><strong>yuv444</strong></th><th><strong>rgb</strong></th><th><strong>bgr</strong></th><th><strong>gray</strong></th><th>featuremap</th></tr></thead><tbody><tr><td>yuv444_128</td><td>yuv444_128</td><td>RGB_128</td><td>BGR_128</td><td>GRAY_128</td><td>featuremap</td></tr></tbody></table>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><p>The bold part in the table is the data type specified by &quot;input_type_rt&quot;, and the second row represents the specific intermediate type corresponding to a specific &quot;input_type_rt&quot;. This intermediate type is the input type of the &quot;original_float_model.onnx&quot;. Each type is explained as follows:</p><ul>
<li>yuv444_128 represents yuv444 data subtracting 128, and each value is represented by int8.</li>
<li>RGB_128 represents RGB data subtracting 128, and each value is represented by int8.</li>
<li>BGR_128 represents BGR data subtracting 128, and each value is represented by int8.</li>
<li>GRAY_128 represents gray data subtracting 128, and each value is represented by int8.- featuremap is a four-dimensional tensor data, and each value is represented as float32.</li>
</ul></div></div>
<p><strong>Model Optimization Phase</strong> implements some optimization strategies for the model that are suitable for the D-Robotics platform, such as BN fusion into Conv.
The output of this phase is an optimized_float_model.onnx file. The computational precision of this ONNX model is still float32, and the optimization will not affect the computational results of the model.
The requirements for the input data of the model are still consistent with the original_float_model mentioned earlier.</p>
<p><strong>Model Calibration Phase</strong> uses the calibration data provided by you to calculate the necessary quantization threshold parameters. These parameters will be directly input into the quantization phase without generating new model states.</p>
<p><strong>Model Quantization Phase</strong> uses the parameters obtained from calibration to complete model quantization. The output of this phase is a quantized_model.onnx file.
The computational precision of this model is int8, and using this model can evaluate the accuracy loss caused by quantization.
The basic data format and layout of the input for this model remain the same as the original_float_model, but the layout and numerical representation have changed.
The overall changes compared to the original_float_model input are described as follows:</p>
<ul>
<li>The data layout of &quot;RDK X3&quot; is NHWC.</li>
<li>When the value of &quot;input_type_rt&quot; is non-&quot;featuremap&quot;, the data type of the input is INT8.
Conversely, when the value of &quot;input_type_rt&quot; is &quot;featuremap&quot;, the data type of the input is float32.</li>
</ul>
<p>The relationship between the data layout is as follows:</p>
<ul>
<li>Original model input layout: NCHW.</li>
<li>input_layout_train: NCHW.</li>
<li>origin.onnx input layout: NCHW.</li>
<li>calibrated_model.onnx input layout: NCHW.</li>
<li>quanti.onnx input layout: NHWC.</li>
</ul>
<p>That is, the input layout of input_layout_train, origin.onnx, and calibrated_model.onnx is consistent with the original model input layout.</p>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><p>Please note that if input_type_rt is nv12, the input layout of quanti.onnx is NHWC.</p></div></div>
<p><strong>Model Compilation Phase</strong> uses the D-Robotics model compiler to convert the quantized model into the computation instructions and data supported by the D-Robotics platform. The output of this phase is a *.bin model, which is the model that can be run on the D-Robotics embedded platform, and it is the final output of the model conversion.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="interpretation-of-conversion-results">Interpretation of Conversion Results<a href="#interpretation-of-conversion-results" class="hash-link" aria-label="Direct link to Interpretation of Conversion Results" title="Direct link to Interpretation of Conversion Results">​</a></h4>
<p>This section will introduce the interpretation of the successful conversion status and the analysis methods for unsuccessful conversions. To confirm the successful model conversion, you need to confirm from three aspects: the &quot;makertbin&quot; status information, similarity information, and the &quot;working_dir&quot; output.
Regarding the &quot;makertbin&quot; status information, the console will output a clear prompt message at the end of the information when the conversion is successful, as follows:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  2021-04-21 11:13:08,337 INFO Convert to runtime bin file successfully!</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  2021-04-21 11:13:08,337 INFO End Model Convert</span><br></span></code></pre></div></div>
<p>The similarity information also exists in the console output of &quot;makertbin&quot;. Before the &quot;makertbin&quot; status information, its content is in the following format:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  ======================================================================</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Node    ON   Subgraph  Type     Cosine Similarity  Threshold</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">``````bash</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">……  ……  ……  ……  0.999936  127.000000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">……  ……  ……  ……  0.999868  2.557209</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">……  ……  ……  ……  0.999268  2.133924</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">……  ……  ……  ……  0.996023  3.251645</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">……  ……  ……  ……  0.996656  4.495638</span><br></span></code></pre></div></div>
<p>The output content listed above, Nodes, ON, Subgraph and Type are consistent with the interpretation of the <code>hb_mapper checker</code> tool, please refer to the previous section <a href="#check_result">Check Results</a>;
Threshold is the calibration threshold for each layer, which is used to provide feedback to D-Robotics technical support in abnormal conditions, and does not need to be paid attention to under normal conditions;
The column of Cosine Similarity reflects the cosine similarity of the output results of the corresponding operator in the Node column between the original floating-point model and the quantization model.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Tips</div><div class="admonitionContent_BuS1"><p>In general, <strong>the cosine similarity of output nodes in the model &gt;= 0.99 can be considered to be normal</strong>, and if the similarity of output nodes is lower than 0.8, there is obvious loss of accuracy. Of course, Cosine Similarity only provides a reference for the stability of quantized data, and there is no obvious direct relationship with the impact on model accuracy. To obtain accurate accuracy information, you need to read the section <a href="#accuracy_evaluation">Model Accuracy Analysis and Tuning</a>.</p></div></div>
<p>The conversion output is stored in the path specified by the conversion configuration parameter <code>working_dir</code>. After the model conversion is successfully completed, you can obtain the following files in this directory (the *** part is specified by the conversion configuration parameter <code>output_model_file_prefix</code>):</p>
<ul>
<li>***_original_float_model.onnx</li>
<li>***_optimized_float_model.onnx</li>
<li>***_calibrated_model.onnx</li>
<li>***_quantized_model.onnx</li>
<li>***.bin</li>
</ul>
<p>The usage of each output is explained in the section <a href="#conversion_output">Conversion Output Interpretation</a>.</p>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Caution</div><div class="admonitionContent_BuS1"><p>Before running on the board, we recommend that you complete the model performance evaluation and tuning process described in <a href="#performance_evaluation">Performance Evaluation of the Model</a> to avoid extending the model conversion issues to the subsequent embedded end.</p></div></div>
<p>If any of the three aspects of successful model conversion mentioned above are missing, it indicates that there is an error in the model conversion. In general, the <code>makertbin</code> tool will output error information to the console when an error occurs. For example, when converting a Caffe model without configuring the <code>prototxt</code> and <code>caffe_model</code> parameters in the YAML file, the model conversion tool gives the following prompt:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">2021-04-21 14:45:34,085 ERROR Key &#x27;model_parameters&#x27; error:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Missing keys: &#x27;caffe_model&#x27;, &#x27;prototxt&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2021-04-21 14:45:34,085 ERROR yaml file parse failed. Please double check your input</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2021-04-21 14:45:34,085 ERROR exception in command: makertbin</span><br></span></code></pre></div></div>
<p>If the log information output to the console cannot help you find the problem, please refer to the section <a href="/rdk_doc/en/FAQ/toolchain#model_convert_errors_and_solutions">Model Quantization Errors and Solutions</a> for troubleshooting. If the above steps still cannot solve the problem, please contact the D-Robotics technical support team or submit your issue in the <a href="https://developer.d-robotics.cc/" target="_blank" rel="noopener noreferrer">official D-Robotics developer community</a>, and we will provide support within 24 hours.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="conversion_output">Conversion Output Interpretation<code>{</code>#conversion_output<code>}</code><a href="#conversion_output" class="hash-link" aria-label="Direct link to conversion_output" title="Direct link to conversion_output">​</a></h4>
<p>As mentioned earlier, the successful conversion of the model produces four parts, each of which will be introduced in this section:</p>
<ul>
<li>***_original_float_model.onnx</li>
<li>***_optimized_float_model.onnx</li>
<li>***_calibrated_model.onnx</li>
<li>***_quantized_model.onnx</li>
<li>***.bin</li>
</ul>
<p>The process of generating ***_original_float_model.onnx can refer to the explanation in <a href="#conversion_interpretation"><strong>Conversion Interpretation</strong></a>.
The computation accuracy of this model is exactly the same as the original float model used in the conversion input.
One important change is the addition of some data preprocessing computations to adapt to the D-Robotics platform (an additional preprocessing operator node called &quot;HzPreprocess&quot; has been added, which can be viewed using the netron tool to open the onnx model. For details about this operator, please see <a href="#pre_process"><strong>Preprocessing Parameters of Operator HzPreprocess</strong></a>).
In general, you do not need to use this model. However, if you encounter abnormal results in the conversion process and the troubleshooting method mentioned earlier does not solve your problem, please provide this model to D-Robotics&#x27;s technical support team, or submit your questions in the <a href="https://developer.d-robotics.cc/" target="_blank" rel="noopener noreferrer"><strong>D-Robotics Official Technical Community</strong></a>. This will help you quickly resolve your issue.</p>
<p>The process of generating ***_calibrated_model.onnx can refer to the explanation in <a href="#conversion_interpretation"><strong>Conversion Interpretation</strong></a>.
This model is produced by the model conversion toolchain, which optimizes the float model&#x27;s structure and obtains the quantization parameters for each node by calculating with calibration data, which are saved in the calibration node as intermediate products.</p>
<p>The process of generating ***_optimized_float_model.onnx can refer to the explanation in <a href="#conversion_interpretation"><strong>Conversion Interpretation</strong></a>.
This model undergoes some operator-level optimization operations, such as operator fusion.
By comparing it with the original_float model visually, you can clearly see some changes at the operator structure level, but these do not affect the model&#x27;s computation accuracy.
In general, you do not need to use this model. However, if you encounter abnormal results in the conversion process and the troubleshooting method mentioned earlier does not solve your problem, please provide this model to D-Robotics&#x27;s technical support team, or submit your questions in the <a href="https://developer.d-robotics.cc/" target="_blank" rel="noopener noreferrer"><strong>D-Robotics Official Technical Community</strong></a>. This will help you quickly resolve your issue.</p>
<p>The process of generating ***_quantized_model.onnx can refer to the explanation in <a href="#conversion_interpretation"><strong>Conversion Interpretation</strong></a>.
This model has completed the calibration and quantization process.
To evaluate the accuracy loss of the quantized model, you can read the content on model accuracy analysis and optimization in the following sections.
This model is necessary for accuracy verification. For specific usage, please refer to the introduction in <a href="#accuracy_evaluation"><strong>Model Accuracy Analysis and Optimization</strong></a>.</p>
<p>***.bin is the model that can be loaded and run on the D-Robotics processor.
With the content introduced in the &quot;Runtime Application Development Guide&quot; section on on-board operation,
you can quickly deploy and run the model on the D-Robotics processor. However, to ensure that the model&#x27;s performance and accuracy meet your expectations, we recommend completing the performance and accuracy analysis process introduced in <a href="#model_conversion"><strong>Model Conversion</strong></a> and <a href="#accuracy_evaluation"><strong>Model Accuracy Analysis and Optimization</strong></a> before entering the application development and deployment stages.</p>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><p>In general, the model that can be run on the D-Robotics processor can be obtained after the model conversion stage. However, to ensure that the performance and accuracy of the model meet the application requirements, D-Robotics recommends completing the performance evaluation and accuracy evaluation steps after each conversion.</p><p>The model conversion process generates the onnx model, which is an intermediate product for users to verify the model&#x27;s accuracy. Therefore, it does not guarantee its compatibility between versions. If you use the evaluation script in the example to evaluate the onnx model on a single image or on a test set, please use the onnx model generated by the current version of the tool for operation.</p></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="performance_evaluation">Model Performance Analysis<code>{</code>#performance_evaluation<code>}</code><a href="#performance_evaluation" class="hash-link" aria-label="Direct link to performance_evaluation" title="Direct link to performance_evaluation">​</a></h3>
<p>This section introduces how to use the tools provided by D-Robotics to evaluate the model&#x27;s performance. By using these tools, you can obtain performance results that are consistent with actual on-board execution. If you find that the evaluation results do not meet your expectations, it is recommended that you try to solve the performance issues based on the optimization suggestions provided by D-Robotics, rather than extending the model&#x27;s performance issues to the application development stage.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="hb_perf">Performance Evaluation on Development Machine<code>{</code>#hb_perf<code>}</code><a href="#hb_perf" class="hash-link" aria-label="Direct link to hb_perf" title="Direct link to hb_perf">​</a></h4>
<p>Use the &quot;hb_perf&quot; tool to evaluate the model&#x27;s performance. The usage is as follows:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  hb_perf ***.bin</span><br></span></code></pre></div></div>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><p>If the analysis is performed on the &quot;pack&quot; model, it is necessary to add the &quot;-p&quot; parameter, and the command becomes &quot;hb_perf -p ***.bin&quot;.
For information about the &quot;pack&quot; model, please refer to the introduction in the other model tools (optional) section.
:::The ***.bin in the command is the quantized model generated in the model conversion step. After the command execution is completed, a <code>hb_perf_result</code> folder will be generated in the current directory, which contains the specific model analysis results.
Here is an example of the evaluation results for the MobileNetv1 model:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  hb_perf_result/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  └── mobilenetv1_224x224_nv12</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      ├── MOBILENET_subgraph_0.html</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      ├── MOBILENET_subgraph_0.json</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      ├── mobilenetv1_224x224_nv12</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      ├── mobilenetv1_224x224_nv12.html</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      ├── mobilenetv1_224x224_nv12.png</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      └── temp.hbm</span><br></span></code></pre></div></div><p>Open the <code>mobilenetv1_224x224_nv12.html</code> main page in a browser. Its content is as shown in the following figure:</p><p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/hb_mapper_perf_2.png" alt="hb_mapper_perf_2" class="img_ev3q"></p><p>The analysis results mainly consist of three parts: Model Performance Summary, Details, and BIN Model Structure.
Model Performance Summary provides an overall performance evaluation of the bin model, with the following metrics:</p><ul>
<li>Model Name - The name of the model.</li>
<li>Model Latency (ms) - The overall time taken for calculating one frame of the model (in milliseconds).</li>
<li>Total DDR (loaded+stored) bytes per frame (MB per frame) - The total amount of DDR used for loading and storing data in the BPU section of the model (in MB per frame).</li>
<li>Loaded Bytes per Frame - The amount of data loaded per frame during the model execution.</li>
<li>Stored Bytes per Frame - The amount of data stored per frame during the model execution.</li>
</ul><p>The BIN Model Structure provides a visualization of the subgraphs in the bin model. The nodes in dark cyan represent the nodes running on the BPU, while the gray nodes represent the nodes computed on the CPU.</p><p>When viewing the Details and BIN Model Structure, you need to understand the concept of subgraphs. If there are CPU operators in the model network structure, the model conversion tool will split the parts of the BPU calculation before and after the CPU operators into two independent subgraphs.
For more information, please refer to the section on <a href="#model_check">Model Verification</a>.</p><p>Details provide specific information for each BPU subgraph in the model. In the <code>mobilenetv1_224x224_nv12.html</code> main page, the metrics for each subgraph are as follows:</p><ul>
<li>Model Subgraph Name - The name of the subgraph.</li>
<li>Model Subgraph Calculation Load (OPpf) - The calculation load of the subgraph per frame.</li>
<li>Model Subgraph DDR Occupation (Mbpf) - The amount of data read and written by the subgraph per frame (in MB).</li>
<li>Model Subgraph Latency (ms) - The calculation time of the subgraph per frame (in milliseconds).</li>
</ul><p>Each subgraph result provides detailed reference information.</p><div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><p>The reference information page may vary depending on whether you have enabled the debug configuration.
The Layer Details shown in the following figure can only be obtained when the <code>debug</code> parameter is set to <code>True</code> in the YAML configuration file.
For more information on configuring the <code>debug</code> parameter, please refer to the section on <a href="#makertbin">Using hb_mapper makertbin tool to convert models</a>.</p></div></div></div></div>
<p>Layer Details provide analysis at the specific operator level and can be used as a reference in the model debugging and analysis stage. For example, if certain BPU operators are causing low model performance, the analysis results can help you locate the specific operator.</p>
<p>Please translate the following content into English, maintaining the original format and content: <img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/layer_details.png" alt="layer_details" class="img_ev3q"></p>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><p>The results obtained from the &quot;hb_perf&quot; tool can help you understand the subgraph structure of the bin model and the static analysis metrics of the BPU calculation part in the model. It is important to note that the analysis results do not include the performance evaluation of the CPU part. If you need to evaluate the performance of CPU calculation, please test the model performance on the development board.</p></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="performance-testing-on-development-board">Performance Testing on Development Board<a href="#performance-testing-on-development-board" class="hash-link" aria-label="Direct link to Performance Testing on Development Board" title="Direct link to Performance Testing on Development Board">​</a></h4>
<p>To quickly evaluate the performance of the model on the development board, please use the &quot;hrt_model_exec perf&quot; tool. This tool allows you to evaluate the inference performance of the model and obtain model information directly on the development board.</p>
<p>Before using the &quot;hrt_model_exec perf&quot; tool, please make sure to:</p>
<ol>
<li>
<p>Refer to the system update section to complete the system update on the development board.</p>
</li>
<li>
<p>Copy the bin model obtained on the Ubuntu development machine to the development board (recommended to put it in the /userdata directory). The development board is a Linux system, and you can use common Linux methods such as &quot;scp&quot; to complete this copying process.</p>
</li>
</ol>
<p>The command for the &quot;hrt_model_exec perf&quot; tool on the development board is as follows (<strong>note that it should be executed on the development board</strong>):</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">./hrt_model_exec perf --model_file mobilenetv1_224x224_nv12.bin \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                      --model_name=&quot;&quot; \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                      --core_id=0 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                      --frame_count=200 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                      --perf_time=0 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                      --thread_num=1 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                      --profile_path=&quot;.&quot;</span><br></span></code></pre></div></div>
<p>Explanation of hrt_model_exec perf parameters:</p>
<p>model_file:<br>
The name of the bin model to be analyzed for performance.</p>
<p>model_name:<br>
The name of the bin model to be analyzed for performance. If &quot;model_file&quot; contains only one model, it can be omitted.</p>
<p>core_id:<br>
Default value is &quot;0&quot;, which represents any core. &quot;0&quot; stands for any core, &quot;1&quot; stands for core 0, and &quot;2&quot; stands for core 1. If you want to analyze the maximum frame rate of dual cores, set it to &quot;0&quot;.</p>
<p>frame_count:<br>
Default value is &quot;200&quot;, which sets the number of inference frames. The tool will execute the specified number of times before analyzing the average time. This takes effect when &quot;perf_time&quot; is &quot;0&quot;.</p>
<p>perf_time:<br>
Default value is &quot;0&quot;, expressed in minutes. Set the inference time, and the tool will execute for the specified time before analyzing the average time.</p>
<p>thread_num:<br>
Default value is &quot;1&quot;, which sets the number of running threads. The value range is between [1, 8]. If you want to analyze the maximum frame rate, increase the number of threads.</p>
<p>profile_path:<br>Default closed, the log path for statistical tool is generated. The analysis results introduced by this parameter will be stored in the profiler.log and profiler.csv files in the specified directory.</p>
<p>The following example is the actual test result on the RDK X3 development board. After the command is executed, you will get the following log on the console:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Running condition:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Thread number is: 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Frame count   is: 200</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  core number   is: 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Program run time: 818.985000 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Perf result:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Frame totally latency is: 800.621155 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Average    latency    is: 4.003106 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Frame      rate       is: 244.204717 FPS</span><br></span></code></pre></div></div>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Tips</div><div class="admonitionContent_BuS1"><p>In the evaluation results, &quot;Average latency&quot; and &quot;Frame rate&quot; respectively indicate the average single-frame inference latency and the model&#x27;s maximum frame rate.
If you want to obtain the maximum frame rate of the model running on the board, try adjusting the value of &quot;thread_num&quot; and find the optimal value for the number of threads. Different values will output different performance results.</p></div></div>
<p>The information obtained from the console only provides an overview. The node_profiler.log file generated by setting the &quot;profile_path&quot; parameter records more abundant model performance information:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">{</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &quot;perf_result&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;FPS&quot;: 244.20471681410527,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;average_latency&quot;: 4.003105640411377</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &quot;running_condition&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;core_id&quot;: 0,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;frame_count&quot;: 200,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;model_name&quot;: &quot;mobilenetv1_224x224_nv12&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;run_time&quot;: 818.985,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;thread_num&quot;: 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">***</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">{</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &quot;chip_latency&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;BPU_inference_time_cost&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;avg_time&quot;: 3.42556,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &quot;model_latency&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;BPU_MOBILENET_subgraph_0&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;avg_time&quot;: 3.42556,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;max_time&quot;: 3.823,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;min_time&quot;: 3.057</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;Dequantize_fc7_1_HzDequantize&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;avg_time&quot;: 0.12307,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;max_time&quot;: 0.274,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;min_time&quot;: 0.044</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;MOBILENET_subgraph_0_output_layout_convert&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;avg_time&quot;: 0.025945,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;max_time&quot;: 0.069,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;min_time&quot;: 0.012</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;Preprocess&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;avg_time&quot;: 0.009245,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;max_time&quot;: 0.027,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;min_time&quot;: 0.003</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;Softmax_prob&quot;: `{`</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;avg_time&quot;: 0.13366999999999998,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;max_time&quot;: 0.338,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;min_time&quot;: 0.042</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &quot;task_latency&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;TaskPendingTime&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;avg_time&quot;: 0.04952,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;max_time&quot;: 0.12,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;min_time&quot;: 0.009</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;TaskRunningTime&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;avg_time&quot;: 3.870965,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;max_time&quot;: 4.48,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;min_time&quot;: 3.219</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}  &quot;max_time&quot;: 3.823,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;min_time&quot;: 3.057</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;CPU_inference_time_cost&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;avg_time&quot;: 0.29193,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;max_time&quot;: 0.708,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;min_time&quot;: 0.101</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &quot;model_latency&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;BPU_MOBILENET_subgraph_0&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;avg_time&quot;: 3.42556,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;max_time&quot;: 3.823,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;min_time&quot;: 3.057</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;Dequantize_fc7_1_HzDequantize&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;avg_time&quot;: 0.12307,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;max_time&quot;: 0.274,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;min_time&quot;: 0.044},</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;MOBILENET_subgraph_0_output_layout_convert&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;avg_time&quot;: 0.025945,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;max_time&quot;: 0.069,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;min_time&quot;: 0.012</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;Preprocess&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;avg_time&quot;: 0.009245,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;max_time&quot;: 0.027,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;min_time&quot;: 0.003</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;Softmax_prob&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;avg_time&quot;: 0.13366999999999998,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;max_time&quot;: 0.338,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;min_time&quot;: 0.042</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &quot;task_latency&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;TaskPendingTime&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;avg_time&quot;: 0.04952,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;max_time&quot;: 0.12,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;min_time&quot;: 0.009</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;TaskRunningTime&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;avg_time&quot;: 3.870965,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;max_time&quot;: 4.48,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &quot;min_time&quot;: 3.219</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span></code></pre></div></div>
<p>The above log corresponds to the bin visualization diagram in the <a href="#hb_perf"><strong>Estimating Performance using hb_perf</strong></a> section of the BIN Model Structure. Each node in the diagram has a corresponding node in the profiler.log file, which can be matched by the &quot;name&quot;. Additionally, the profiler.log file also records the execution time of each node, providing reference for optimizing model operators. Since the BPU nodes in the model have special requirements for input and output, such as special layout and padding alignment requirements, the input and output data of BPU nodes need to be processed.</p>
<ul>
<li>&quot;Preprocess&quot;: Indicates padding and layout conversion operations on model input data, and its time consumption is recorded in &quot;Preprocess&quot;.</li>
<li>&quot;xxxx_input_layout_convert&quot;: Indicates padding and layout conversion operations on the input data of BPU nodes, and its time consumption is recorded in &quot;xxxx_input_layout_convert&quot;.</li>
<li>&quot;xxxx_output_layout_convert&quot;: Indicates removal of padding and layout conversion operations on the output data of BPU nodes, and its time consumption is recorded in &quot;xxxx_output_layout_convert&quot;.
&quot;Profiler&quot; analysis is a commonly used operation in model performance tuning. As mentioned in the previous <a href="#check_result"><strong>Interpreting Check Results</strong></a> section, the CPU operators do not need to be focused on during the check phase. In this phase, the specific time consumption of CPU operators can be observed, and model performance tuning can be performed based on the time consumption of the corresponding operators.</li>
</ul>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Tips</div><div class="admonitionContent_BuS1"><p>If the model&#x27;s time consumption is severe, you can also optimize the performance in the following ways:</p><ol>
<li>Single-frame single-core: When a frame of data comes in, a model is used to perform inference on a single core.</li>
<li>Single-frame dual-core: The model is specified as a dual-core model during compilation (core_num: 2 in the yaml configuration file). After running, it will automatically occupy the resources of both cores. Then, when a frame of data comes in, it will be split into two parts and calculated separately, and finally reassembled. This mode has a more obvious optimization effect on large models and may increase latency. However, small models may actually slow down due to this dual-core scheduling.</li>
<li>Dual-frame dual-core: Two cores are used to independently process data frames using separate models. The latency will not be reduced, but the frame rate can reach about double.</li>
</ol></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="model-performance-optimization">Model Performance Optimization<a href="#model-performance-optimization" class="hash-link" aria-label="Direct link to Model Performance Optimization" title="Direct link to Model Performance Optimization">​</a></h4>
<p>Based on the performance analysis results above, you may find that the model performance is not as expected. This section introduces D-Robotics&#x27;s suggestions and measures to improve model performance, including checking YAML configuration parameters, handling CPU operators, high-performance model design suggestions, and using Horizon-friendly structures and models.</p>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><p>Some of the modification suggestions in this section may affect the parameter space of the original floating-point model. Therefore, you need to retrain the model. To avoid repeatedly adjusting and training the model during performance tuning, it is recommended that you use random parameters to export the model for performance verification until you obtain satisfactory model performance.</p></div></div>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="check-yaml-parameters-that-affect-model-performance">Check YAML parameters that affect model performance<a href="#check-yaml-parameters-that-affect-model-performance" class="hash-link" aria-label="Direct link to Check YAML parameters that affect model performance" title="Direct link to Check YAML parameters that affect model performance">​</a></h5>
<p>In the YAML configuration file for model conversion, some parameters actually affect the final performance of the model. Please check if they have been correctly configured according to the model expectations. For the specific meanings and functions of each parameter, please refer to the &quot;Compiler Parameters&quot; section.</p>
<ul>
<li><code>layer_out_dump</code>: Specifies whether to output intermediate results of the model during model conversion. This is generally only used for debugging purposes. If set to <code>True</code>, an additional dequantization output node will be added to each convolution operator, which will significantly reduce the performance of the model after it is deployed. Therefore, when evaluating performance, be sure to set this parameter to <code>False</code>.</li>
<li><code>compile_mode</code>: This parameter is used to select the optimization direction when compiling the model, which can be either &quot;bandwidth&quot; or &quot;latency&quot;. When focusing on performance, please set it to <code>latency</code>.</li>
<li><code>optimize_level</code>: This parameter is used to select the optimization level of the compiler. In practical use, it should be set to <code>O3</code> to achieve the best performance.</li>
<li><code>core_num</code>: <strong>Note:</strong> This parameter only applies to <strong>RDK X3</strong>. When set to <code>2</code>, it can run on two cores simultaneously, reducing the inference delay per frame, but it will also affect the overall throughput.</li>
<li><code>debug</code>: Setting this parameter to <code>True</code> will enable the debug mode of the compiler, which can output related information for performance simulation, such as frame rate and DDR bandwidth usage. It is generally used during performance evaluation. When delivering for commercialization, you can turn off this parameter to reduce the model size and improve model execution efficiency.</li>
<li><code>max_time_per_fc</code>: This parameter is used to control the execution duration of the function call of compiled model data instructions, thereby implementing model priority preemption. Modifying this parameter to change the execution duration of the preempted model&#x27;s function call will affect the performance of the model after deployment.</li>
</ul>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="handling-cpu-operators">Handling CPU operators<a href="#handling-cpu-operators" class="hash-link" aria-label="Direct link to Handling CPU operators" title="Direct link to Handling CPU operators">​</a></h5>
<p>According to the evaluation of the <code>hrt_model_exec perf</code> tool, if it can be confirmed that the performance bottleneck of the model is caused by CPU operators, in this case, it is recommended that you refer to the contents of the &quot;Supported Operator List&quot; section to check if the current CPU operator running on the CPU has the BPU support capability.</p>
<p>If the operator has the BPU support capability in the Supported Operator List, it means that the parameters of the operator exceed the constraints of BPU support. It is recommended that you adjust the corresponding original floating-point model calculation parameters to within the constraints. To quickly understand the specific parameters that exceed the constraints, it is recommended that you use the method described in the &quot;Model Check&quot; section to check again. The tool will directly provide parameter prompts that exceed the BPU support range.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><p>Modifying the parameters of the original floating-point model and its impact on model calculation accuracy needs to be controlled by yourself. For example, exceeding the range of parameters such as <code>input_channel</code> or <code>output_channel</code> of Convolution is a relatively typical case. After reducing the channels, the operator will be supported by BPU, but making only this modification may also affect the model accuracy.</p></div></div>
<p>If the operator does not have the BPU support capability, you need to perform corresponding optimization operations based on the following situations:</p>
<ul>
<li>
<p>CPU operator is in the middle of the model</p>
<p>For the case where the CPU operator is in the middle of the model, it is recommended that you try parameter adjustment, operator replacement, or modify the model first.</p>
</li>
<li>
<p>CPU operator is at the beginning or end of the model</p>
<p>For the case where the CPU operator is at the beginning or end of the model, please refer to the following example below, using quantization/dequantization nodes as an example:</p>
<ul>
<li>
<p>For nodes connected to the input and output of the model, you can add the &quot;remove_node_type&quot; parameter in the YAML file&#x27;s &quot;model_parameters&quot; configuration group and recompile the model.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  remove_node_type: &quot;Quantize; Dequantize&quot;</span><br></span></code></pre></div></div>
</li>
<li>
<p>The bin model can be modified using the hb_model_modifier tool:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  hb_model_modifier x.bin -a Quantize -a Dequantize</span><br></span></code></pre></div></div>
</li>
<li>
<p>For models like the one in the picture below that are not connected to input and output nodes, the hb_model_modifier tool needs to be used to determine if the connected nodes support deletion. The nodes can then be deleted one by one in order.</p>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/nodes_connected.png" alt="nodes_connected" class="img_ev3q"></p>
<p>First, use the hb_perf tool to get the model structure image, and then use the following two commands to remove the Quantize nodes from top to bottom. For the Dequantize nodes, they can be deleted one by one from bottom to top. The name of the node that can be deleted at each step can be checked using <code>hb_model_modifier x.bin</code>.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  hb_model_modifier x.bin -r res2a_branch1_NCHW2NHWC_LayoutConvert_Input0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  hb_model_modifier x_modified.bin -r data_res2a_branch1_HzQuantize</span><br></span></code></pre></div></div>
</li>
</ul>
</li>
</ul>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="suggestions-for-high-performance-model-design">Suggestions for High-Performance Model Design<a href="#suggestions-for-high-performance-model-design" class="hash-link" aria-label="Direct link to Suggestions for High-Performance Model Design" title="Direct link to Suggestions for High-Performance Model Design">​</a></h5>
<p>Based on performance evaluation results, the percentage of time consumed on the CPU may be small, indicating that the bottleneck is the long BPU inference time. In such cases, all BPU computing resources are already being used, so the next step is to improve resource utilization to optimize performance. Each processor has its own hardware characteristics, and how well the computation parameters of the algorithm model match the respective hardware characteristics directly affects the computational resource utilization. The higher the fit, the higher the utilization rate, and vice versa.</p>
<p>This section focuses on the hardware characteristics of D-Robotics Processors. D-Robotics provides processors designed to accelerate CNN (Convolutional Neural Network) processing, with the main computing resources focused on various convolution calculations. We recommend designing models that are primarily based on convolution calculations, as operators outside of convolution will reduce the utilization rate of computational resources, and the impact on performance varies depending on the specific OP.</p>
<ul>
<li>
<p><strong>Other Suggestions</strong></p>
<p>The computation efficiency of <code>depthwise convolution</code> on D-Robotics Processors is close to 100%, so for models like <code>MobileNet</code>, BPU has an efficiency advantage.</p>
<p>It is recommended to reduce the input and output dimensions of the BPU segment in model design to reduce the time consumed by quantization, dequantization nodes, and the bandwidth pressure on the hardware. For typical segmentation models, it is recommended to directly integrate the Argmax operator into the model itself. However, please note that Argmax can only be accelerated by BPU if the following conditions are met:</p>
<ol>
<li>In Caffe, the Softmax layer defaults to axis=1, while the ArgMax layer defaults to axis=0. The replacement operator should maintain consistency in the axis.</li>
<li>The channel of Argmax should be less than or equal to 64, otherwise it can only be computed on the CPU.</li>
</ol>
</li>
<li>
<p><strong>BPU-Oriented Efficiency Model Optimization</strong></p>
<p>D-Robotics&#x27;s BPU has targeted optimizations for <code>depthwise convolution</code> and <code>group convolution</code>. Therefore, we recommend using models with a Depthwise+Pointwise structure, such as MobileNetv2, EfficientNet_lite, and the custom-designed VarGNet based on GroupConv, as the backbone of the model to achieve higher performance benefits.</p>
<p>We are continually exploring more model structures and business models, and we will provide more diverse models for your reference. These outputs will be periodically updated to <a href="https://github.com/D-RoboticsRobotics-Platform/ModelZoo/tree/master" target="_blank" rel="noopener noreferrer">https://github.com/D-RoboticsRobotics-Platform/ModelZoo/tree/master</a>. If you still cannot find a suitable model, please feel free to reach out to us on the <a href="https://developer.d-robotics.cc" target="_blank" rel="noopener noreferrer">D-Robotics Official Technical Community</a>. We will provide more targeted guidance and suggestions based on your specific problems.</p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="accuracy_evaluation">Model Accuracy Analysis <code>{</code>#accuracy_evaluation<code>}</code><a href="#accuracy_evaluation" class="hash-link" aria-label="Direct link to accuracy_evaluation" title="Direct link to accuracy_evaluation">​</a></h3>
<p>PTQ post-quantization method based on tens or hundreds of calibration data unavoidably incurs certain precision loss. D-Robotics&#x27;s PTQ conversion tool has been extensively verified through practical use, and in most cases, the precision loss of the model can be kept within 1%.</p>
<p>This section first introduces how to correctly analyze the model precision. If the evaluation shows that the precision is lower than expected, you can refer to the content in the <strong>Precision Optimization</strong> section for model precision optimization.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="precision-analysis">Precision Analysis<a href="#precision-analysis" class="hash-link" aria-label="Direct link to Precision Analysis" title="Direct link to Precision Analysis">​</a></h4>
<p>As mentioned earlier, the output of a successfully converted model includes the following four parts:</p>
<ul>
<li>***_original_float_model.onnx</li>
<li>***_optimized_float_model.onnx</li>
<li>***_quantized_model.onnx</li>
<li>***.bin</li>
</ul>
<p>Although the final bin model is the one deployed on D-Robotics processors, for the convenience of quickly obtaining the model precision on Ubuntu/CentOS development machines, we also support using ***_quantized_model.onnx for precision testing. The quantized model ***_quantized_model.onnx has consistent precision performance with the bin model running on the X3 processor.</p>
<p>We recommend using the D-Robotics development library to load the ONNX model for inference. The basic process is as follows:</p>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><ol>
<li>
<p>The sample code is applicable to quantized models as well as original and optimized models. You can prepare the data for model inference according to the input types and layout requirements of different models.</p>
</li>
<li>
<p>It is recommended to refer to the precision validation method of the sample models in the &quot;horizon_model_convert_sample&quot; package for D-Robotics model conversion, such as caffe, onnx, etc.: &quot;04_inference.sh&quot; and &quot;postprocess.py&quot;.</p>
</li>
</ol></div></div>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># Load D-Robotics dependency libraries</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from horizon_tc_ui import HB_ONNXRuntime</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Prepare the feed_dict for model execution</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">def prepare_input_dict(input_names):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  feed_dict = dict()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  for input_name in input_names:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      # your_custom_data_prepare represents your custom data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      # Prepare the data based on the input node&#x27;s type and layout requirements</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      feed_dict[input_name] = your_custom_data_prepare(input_name)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  return feed_dict</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">if __name__ == &#x27;__main__&#x27;:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Create an inference session</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  sess = HB_ONNXRuntime(model_file=&#x27;***_quantized_model.onnx&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Get the input node names</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  input_names = [input.name for input in sess.get_inputs()]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # or</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  input_names = sess.input_names</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Get the output node names</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  output_names = [output.name for output in sess.get_outputs()]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # or</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  output_names = sess.output_names</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Prepare model input data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">feed_dict = prepare_input_dict(input_names)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Start model inference, the return value of inference is a list, which corresponds to the specified names in output_names one by one</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># The input image type range is (RGB/BGR/NV12/YUV444/GRAY)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">outputs = sess.run(output_names, feed_dict, input_offset=128)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># The input data type range is (FEATURE)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">outputs = sess.run_feature(output_names, feed_dict, input_offset=0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre></div></div>
<p>In the above code, the <code>input_offset</code> parameter has a default value of 128. For models with preprocessing nodes, a -128 operation is required here. If there is no preprocessing node before the model input, <code>input_offset</code> needs to be set as 0.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><p>For models with multiple inputs:</p><ul>
<li>
<p>If all input_type_rt belong to (RGB/BGR/NV12/YUV444/GRAY), you can use the <code>sess.run</code> method for inference.</p>
</li>
<li>
<p>If all input_type_rt belong to (FEATUREMAP), you can use the <code>sess.run_feature</code> method for inference.</p>
</li>
<li>
<p>Please note that currently, mixed input_type_rt of FEATUREMAP and non-FEATUREMAP is not supported to use sess.* methods for inference.</p>
</li>
</ul></div></div>
<p>In addition, the <code>your_custom_data_prepare</code> function, which represents the input data preparation process, is the part most prone to misoperation.
Compared with the accuracy verification process during the design and training of your original floating-point model, it is recommended that you adjust the inference input data after data preprocessing: mainly the data format (RGB, NV12, etc.), data precision (int8, float32, etc.), and data arrangement (NCHW or NHWC).
The adjustment method is determined by the four parameters <code>input_type_train</code>, <code>input_layout_train</code>, <code>input_type_rt</code>, and <code>input_layout_rt</code> set in the yaml configuration file during model conversion, and their detailed rules can be found in the <a href="#conversion_interpretation"><strong>Conversion Interpretation</strong></a> section.</p>
<p>For example, consider the original floating-point model trained with ImageNet for classification, which has only one input node. This node takes in a three-channel image in BGR order, and the input data layout is NCHW.
During the design and training of the original floating-point model, the data preprocessing before inference on the validation set is as follows:</p>
<ol>
<li>Scale the image proportionally to have a short side of 256 pixels.</li>
<li>Use the <code>center_crop</code> method to obtain a 224x224 image.</li>
<li>Subtract the mean value across channels.</li>
<li>Multiply the data by a scale factor.</li>
</ol>
<p>When using D-Robotics as the conversion tool for this original floating-point model, set <code>input_type_train</code> to <code>bgr</code>, <code>input_layout_train</code> to <code>NCHW</code>, <code>input_type_rt</code> to <code>bgr</code>, and <code>input_layout_rt</code> to <code>NHWC</code>.
According to the rules mentioned in the <a href="#conversion_interpretation"><strong>Conversion Interpretation</strong></a> section, the input accepted by ***_quantized_model.onnx should be bgr_128 with NHWC arrangement.
Based on the previous example code, the data processing in the <code>your_custom_data_prepare</code> part would be as follows:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># This example uses skimage, it may be different if you use OpenCV</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># It is worth noting that the transformers do not include the subtraction of mean and multiplication by scale</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># The mean and scale operations have been integrated into the model, refer to the configuration of norm_type/mean_value/scale_value mentioned earlier</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">def your_custom_data_prepare_sample(image_file):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Read the image using skimage, already in NHWC layout</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  image = skimage.img_as_float(skimage.io.imread(image_file))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Scale the image proportionally to have a short side of 256 pixels</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  image = ShortSideResize(image, short_size=256)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Use CenterCrop to obtain a 224x224 imageimage = CenterCrop(image, crop_size=224)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Skimage reads the channels in RGB order, we need BGR order for RGB2BGR conversion</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">image = RGB2BGR(image)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># If the original model uses NCHW format as input (except for input_type_rt with nv12), convert to CHW format</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">if layout == &quot;NCHW&quot;:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    image = HWC2CHW(image)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Skimage reads the values in the range of [0.0, 1.0], adjust to the range required for BGR</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">image = image * 255</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Subtract 128 for bgr_128</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">image = image - 128</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Convert to int8 for bgr_128</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">image = image.astype(np.int8)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">return image</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="precision-optimization">Precision Optimization<a href="#precision-optimization" class="hash-link" aria-label="Direct link to Precision Optimization" title="Direct link to Precision Optimization">​</a></h4>
<p>Based on the previous precision analysis, if you determine that the model&#x27;s quantization accuracy does not meet expectations, the following two main scenarios can be addressed:</p>
<ul>
<li>
<p><strong>Significant Loss (greater than 4%)</strong>
This issue is often caused by inappropriate YAML configuration or an imbalanced validation dataset. It is recommended to follow D-Robotics&#x27;s suggested steps for troubleshooting one by one.</p>
</li>
<li>
<p><strong>Small Loss (1.5% to 3%)</strong>
After excluding issues from scenario 1, if there&#x27;s still a slight loss in accuracy, it&#x27;s usually due to the model&#x27;s inherent sensitivity. In this case, use D-Robotics&#x27;s provided precision tuning tools for optimization.</p>
</li>
<li>
<p><strong>After Trying 1 and 2</strong>
If the precision still doesn&#x27;t meet expectations, try using our precision debugging tool for further attempts.</p>
</li>
</ul>
<p>The overall process for addressing precision issues is illustrated below:</p>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/accuracy_problem.png" alt="accuracy_problem" class="img_ev3q"></p>
<p><strong>Significant Loss (Greater than 4%)</strong></p>
<p>If the model suffers from a loss greater than 4%, it&#x27;s typically because of incorrect YAML configuration or an imbalanced calibration dataset. You should check the following aspects:</p>
<p><strong>Pipeline Check</strong></p>
<p>The pipeline refers to the entire process from data preprocessing, model conversion, inference, post-processing, to accuracy evaluation. Please refer to the corresponding chapters in the text for checks.</p>
<p>In practical problem-solving experiences, we&#x27;ve found that most issues arise from changes during the original float model training phase that aren&#x27;t promptly updated in the model conversion stage, leading to unexpected accuracy validation results.</p>
<p><strong>Model Conversion Configuration Check</strong></p>
<ul>
<li>
<p><code>input_type_rt</code> and <code>input_type_train</code>: These parameters determine the data format required for the mixed-heterogeneous post-converted model compared to the original float model. Ensure they match your expectations, especially regarding the BGR and RGB channel order.</p>
</li>
<li>
<p><code>norm_type</code>, <code>mean_value</code>, <code>scale_value</code>, etc. Verify that these parameters are correctly configured. Directly inserting mean and scale operations through the conversion configuration can lead to duplicate preprocessing, which is a common mistake.</p>
</li>
</ul>
<p><strong>Data Processing Consistency Check</strong></p>
<p>This part applies mainly to users who prepare calibration data and evaluation code using reference algorithm toolchain development packages. Common errors include:</p>
<ul>
<li>
<p>Incorrect <code>read_mode</code> specification: In <code>02_preprocess.sh</code>, you can specify the image reading method with the <code>--read_mode</code> parameter, supporting <code>opencv</code> and <code>skimage</code>. Similarly, in <code>preprocess.py</code>, ensure the <code>imread_mode</code> parameter is set correctly. Using <code>skimage</code> may read RGB channels with values between <code>0~1</code> as <code>float</code>, while <code>opencv</code> reads BGR with values between <code>0~255</code> as <code>uint8</code>.</p>
</li>
<li>
<p>Incorrect storage format for calibration datasets: D-Robotics uses <code>numpy.tofile</code> for saving calibration data, which does not preserve shape or type information. If <code>input_type_train</code> is not in &quot;featuremap&quot; format, the program will infer the data type based on whether the calibration data path contains &quot;f32&quot;. From X3 algorithm toolchain v2.2.3a, a new parameter <code>cal_data_type</code> has been added to set the binary file data storage type.</p>
</li>
<li>
<p>Inconsistent transformer implementation: D-Robotics provides common preprocessing functions in <code>/horizon_model_convert_sample/01_common/python/data/transformer.py</code>. Differences in ResizeTransformer implementation might exist, such as using OpenCV&#x27;s default interpolation method (linear). To modify other interpolation methods, edit the <code>transformer.py</code> source code and ensure consistency with the training-time preprocessing code. Refer to the <a href="/rdk_doc/en/FAQ/toolchain#transposetransformer"><strong>Transformer Usage</strong></a> section for more details.</p>
</li>
<li>
<p>Continue using the original float model&#x27;s data processing library during the D-Robotics algorithm toolchain. For less robust models, discrepancies in resize, crop, and other common functions across libraries can affect precision.</p>
</li>
<li>
<p>Ensure a reasonable validation image set. The calibration dataset should contain around &quot;a hundred&quot; images, covering various scenarios in the data distribution. For multi-task or multi-class models, the validation set should cover all prediction branches or classes.</p>
</li>
<li>
<p>Avoid using abnormal images that deviate from the data distribution, such as overexposed ones.</p>
</li>
<li>
<p>Re-validate the accuracy using the <code>***_original_float_model.onnx</code> model. Normally, this model&#x27;s accuracy should align to the original float model&#x27;s accuracy to three to five decimal places. If the alignment is not met, it indicates the need for a more thorough review of your data processing.</p>
</li>
</ul>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="minor-accuracy-improvement">Minor Accuracy Improvement<a href="#minor-accuracy-improvement" class="hash-link" aria-label="Direct link to Minor Accuracy Improvement" title="Direct link to Minor Accuracy Improvement">​</a></h5>
<p>In general, to reduce the difficulty of accuracy tuning, it is recommended that you set the <code>calibration_type</code> to <code>default</code>. Default is an automatic search function that selects the optimal calibration method based on the cosine similarity of the output nodes of the first calibration data, selecting from methods such as max, max-Percentile 0.99995, and KL. The selected calibration method can be found in the conversion log with a hint like &quot;Select kl method.&quot; If the accuracy results of the automatic search still do not meet expectations, the following suggestions can be tried for tuning:</p>
<p><strong>Adjust Calibration Method</strong></p>
<ul>
<li>
<p>Manually specify the <code>calibration_type</code>, which can be set to <code>kl/max</code>.</p>
</li>
<li>
<p>Set the calibration_type to max, and configure max_percentile to different percentiles (ranging from 0 to 1). We recommend trying 0.99999, 0.99995, 0.9999, 0.9995, 0.999, and observing the changes in model accuracy through these five configurations to find the best percentile.</p>
</li>
<li>
<p>Try enabling <code>per_channel</code> and use it in combination with any calibration method mentioned above.</p>
</li>
</ul>
<p><strong>Adjust Calibration Dataset</strong></p>
<ul>
<li>
<p>Try increasing or decreasing the data quantity appropriately (usually, fewer calibration data are required for detection scenarios compared to classification scenarios). Additionally, observe cases with missed detections in the model output and increase the calibration data for those scenarios.</p>
</li>
<li>
<p>Avoid using abnormal data such as pure black or pure white, and try to minimize the use of background images without targets as calibration data. Cover typical scenario tasks comprehensively to make the distribution of the calibration dataset approximate to the training dataset.</p>
</li>
</ul>
<p><strong>Retreat Some Tail Operators to High-precision CPU Computation</strong></p>
<ul>
<li>
<p>Usually, we only try to retreat &quot;1 to 2&quot; operators of the model output layer to the CPU. Having too many operators on the CPU will significantly affect the final model performance, and the judgment can be based on observing the cosine similarity of the model.</p>
</li>
<li>
<p>To specify operators running on the CPU, use the <code>run_on_cpu</code> parameter in the YAML file. Specify the node name to indicate the corresponding operator running on the CPU (e.g., run_on_cpu: conv_0).</p>
</li>
<li>
<p>If there is an error during model compilation after specifying run_on_cpu, please contact the D-Robotics technical support team.</p>
</li>
</ul>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="accuracy-debugging-toolsafter-trying-the-above-two-methods-for-accuracy-fine-tuning-if-your-accuracy-still-does-not-meet-expectations-we-provide-an-accuracy-debug-tool-to-help-you-locate-the-problem">Accuracy Debugging ToolsAfter trying the above two methods for accuracy fine-tuning, if your accuracy still does not meet expectations, we provide an accuracy debug tool to help you locate the problem.<a href="#accuracy-debugging-toolsafter-trying-the-above-two-methods-for-accuracy-fine-tuning-if-your-accuracy-still-does-not-meet-expectations-we-provide-an-accuracy-debug-tool-to-help-you-locate-the-problem" class="hash-link" aria-label="Direct link to Accuracy Debugging ToolsAfter trying the above two methods for accuracy fine-tuning, if your accuracy still does not meet expectations, we provide an accuracy debug tool to help you locate the problem." title="Direct link to Accuracy Debugging ToolsAfter trying the above two methods for accuracy fine-tuning, if your accuracy still does not meet expectations, we provide an accuracy debug tool to help you locate the problem.">​</a></h5>
<p>This tool can assist you in analyzing the quantization error of the calibration model at the node level and quickly identify nodes with accuracy issues.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Tips</div><div class="admonitionContent_BuS1"><p>If you are using the <strong>RDK Ultra</strong> product, you can also try precision tuning by configuring some ops to calculate in int16 ( <strong>RDK X3</strong> does not support int16 calculation for ops):</p><p>During the model conversion process, most ops are calculated using int8 data by default. In some cases, using int8 calculation for some ops may result in noticeable accuracy loss.
For <strong>RDK Ultra</strong> products, the algorithm toolchain already provides the ability to specify certain ops to calculate in int16 bit, as described in the <a href="#int16_config"><strong>int16 configuration</strong></a> parameter configuration. By configuring ops that are sensitive to quantization loss (with cosine similarity as a reference) to calculate in int16 bit, accuracy loss in some scenarios can be resolved.</p></div></div>
<p>During the process of model conversion, accuracy loss may occur due to the quantization process from floating point to fixed point. The main reasons for accuracy loss may include:</p>
<ol>
<li>
<p>Certain nodes in the model are sensitive to quantization and introduce large errors, referred to as sensitivity issues.</p>
</li>
<li>
<p>Accumulated errors in each node of the model result in large calibration errors for the overall model, mainly including: accumulated errors caused by weight quantization, accumulated errors caused by activation quantization, and accumulated errors caused by full quantization.</p>
</li>
</ol>
<p>In response to this situation, D-Robotics provides an accuracy debug tool to help you independently locate accuracy issues that occur during the model quantization process.
This tool can assist you in analyzing the quantization error of the calibration model at the node level and ultimately help you quickly identify nodes with accuracy exceptions.</p>
<p>The accuracy debug tool provides various analysis functions for your use, such as:</p>
<ul>
<li>
<p>Obtaining the quantization sensitivity of nodes.</p>
</li>
<li>
<p>Obtaining the accumulated error curve of the model.</p>
</li>
<li>
<p>Obtaining the data distribution of specified nodes.</p>
</li>
<li>
<p>Obtaining box plots of data distribution between input data channels of specified nodes, etc.</p>
</li>
</ul>
<h6 class="anchor anchorWithStickyNavbar_LWe7" id="instructions-for-use">Instructions for Use<a href="#instructions-for-use" class="hash-link" aria-label="Direct link to Instructions for Use" title="Direct link to Instructions for Use">​</a></h6>
<p>Using the accuracy debug tool mainly involves the following steps:</p>
<ol>
<li>
<p>In the <code>model parameters</code> section of the YAML file, configure the parameter <code>debug_mode=&quot;dump_calibration_data&quot;</code> to save the calibration data.</p>
</li>
<li>
<p>Import the debug module and load the calibration model and data.</p>
</li>
<li>
<p>Analyze the models with noticeable accuracy loss using the APIs or command line provided by the accuracy debug tool.</p>
</li>
</ol>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><p>For the current version of the accuracy debug tool: For the <strong>RDK Ultra</strong> corresponding to the <code>bayes</code> architecture model, both command line and API methods are supported for debugging, while for the <strong>RDK X3</strong> corresponding to the <code>bernoulli2</code> architecture model, only the API method is supported for debug.</p></div></div>
<p>The overall process is shown in the following diagram:</p>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/accuracy_debug_process.png" alt="accuracy_debug_process" class="img_ev3q"></p>
<p><strong>Saving Calibration Models and Data</strong></p>
<p>To enable the accuracy debug feature, the <code>debug_mode=&quot;dump_calibration_data&quot;</code> needs to be set in the YAML file, and the calibration data (calibration_data) and the corresponding calibrated model (calibrated_model.onnx) need to be saved. Specifically:</p>
<ol>
<li>
<p>Calibration Data (calibration_data): During the calibration phase, the model performs forward inference on these data to obtain the quantization parameters for each quantized node, including the scaling factor (scale) and threshold.</p>
</li>
<li>
<p>Calibrated Model (calibrated_model.onnx): The quantization parameters obtained during the calibration phase for each quantized node are saved in the calibration nodes to generate the calibrated model.</p>
</li>
</ol>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><p><strong>Difference between the saved calibration data here and the calibration data generated by 02_preprocess.sh:</strong></p><p>The calibration data obtained by <code>02_preprocess.sh</code> is data in the BGR color space, which will be transformed from BGR to the actual input format of the model, such as YUV444 or gray, within the toolchain.
The calibration data saved here is in the form of .npy after color space conversion and preprocessing. This data can be directly loaded using np.load() and input into the model for inference.</p></div></div>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><p><strong>Interpretation of Calibrated Model (calibrated_model.onnx):</strong></p><p>The calibrated model is an intermediate product obtained by the model transformation toolchain. It is obtained by calculating the quantization parameters for each node based on the calibration data and saving them in the calibration nodes.
The main characteristic of the calibrated model is that it contains calibration nodes, which have a node type of HzCalibration.
These calibration nodes are mainly divided into two categories: <strong>activation calibration nodes</strong> and <strong>weight calibration nodes</strong>.</p><p>The input of an <strong>activation calibration node</strong> is the output of the previous node, and the input data is quantized and then de-quantized based on the quantization parameters (scales and thresholds) saved in the activation calibration node.</p><p>The input of a <strong>weight calibration node</strong> is the original floating-point weights of the model, which are quantized and then de-quantized based on the quantization parameters (scales and thresholds) saved in the weight calibration node.</p><p>Other nodes in the calibrated model, excluding the above calibration nodes, are referred to as <strong>ordinary nodes</strong> by the accuracy debug tools.
The types of <strong>ordinary nodes</strong> include Conv, Mul, Add, etc.</p></div></div>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/debug_node.png" alt="debug_node" class="img_ev3q"></p>
<p>The folder structure of the calibration_data is as follows:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  |--calibration_data: Calibration Data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  |----input.1: The folder is named after the input nodes of the model, and the corresponding input data is saved in it.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  |--------0.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  |--------1.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  |-------- ...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  |----input.2: For models with multiple inputs, multiple folders will be saved.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  |--------0.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  |--------1.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  |-------- ...</span><br></span></code></pre></div></div>
<ul>
<li><strong>Importing and Using the Precision Debug Module</strong></li>
</ul>
<p>Next, you need to import the debug module in your code and use the <code>get_sensitivity_of_nodes</code> interface to retrieve the quantization sensitivity of nodes (defaulting to the cosine similarity of model outputs).</p>
<p>Details about the parameters of <code>get_sensitivity_of_nodes</code> can be found in the corresponding chapter.</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Import the debug module</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  import horizon_nn.debug as dbg</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Import the log module</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  import logging</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Set the log level to INFO if verbose=True</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  logging.getLogger().setLevel(logging.INFO)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Retrieve the quantization sensitivity of nodes</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  node_message = dbg.get_sensitivity_of_nodes(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          model_or_file=&#x27;./calibrated_model.onnx&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          metrics=[&#x27;cosine-similarity&#x27;, &#x27;mse&#x27;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          calibrated_data=&#x27;./calibration_data/&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          output_node=None,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          node_type=&#x27;node&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          data_num=None,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          verbose=True,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          interested_nodes=None)</span><br></span></code></pre></div></div>
<ul>
<li><strong>Analysis Results Display</strong></li>
</ul>
<p>Here are the printed results when <code>verbose=True</code>:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ==========================node==========================</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Node        cosine-similarity   mse</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --------------------------------------------------------</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Conv_3      0.999009567957658   0.027825591154396534</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  MaxPool_2   0.9993462241612948  0.017706592209064044</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Conv_6      0.9998359175828787  0.004541242333988731</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  MaxPool_5   0.9998616805443397  0.0038416787014844325</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Conv_0      0.9999297948984     0.0019312848587735342</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Gemm_19     0.9999609772975628  0.0010773885699633795</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Conv_8      0.9999629625907311  0.0010301886404004807</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Gemm_15     0.9999847687207736  0.00041888411550854263</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  MaxPool_12  0.9999853235024673  0.0004039733791544747</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Conv_10     0.999985763659844   0.0004040437432614943</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Gemm_17     0.9999913985912616  0.0002379088904350423</span><br></span></code></pre></div></div>
<p>In addition, the API will return the quantization sensitivity information of the nodes to you in the form of a dictionary (Dict) for subsequent use and analysis.</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Out: </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  `{`&#x27;Conv_3&#x27;: `{`&#x27;cosine-similarity&#x27;: &#x27;0.999009567957658&#x27;, &#x27;mse&#x27;: &#x27;0.027825591154396534&#x27;`}`, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   &#x27;MaxPool_2&#x27;: `{`&#x27;cosine-similarity&#x27;: &#x27;0.9993462241612948&#x27;, &#x27;mse&#x27;: &#x27;0.017706592209064044&#x27;`}`, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   &#x27;Conv_6&#x27;: `{`&#x27;cosine-similarity&#x27;: &#x27;0.9998359175828787&#x27;, &#x27;mse&#x27;: &#x27;0.004541242333988731&#x27;`}`, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   &#x27;MaxPool_5&#x27;: `{`&#x27;cosine-similarity&#x27;: &#x27;0.9998616805443397&#x27;, &#x27;mse&#x27;: &#x27;0.0038416787014844325&#x27;`}`, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   &#x27;Conv_0&#x27;: `{`&#x27;cosine-similarity&#x27;: &#x27;0.9999297948984&#x27;, &#x27;mse&#x27;: &#x27;0.0019312848587735342&#x27;`}`, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   &#x27;Gemm_19&#x27;: `{`&#x27;cosine-similarity&#x27;: &#x27;0.9999609772975628&#x27;, &#x27;mse&#x27;: &#x27;0.0010773885699633795&#x27;`}`, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   &#x27;Conv_8&#x27;: `{`&#x27;cosine-similarity&#x27;: &#x27;0.9999629625907311&#x27;, &#x27;mse&#x27;: &#x27;0.0010301886404004807&#x27;`}`, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   &#x27;Gemm_15&#x27;: `{`&#x27;cosine-similarity&#x27;: &#x27;0.9999847687207736&#x27;, &#x27;mse&#x27;: &#x27;0.00041888411550854263&#x27;`}`, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   &#x27;MaxPool_12&#x27;: `{`&#x27;cosine-similarity&#x27;: &#x27;0.9999853235024673&#x27;, &#x27;mse&#x27;: &#x27;0.0004039733791544747&#x27;`}`, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   &#x27;Conv_10&#x27;: `{`&#x27;cosine-similarity&#x27;: &#x27;0.999985763659844&#x27;, &#x27;mse&#x27;: &#x27;0.0004040437432614943&#x27;`}`, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   &#x27;Gemm_17&#x27;: `{`&#x27;cosine-similarity&#x27;: &#x27;0.9999913985912616&#x27;, &#x27;mse&#x27;: &#x27;0.0002379088904350423&#x27;`}``}`</span><br></span></code></pre></div></div>
<p>For more functions, please refer to the &quot;Function Description&quot; section.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Tips</div><div class="admonitionContent_BuS1"><p>The precision debugging tool can also be checked by the command <code>hmct-debugger -h/--help</code> to see the sub-commands corresponding to each function.
The detailed parameters and usage of each sub-command are described in the &quot;Function Description&quot; section.</p></div></div>
<h6 class="anchor anchorWithStickyNavbar_LWe7" id="function-description">Function Description<a href="#function-description" class="hash-link" aria-label="Direct link to Function Description" title="Direct link to Function Description">​</a></h6>
<ul>
<li><strong>get_sensitivity_of_nodes</strong></li>
</ul>
<p><strong>Function</strong>: Get the quantization sensitivity of the nodes.</p>
<p><strong>Command format</strong>:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  hmct-debugger get-sensitivity-of-nodes MODEL_OR_FILE CALIBRATION_DATA --other options</span><br></span></code></pre></div></div>
<p>You can use <code>hmct-debugger get-sensitivity-of-nodes -h/--help</code> to view related parameters.</p>
<p><strong>Parameter group</strong>:</p>
<table><thead><tr><th>Parameter Name</th><th>Parameter Description</th><th>Value Range</th><th>Optional/Required</th></tr></thead><tbody><tr><td><code>model_or_file</code></td><td><strong>Description</strong>: Specify the calibration model. <br><strong>Note</strong>: Required. Specify the calibration model to be analyzed.</td><td><strong>Range</strong>: N/A <br> <strong>Default</strong>: N/A.</td><td>Required</td></tr><tr><td><code>metrics 或 - m</code></td><td><strong>Description</strong>: The measurement method for quantization sensitivity of nodes.  <br><strong>Note</strong>: Specify the calculation method of the quantization sensitivity of nodes. This parameter can be a list, which means that the quantization sensitivity is calculated in multiple ways, but the output results are sorted based on the calculation method in the first position of the list. The higher the ranking, the larger the error introduced by quantizing the node.</td><td><strong>Range</strong>: <code>&#x27;cosine-similarity&#x27;</code> , <code>&#x27;mse&#x27;</code> , <code>&#x27;mre&#x27;</code> , <code>&#x27;sqnr&#x27;</code> , <code>&#x27;chebyshev&#x27;</code>  <br> <strong>Default</strong>: <code>&#x27;cosine-similarity&#x27;</code>.</td><td>Optional</td></tr><tr><td><code>calibrated_data</code></td><td><strong>Description</strong>: Specify the calibration data. <br><strong>Note</strong>: Required. Specify the calibration data needed for analysis.</td><td><strong>Range</strong>: N/A. <br> <strong>Default</strong>: N/A.</td><td>Required</td></tr><tr><td><code>output_node 或 -o</code></td><td><strong>Description</strong>: Specify the output node. <br><strong>Note</strong>: This parameter allows you to specify intermediate nodes as outputs and calculate the quantization sensitivity of the nodes. If the default parameter is None, the precision debugging tool will obtain the final output of the model and calculate the quantization sensitivity based on it.</td><td><strong>Range</strong>: Ordinary nodes with corresponding calibration nodes in the calibration model. <br> <strong>Default</strong>: None.</td><td>Optional</td></tr><tr><td><code>node_type 或 -n</code></td><td><strong>Description</strong>: Node type. <br><strong>Note</strong>: The type of node to calculate the quantization sensitivity, including: &#x27;node&#x27; (ordinary node), &#x27;weight&#x27; (weight calibration node), &#x27;activation&#x27; (activation calibration node).</td><td><strong>Range</strong>: <code>&#x27;node&#x27;</code> , <code>&#x27;weight&#x27;</code> , <code>&#x27;activation&#x27;</code>. <br> <strong>Default</strong>: <code>&#x27;node&#x27;</code>.</td><td>Optional</td></tr><tr><td><code>verbose or -v</code></td><td><strong>Parameter Function</strong>：Choose whether to print the information on the terminal.<br><strong>Parameter Description</strong>：If True, the quantization sensitivity information will be printed on the terminal. If there are multiple metrics in <code>metrics</code>, they will be sorted according to the first one.</td><td><strong>Value Range</strong>：<code>True</code> 、 <code>False</code>.<br> <strong>Default Configuration</strong>：<code>False</code>.</td><td>Optional</td></tr><tr><td><code>interested_nodes or -i</code></td><td><strong>Parameter Function</strong>：Set interested nodes.<br><strong>Parameter Description</strong>：If specified, only the quantization sensitivity of this node will be obtained, and the remaining nodes will not be obtained. At the same time, if this parameter is specified, the priority will be given to <code>interested_nodes</code> rather than <code>node_type</code>. If the default parameter None is maintained, the quantization sensitivity of all quantizable nodes in the model will be calculated.</td><td><strong>Value Range</strong>：All nodes in the calibrated model.<br> <strong>Default Configuration</strong>：None.</td><td>Optional</td></tr></tbody></table>
<p>Function Usage:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Import the debug module</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  import horizon_nn.debug as dbg</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Import the log module</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  import logging</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Set the log level to INFO if verbose=True</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  logging.getLogger().setLevel(logging.INFO)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Get the sensitivity of nodes</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  node_message = dbg.get_sensitivity_of_nodes(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          model_or_file=&#x27;./calibrated_model.onnx&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          metrics=[&#x27;cosine-similarity&#x27;, &#x27;mse&#x27;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          calibrated_data=&#x27;./calibration_data/&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          output_node=None,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          node_type=&#x27;node&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          data_num=None,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          verbose=True,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          interested_nodes=None)</span><br></span></code></pre></div></div>
<p>Command-line Usage:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  hmct-debugger get-sensitivity-of-nodes calibrated_model.onnx calibration_data -m [&#x27;cosine-similarity&#x27;,&#x27;mse&#x27;] -v True</span><br></span></code></pre></div></div>
<p><strong>Result Display</strong>：</p>
<p><strong>Description</strong>: First, you set the node type that needs to calculate the sensitivity through <code>node_type</code>. Then, the tool obtains all nodes in the calibrated model that meet <code>node_type</code> and obtains the quantization sensitivity of these nodes.
When verbose is set to True, the tool will print the quantization sensitivity of the nodes on the terminal after sorting them. The higher the sorting order, the greater the quantization error introduced by the node.</p>
<p>When verbose=True, the print result is as follows:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ==========================node==========================</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Node        cosine-similarity   mse</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --------------------------------------------------------</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Conv_3      0.999009567957658   0.027825591154396534</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  MaxPool_2   0.9993462241612948  0.017706592209064044</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Conv_6      0.9998359175828787  0.004541242333988731</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  MaxPool_5   0.9998616805443397  0.0038416787014844325</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Conv_0      0.9999297948984     0.0019312848587735342</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Gemm_19     0.9999609772975628  0.0010773885699633795</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Conv_8      0.9999629625907311  0.0010301886404004807</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Gemm_15     0.9999847687207736  0.00041888411550854263</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  MaxPool_12  0.9999853235024673  0.0004039733791544747</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Conv_10     0.999985763659844   0.0004040437432614943</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Gemm_17     0.9999913985912616  0.0002379088904350423</span><br></span></code></pre></div></div>
<p>Return:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">{&#x27;Conv_3&#x27;: `{`&#x27;cosine-similarity&#x27;: &#x27;0.999009567957658&#x27;, &#x27;mse&#x27;: &#x27;0.027825591154396534&#x27;`}`, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;MaxPool_2&#x27;: `{`&#x27;cosine-similarity&#x27;: &#x27;0.9993462241612948&#x27;, &#x27;mse&#x27;: &#x27;0.017706592209064044&#x27;`}`, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;Conv_6&#x27;: `{`&#x27;cosine-similarity&#x27;: &#x27;0.9998359175828787&#x27;, &#x27;mse&#x27;: &#x27;0.004541242333988731&#x27;`}`, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;MaxPool_5&#x27;: `{`&#x27;cosine-similarity&#x27;: &#x27;0.9998616805443397&#x27;, &#x27;mse&#x27;: &#x27;0.0038416787014844325&#x27;`}`, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;Conv_0&#x27;: `{`&#x27;cosine-similarity&#x27;: &#x27;0.9999297948984&#x27;, &#x27;mse&#x27;: &#x27;0.0019312848587735342&#x27;`}`, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;Gemm_19&#x27;: `{`&#x27;cosine-similarity&#x27;: &#x27;0.9999609772975628&#x27;, &#x27;mse&#x27;: &#x27;0.0010773885699633795&#x27;`}`, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;Conv_8&#x27;: `{`&#x27;cosine-similarity&#x27;: &#x27;0.9999629625907311&#x27;, &#x27;mse&#x27;: &#x27;0.0010301886404004807&#x27;`}`, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;Gemm_15&#x27;: `{`&#x27;cosine-similarity&#x27;: &#x27;0.9999847687207736&#x27;, &#x27;mse&#x27;: &#x27;0.00041888411550854263&#x27;`}`, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;MaxPool_12&#x27;: `{`&#x27;cosine-similarity&#x27;: &#x27;0.9999853235024673&#x27;, &#x27;mse&#x27;: &#x27;0.0004039733791544747&#x27;`}`, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;Conv_10&#x27;: `{`&#x27;cosine-similarity&#x27;: &#x27;0.999985763659844&#x27;, &#x27;mse&#x27;: &#x27;0.0004040437432614943&#x27;`}`, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;Gemm_17&#x27;: `{`&#x27;cosine-similarity&#x27;: &#x27;0.9999913985912616&#x27;, &#x27;mse&#x27;: &#x27;0.0002379088904350423&#x27;`}`} ...}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre></div></div>
<ul>
<li><strong>plot_acc_error</strong></li>
</ul>
<p>This is a function that quantifies a single node in a floating-point model and calculates the error between the output of this node in the model and the corresponding output in the floating-point model, generating a cumulative error curve.</p>
<p><strong>Command Line Format</strong>:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">hmct-debugger plot-acc-error MODEL_OR_FILE CALIBRATION_DATA --other options</span><br></span></code></pre></div></div>
<p>You can view related parameters by using <code>hmct-debugger plot-acc-error -h/--help</code>.</p>
<p><strong>Parameter Groups</strong>:</p>
<table><thead><tr><th>Parameter Name</th><th>Description</th><th>Value Range</th><th>Optional/Required</th></tr></thead><tbody><tr><td><code>save_dir</code> or <code>-s</code></td><td><strong>Purpose</strong>: Path to save the results.<br><strong>Description</strong>: Optional, specifies the path for saving the analysis results.</td><td>N/A</td><td>Optional</td></tr><tr><td><code>calibrated_data</code></td><td><strong>Purpose</strong>: Specifies calibration data.<br><strong>Description</strong>: Required, specifies the calibration data to be analyzed.</td><td>N/A</td><td>Required</td></tr><tr><td><code>model_or_file</code></td><td><strong>Purpose</strong>: Specifies the calibrated model.<br><strong>Description</strong>: Required, specifies the calibrated model to be analyzed.</td><td>N/A</td><td>Required</td></tr><tr><td><code>quantize_node</code> or <code>-q</code></td><td><strong>Purpose</strong>: Quantizes a specified node in the model and visualizes the cumulative error curve.<br><strong>Description</strong>: Optional parameter. Specifies the node in the model to be quantized, ensuring that all other nodes remain unquantized. The parameter can be a nested list to handle single or partial node quantization. Examples:<br>- quantize_node=[&#x27;Conv_2&#x27;,&#x27;Conv_9&#x27;]: Quantizes Conv_2 and Conv_9 individually while keeping others unquantized.<br>- quantize_node=[[&#x27;Conv_2&#x27;],[&#x27;Conv_9&#x27;,&#x27;Conv_2&#x27;]]: Tests model cumulative error with both Conv_2 and Conv_9 quantized separately.<br>- Special parameters: &#x27;weight&#x27; and &#x27;activation&#x27;.<br>When:<br>- quantize_node = [&#x27;weight&#x27;]: Only quantizes weights, not activations.<br>- quantize_node = [&#x27;activation&#x27;]: Only quantizes activations, not weights.<br>- quantize_node = [&#x27;weight&#x27;,&#x27;activation&#x27;]: Weights and activations are quantized separately.<br>Note: Both quantize_node and non_quantize_node cannot be None, one must be provided.</td><td>All nodes in the calibrated model.</td><td>Optional</td></tr><tr><td><code>non_quantize_node</code> or <code>-nq</code></td><td><strong>Purpose</strong>: Specifies the type of nodes for which to calculate cumulative error.<br><strong>Description</strong>: Optional parameter. Specifies nodes that should not be quantized, with all others quantized. Similar to <code>quantize_node</code>, it can be a nested list for single or partial node de-quantization. See the examples for <code>quantize_node</code>.<br>Note: Both quantize_node and non_quantize_node cannot be None, one must be provided.</td><td>All nodes in the calibrated model.</td><td>Optional</td></tr><tr><td><code>metric</code> or <code>-m</code></td><td><strong>Purpose</strong>: Error measurement method.<br><strong>Description</strong>: Sets the way to compute model errors.</td><td>Options: <code>&#x27;cosine-similarity&#x27;</code>, <code>&#x27;mse&#x27;</code>, <code>&#x27;mre&#x27;</code>, <code>&#x27;sqnr&#x27;</code>, <code>&#x27;chebyshev&#x27;</code></td><td>Optional</td></tr><tr><td><code>average_mode</code> or <code>-a</code></td><td><strong>Purpose</strong>: Specifies the output mode for the cumulative error curve.<br><strong>Description</strong>: Defaults to False. If set to True, the average of the cumulative errors is returned as the result.</td><td>Options: <code>True</code>, <code>False</code></td><td>Optional</td></tr></tbody></table>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Import the debug module</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  import horizon_nn.debug as dbg</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  dbg.plot_acc_error(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          save_dir: str,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          calibrated_data: str or CalibrationDataSet,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          model_or_file: ModelProto or str,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          quantize_node: List or str,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          non_quantize_node: List or str,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          metric: str = &#x27;cosine-similarity&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          average_mode: bool = False)</span><br></span></code></pre></div></div>
<p><strong>Analysis Result Display</strong></p>
<p><strong>1. Test the accumulated error of specific quantized nodes</strong></p>
<ul>
<li>Specify a single node for quantization</li>
</ul>
<p><strong>Configuration</strong>: quantize_node=[&#x27;Conv_2&#x27;, &#x27;Conv_90&#x27;], quantize_node is a single list.</p>
<p>API function usage:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Import the debug module</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  import horizon_nn.debug as dbg</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  dbg.plot_acc_error(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          save_dir=&#x27;./&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          calibrated_data=&#x27;./calibration_data/&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          model_or_file=&#x27;./calibrated_model.onnx&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          quantize_node=[&#x27;Conv_2&#x27;, &#x27;Conv_90&#x27;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          metric=&#x27;cosine-similarity&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          average_mode=False)</span><br></span></code></pre></div></div>
<p>Command line usage:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  hmct-debugger plot-acc-error calibrated_model.onnx calibrated_data -q [&#x27;Conv_2&#x27;,&#x27;Conv_90&#x27;]</span><br></span></code></pre></div></div>
<p><strong>Description</strong>: When the <code>quantize_node</code> is a single list, for the specified <code>quantize_node</code>, each node in the <code>quantize_node</code> is quantized separately while keeping other nodes in the model unquantized. After obtaining the corresponding model, the error between the output of each node in the model and the corresponding node in the floating-point model is calculated, resulting in an accumulated error curve.</p>
<p>When <code>average_mode = False</code>:</p>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/average_mode_false_1.png" alt="average_mode_false_1" class="img_ev3q"></p>
<p>When <code>average_mode = True</code>:</p>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/average_mode_true_1.png" alt="average_mode_true_1" class="img_ev3q"></p>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Caution</div><div class="admonitionContent_BuS1"><p><strong>average_mode</strong></p><p>The default value of <code>average_mode</code> is False. For some models, it is not possible to determine which quantization strategy is more effective based on the accumulated error curve. Therefore, <code>average_mode</code> needs to be set to True. In this mode, the average of the accumulated errors of the previous n nodes is used as the accumulated error of the nth node.</p><p>The specific calculation method is as follows, for example:</p><p>When <code>average_mode=False</code>, <code>accumulate_error=[1.0, 0.9, 0.9, 0.8]</code>.</p><p>When <code>average_mode=True</code>, <code>accumulate_error=[1.0, 0.95, 0.933, 0.9]</code>.</p></div></div>
<ul>
<li>Specify multiple nodes for quantization</li>
</ul>
<p><strong>Configuration</strong>: <code>quantize_node=[[&#x27;Conv_2&#x27;], [&#x27;Conv_2&#x27;, &#x27;Conv_90&#x27;]]</code>, where <code>quantize_node</code> is a nested list</p>
<p>API usage:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Import the debug module</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  import horizon_nn.debug as dbg</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  dbg.plot_acc_error(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          save_dir=&#x27;./&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          calibrated_data=&#x27;./calibration_data/&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          model_or_file=&#x27;./calibrated_model.onnx&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          quantize_node=[[&#x27;Conv_2&#x27;], [&#x27;Conv_2&#x27;, &#x27;Conv_90&#x27;]],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          metric=&#x27;cosine-similarity&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          average_mode=False)</span><br></span></code></pre></div></div>
<p>Command line usage:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">hmct-debugger plot-acc-error calibrated_model.onnx calibration_data -q [[&#x27;Conv_2&#x27;],[&#x27;Conv_2&#x27;,&#x27;Conv_90&#x27;]]</span><br></span></code></pre></div></div>
<p><strong>Description</strong>: When quantize_node is a nested list, for the quantize_node you set, each node specified in each sub-list of quantize_node is quantized while keeping other nodes in the model unquantized. After obtaining the corresponding model, calculate the error between the output of each node in the model and the corresponding node in the floating-point model, and obtain the corresponding cumulative error curve.</p>
<ul>
<li>partial_qmodel_0: Quantize only the Conv_2 node, other nodes are unquantized;</li>
<li>partial_qmodel_1: Quantize both the Conv_2 and Conv_90 nodes, other nodes are unquantized.</li>
</ul>
<p>When average_mode=False:</p>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/new_average_mode_false_1.png" alt="new_average_mode_false_1" class="img_ev3q"></p>
<p>When average_mode=True:</p>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/new_average_mode_true_1.png" alt="new_average_mode_true_1" class="img_ev3q"></p>
<p><strong>2. Accumulated error test after dequantizing partial nodes in the model</strong></p>
<ul>
<li>Specify single nodes to be unquantized</li>
</ul>
<p><strong>Configuration</strong>: non_quantize_node=[&#x27;Conv_2&#x27;, &#x27;Conv_90&#x27;], non_quantize_node is a single list.</p>
<p>API usage:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Import the debug module</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  import horizon_nn.debug as dbg</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  dbg.plot_acc_error(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          save_dir=&#x27;./&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          calibrated_data=&#x27;./calibration_data/&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          model_or_file=&#x27;./calibrated_model.onnx&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          non_quantize_node=[&#x27;Conv_2&#x27;, &#x27;Conv_90&#x27;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          metric=&#x27;cosine-similarity&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          average_mode=True)</span><br></span></code></pre></div></div>
<p>Command line usage:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  hmct-debugger plot-acc-error calibrated_model.onnx calibration_data -nq [&#x27;Conv_2&#x27;,&#x27;Conv_90&#x27;] -a True</span><br></span></code></pre></div></div>
<p><strong>Description</strong>: When non_quantize_node is a single list, for the non_quantize_node you set, each node specified in non_quantize_node is dequantized while keeping other nodes fully quantized. After obtaining the corresponding model,For each node in the model, calculate the error between its output and the corresponding output of the floating-point model, and obtain the cumulative error curve.</p>
<p>When average_mode = False:</p>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/average_mode_false_2.png" alt="average_mode_false_2" class="img_ev3q"></p>
<p>When average_mode = True:</p>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/average_mode_true_2.png" alt="average_mode_true_2" class="img_ev3q"></p>
<ul>
<li>Specifying multiple nodes as non-quantized</li>
</ul>
<p><strong>Configuration</strong>: non_quantize_node=[[&#x27;Conv_2&#x27;], [&#x27;Conv_2&#x27;, &#x27;Conv_90&#x27;]], where non_quantize_node is a nested list.</p>
<p>API usage:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Import the debug module</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  import horizon_nn.debug as dbg</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  dbg.plot_acc_error(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          save_dir=&#x27;./&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          calibrated_data=&#x27;./calibration_data/&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          model_or_file=&#x27;./calibrated_model.onnx&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          non_quantize_node=[[&#x27;Conv_2&#x27;], [&#x27;Conv_2&#x27;, &#x27;Conv_90&#x27;]],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          metric=&#x27;cosine-similarity&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          average_mode=False)</span><br></span></code></pre></div></div>
<p>Command line usage:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  hmct-debugger plot-acc-error calibrated_model.onnx calibration_data -nq [[&#x27;Conv_2&#x27;],[&#x27;Conv_2&#x27;,&#x27;Conv_90&#x27;]]</span><br></span></code></pre></div></div>
<p><strong>Description</strong>: When non_quantize_node is a nested list, for the non_quantize_node you set,
each single list in non_quantize_node specifies a node that is not quantized, while the rest of the nodes in the model are quantized.
After obtaining the corresponding model, calculate the error between the output of each node in the model and the corresponding output of the floating-point model,
and obtain the cumulative error curve.</p>
<ul>
<li>
<p>partial_qmodel_0: Conv_2 node is not quantized, while the rest of the nodes are quantized;</p>
</li>
<li>
<p>partial_qmodel_1: Conv_2 and Conv_90 nodes are not quantized, while the rest of the nodes are quantized.</p>
</li>
</ul>
<p>When average_mode = False:</p>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/new_average_mode_false_2.png" alt="new_average_mode_false_2" class="img_ev3q">When <code>average_mode = True</code>:</p>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/new_average_mode_true_2.png" alt="new_average_mode_true_2" class="img_ev3q"></p>
<p><strong>Testing Technique</strong>:</p>
<p>When quantifying accuracy in the testing phase, you may compare the accuracy of multiple quantization strategies based on their quantization sensitivity. You can refer to the following usage:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Import the debug module</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  import horizon_nn.debug as dbg</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Firstly, use the quantization sensitivity sorting function to get the quantization sensitivity sorting of the nodes in the model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  node_message = dbg.get_sensitivity_of_nodes(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          model_or_file=&#x27;./calibrated_model.onnx&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          metrics=&#x27;cosine-similarity&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          calibrated_data=&#x27;./calibration_data/&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          output_node=None,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          node_type=&#x27;node&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          verbose=False,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          interested_nodes=None)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Node_message is a dictionary, with its key being the name of the node</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  nodes = list(node_message.keys())</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Use `nodes` to specify the nodes that are not quantized, which can be easily used</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  dbg.plot_acc_error(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          save_dir=&#x27;./&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          calibrated_data=&#x27;./calibration_data/&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          model_or_file=&#x27;./calibrated_model.onnx&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          non_quantize_node=[nodes[:1],nodes[:2]],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          metric=&#x27;cosine-similarity&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          average_mode=True)</span><br></span></code></pre></div></div>
<p><strong>3. Quantize Activation and Weight Separately</strong></p>
<p><strong>Configuration Method</strong>: <code>quantize_node=[&#x27;weight&#x27;,&#x27;activation&#x27;]</code>.</p>
<p>API Usage:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  import horizon_nn.debug as dbg</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  dbg.plot_acc_error(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          save_dir=&#x27;./&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          calibrated_data=&#x27;./calibration_data/&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          model_or_file=&#x27;./calibrated_model.onnx&#x27;,quantize_node = [&#x27;weight&#x27;, &#x27;activation&#x27;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          metric = &#x27;cosine-similarity&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          average_mode = False)</span><br></span></code></pre></div></div>
<p>Command-line usage:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  hmct-debugger plot_acc_error calibrated_model.onnx calibration_data -q [&#x27;weight&#x27;, &#x27;activation&#x27;]</span><br></span></code></pre></div></div>
<p><strong>Description</strong>: quantize_node can also directly specify &#x27;weight&#x27; or &#x27;activation&#x27;. When:</p>
<ul>
<li>
<p>quantize_node = [&#x27;weight&#x27;]: only quantize weights, do not quantize activations.</p>
</li>
<li>
<p>quantize_node = [&#x27;activation&#x27;]: only quantize activations, do not quantize weights.</p>
</li>
<li>
<p>quantize_node = [&#x27;weight&#x27;, &#x27;activation&#x27;]: quantize weights and activations separately.</p>
</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/weight_activation_quantized.png" alt="weight_activation_quantized" class="img_ev3q"></p>
<ul>
<li><strong>plot_distribution</strong></li>
</ul>
<p><strong>Function</strong>: Select nodes, obtain the outputs of the nodes in both the floating-point model and the calibrated model, and get the output data distribution. In addition, subtract the two output results to obtain the error distribution between the two outputs.</p>
<p><strong>Command-line format</strong>:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  hmct-debugger plot-distribution MODEL_OR_FILE CALIBRATION_DATA --other options</span><br></span></code></pre></div></div>
<p>Use <code>hmct-debugger plot-distribution -h/--help</code> to view related parameters.</p>
<p><strong>Parameter groups</strong>:</p>
<table><thead><tr><th>Parameter Name</th><th>Parameter Configuration Explanation</th><th>Value Range Explanation</th><th>Optional/Required</th></tr></thead><tbody><tr><td><code>save_dir or -s</code></td><td><strong>Parameter Function</strong>: Save directory.<br><strong>Parameter Explanation</strong>: Optional, specify the save path of the analysis results.</td><td><strong>Value Range</strong>: None.<br> <strong>Default Configuration</strong>: None.</td><td>Optional</td></tr><tr><td><code>model_or_file</code></td><td><strong>Parameter Function</strong>: Specify the calibrated model.<br><strong>Parameter Explanation</strong>: Required, specify the calibrated model to be analyzed.</td><td><strong>Value Range</strong>: None.<br> <strong>Default Configuration</strong>: None.</td><td>Required</td></tr><tr><td><code>calibrated_data</code></td><td><strong>Parameter Function</strong>: Specify the calibrated data.<br><strong>Parameter Explanation</strong>: Required, specify the calibration data required for analysis.</td><td><strong>Value Range</strong>: None.<br> <strong>Default Configuration</strong>: None.</td><td>Required</td></tr><tr><td><code>nodes_list or -n</code></td><td><strong>Parameter Function</strong>: Specify the nodes to be analyzed.<br><strong>Parameter Explanation</strong>: Required, specify the nodes to be analyzed.<br>If the node type in nodes_list is:<br>- Weight calibration node: Draw the data distribution of the original weights and the calibrated weights. <br>- Activation calibration node: Draw the data distribution of the input of the activation calibration node.<br>- Normal node: Draw the output data distribution of the node before and after quantization, and draw the error distribution between the two.<br>Note: nodes_list is of type list, and a series of nodes can be specified, and the above three types of nodes can be specified at the same time.</td><td><strong>Value Range</strong>: All nodes in the calibrated model.<br> <strong>Default Configuration</strong>: None.</td><td>Required</td></tr></tbody></table>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Import the debug module</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  import horizon_nn.debug as dbg</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  dbg.plot_distribution(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    save_dir: str, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    model_or_file: ModelProto or str,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    calibrated_data: str or CalibrationDataSet,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    nodes_list: List[str] or str) </span><br></span></code></pre></div></div>
<p><strong>Analysis Result Display</strong>:</p>
<p>API Usage:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Import the debug module</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  import horizon_nn.debug as dbg</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  dbg.plot_distribution(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          save_dir=&#x27;./&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          model_or_file=&#x27;./calibrated_model.onnx&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          calibrated_data=&#x27;./calibration_data&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          nodes_list=[&#x27;317_HzCalibration&#x27;, # Activation node</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                      &#x27;471_HzCalibration&#x27;, # Weight node</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                      &#x27;Conv_2&#x27;]) # Regular node</span><br></span></code></pre></div></div>
<p>Command Line Usage:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  hmct-debugger plot-distribution calibrated_model.onnx calibration_data -n [&#x27;317_HzCalibration&#x27;,&#x27;471_HzCalibration&#x27;,&#x27;Conv_2&#x27;]</span><br></span></code></pre></div></div>
<p>node_output：</p>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/node_output.png" alt="node_output" class="img_ev3q"></p>
<p>weight：</p>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/weight.png" alt="weight" class="img_ev3q"></p>
<p>activation：</p>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/activation.png" alt="activation" class="img_ev3q"></p>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><p>In the three graphs above, the blue triangles represent the maximum absolute value of the data. The red dashed line represents the minimum calibration threshold.</p></div></div>
<ul>
<li><strong>get_channelwise_data_distribution****Function</strong>: Draw a box plot of the data distribution between specified calibration node input data channels.</li>
</ul>
<p><strong>Command line format</strong>:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  hmct-debugger get-channelwise-data-distribution MODEL_OR_FILE CALIBRATION_DATA --other options</span><br></span></code></pre></div></div>
<p>Related parameters can be viewed by <code>hmct-debugger get-channelwise-data-distribution -h/--help</code>.</p>
<p><strong>Parameter group</strong>:</p>
<table><thead><tr><th>Parameter Name</th><th>Parameter Configuration</th><th>Value Range Description</th><th>Optional/Required</th></tr></thead><tbody><tr><td><code>save_dir or -s</code></td><td><strong>Parameter purpose</strong>: Save path.<br><strong>Parameter description</strong>: Optional, specify the save path for analysis results.</td><td><strong>Value range</strong>: None.<br> <strong>Default configuration</strong>: None.</td><td>Optional</td></tr><tr><td><code>model_or_file</code></td><td><strong>Parameter purpose</strong>: Specify the calibration model. <br><strong>Parameter description</strong>: Required, specify the calibration model to be analyzed.</td><td><strong>Value Range</strong>: None.<br> <strong>Default configuration</strong>: None.</td><td>Required</td></tr><tr><td><code>calibrated_data</code></td><td><strong>Parameter purpose</strong>: Specify the calibration data.<br><strong>Parameter description</strong>: Required, specify the calibration data required for analysis.</td><td><strong>Value Range</strong>: None.<br> <strong>Default configuration</strong>: None.</td><td>Required</td></tr><tr><td><code>nodes_list or -n</code></td><td><strong>Parameter purpose</strong>: Specify the calibration node.<br><strong>Parameter description</strong>: Required, specify the calibration node.</td><td><strong>Value Range</strong>: All weight calibration nodes and activation calibration nodes in the calibration model.<br> <strong>Default configuration</strong>: None.</td><td>Required</td></tr><tr><td><code>axis or -a</code></td><td><strong>Parameter purpose</strong>: Specify the dimension where the channel is located.<br><strong>Parameter description</strong>: The position of the channel information in the shape. The parameter defaults to None. For activation calibration nodes, the second dimension of the input data is assumed to represent the channel information, that is, axis=1; For weight calibration nodes, the axis parameter in the node attribute will be read as the channel information.</td><td><strong>Value Range</strong>: Less than the dimension of the node input data. <br> <strong>Default configuration</strong>: None.</td><td>Optional</td></tr></tbody></table>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Import debug module</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  import horizon_nn.debug as dbg</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  dbg.get_channelwise_data_distribution(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          save_dir: str, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          model_or_file: ModelProto or str,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          calibrated_data: str or CalibrationDataSet,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          nodes_list: List[str],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          axis: int = None)</span><br></span></code></pre></div></div>
<p><strong>Analysis result display</strong>:</p>
<p><strong>Description</strong>: For the calibration node list set by the user, the dimension of the channel is obtained from the parameter axis, and the data distribution between the input data channels of the node is obtained.
The axis parameter defaults to None. If the node is a weight calibration node, the dimension where the channel is located is default to 0; if the node is an activation calibration node, the dimension where the channel is located is default to 1.</p>
<p>Weight calibration node:</p>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/weight_calibration_node.png" alt="weight_calibration_node" class="img_ev3q"></p>
<p>Activation calibration node:</p>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/activate_calibration_node.png" alt="activate_calibration_node" class="img_ev3q"></p>
<p>The output result is shown in the following figure:</p>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/box_plot.png" alt="box_plot" class="img_ev3q"></p>
<p>Here&#x27;s how the chart is structured:</p>
<ul>
<li>
<p>The x-axis represents the number of channels in the input data nodes, with the legend indicating that there are 96 channels.</p>
</li>
<li>
<p>The y-axis shows the range of data distribution for each channel. The red solid line denotes the median of the channel data, while the blue dashed line indicates the mean.</p>
</li>
<li>
<p><strong>runall</strong></p>
</li>
</ul>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>注意</div><div class="admonitionContent_BuS1"><p>runall can be used on only <strong>RDK Ultra</strong></p></div></div>
<p><strong>Command Line Format</strong>:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">hmct-debugger runall MODEL_OR_FILE CALIBRATION_DATA --other options</span><br></span></code></pre></div></div>
<p>You can view related parameters using <code>hmct-debugger runall -h/--help</code>.</p>
<p><strong>Parameter Groups</strong>:</p>
<table><thead><tr><th>Parameter Name</th><th>Description</th><th>Value Range</th><th>Optional/Required</th></tr></thead><tbody><tr><td><code>model_or_file</code></td><td>Specifies the calibrated model.<br> <strong>Description</strong>: Required, specifies the model to be analyzed.</td><td>N/A</td><td>Required</td></tr><tr><td><code>calibrated_data</code></td><td>Specifies the calibration data.<br> <strong>Description</strong>: Required, provides the data needed for analysis.</td><td>N/A</td><td>Required</td></tr><tr><td><code>save_dir</code> or <code>-s</code></td><td>Specifies the save path.<br> <strong>Description</strong>: Sets the path for saving the analysis results.</td><td>N/A</td><td>Optional</td></tr><tr><td><code>ns_metrics</code> or <code>-nm</code></td><td>Measures node quantization sensitivity.<br> <strong>Description</strong>: Defines the way to calculate node quantization sensitivity. It can be a list (List), calculating sensitivity in multiple ways but only the first method&#x27;s result is sorted. A higher ranking indicates greater error introduced by quantizing the node.</td><td>Values: <code>&#x27;cosine-similarity&#x27;</code>, <code>&#x27;mse&#x27;</code>, <code>&#x27;mre&#x27;</code>, <code>&#x27;sqnr&#x27;</code>, <code>&#x27;chebyshev&#x27;</code>.<br> <strong>Default</strong>: <code>&#x27;cosine-similarity&#x27;</code></td><td>Optional</td></tr><tr><td><code>output_node</code> or <code>-o</code></td><td>Specifies the output node.<br> <strong>Description</strong>: Allows you to specify an intermediate node for output and calculate its quantization sensitivity. If left default (None), the precision debugger uses the model&#x27;s final output to calculate sensitivity for all nodes.</td><td>N/A</td><td>Optional</td></tr><tr><td><code>node_type</code> or <code>-nt</code></td><td>Node type.<br> <strong>Description</strong>: The type of node to calculate sensitivity for, including: <code>node</code> (normal node), <code>weight</code> (weight calibration node), <code>activation</code> (activation calibration node).</td><td>Values: <code>&#x27;node&#x27;</code>, <code>&#x27;weight&#x27;</code>, <code>&#x27;activation&#x27;</code>.<br> <strong>Default</strong>: <code>&#x27;node&#x27;</code></td><td>Optional</td></tr><tr><td><code>data_num</code> or <code>-dn</code></td><td>Number of data points for sensitivity calculation.<br> <strong>Description</strong>: Sets the number of data points used for calculating node sensitivity. Defaults to None, using all data in <code>calibration_data</code>. Must be between 1 and the total number of calibration data points.</td><td>Range: Positive, less than or equal to the total calibration data count.<br> <strong>Default</strong>: None</td><td>Optional</td></tr><tr><td><code>verbose</code> or <code>-v</code></td><td>Enables terminal output.<br> <strong>Description</strong>: If set to True, displays sensitivity information on the terminal. If <code>metrics</code> contains multiple measures, they are sorted based on the first one.</td><td>Values: <code>True</code>, <code>False</code>.<br> <strong>Default</strong>: <code>False</code></td><td>Optional</td></tr><tr><td><code>interested_nodes</code> or <code>-i</code></td><td>Selects specific nodes of interest.<br> <strong>Description</strong>: If specified, only retrieves sensitivity for the specified nodes, ignoring others. Takes priority over <code>node_type</code> if set. If left default (None), calculates sensitivity for all quantifiable nodes in the model.</td><td>N/A</td><td>Optional</td></tr><tr><td><code>dis_nodes_list</code> or <code>-dnl</code></td><td>Analyzed nodes.<br> <strong>Description</strong>: Specifies the nodes to analyze. For weight calibration nodes, it shows the original and calibrated weights distribution. For activation calibration nodes, it displays input data distribution. For normal nodes, it shows output data distribution before and after quantization, along with the error distribution.</td><td>N/A</td><td>Optional</td></tr><tr><td><code>cw_nodes_list</code> or <code>-cn</code></td><td>Calibration nodes.<br> <strong>Description</strong>: Specifies calibration nodes.</td><td>N/A</td><td>Optional</td></tr><tr><td><code>axis</code> or <code>-a</code></td><td>Channel dimension.<br> <strong>Description</strong>: Specifies the position of the channel information within the node&#x27;s shape. Defaults to None, assuming axis=1 for activation calibration nodes, and reads the <code>axis</code> attribute from the node for weight calibration nodes.</td><td>Range: Less than the dimension of the node&#x27;s input data.<br> <strong>Default</strong>: None</td><td>Optional</td></tr><tr><td><code>quantize_node</code> or <code>-qn</code></td><td>Quantizes specified nodes, showing cumulative error curves.<br> <strong>Description</strong>: Optionally quantizes specified nodes, ensuring others remain unquantized. The parameter is a nested list to determine single-node or partial quantization. Examples: <code>quantize_node=[&#x27;Conv_2&#x27;,&#x27;Conv_9&#x27;]</code>, <code>quantize_node=[[&#x27;Conv_2&#x27;],[&#x27;Conv_9&#x27;,&#x27;Conv_2&#x27;]]</code>. Special values: <code>&#x27;weight&#x27;</code> and <code>&#x27;activation&#x27;</code>.<br> <strong>Range</strong>: All nodes in the calibrated model.<br> <strong>Default</strong>: None</td><td>Optional</td><td></td></tr><tr><td><code>non_quantize_node</code> or <code>-nqn</code></td><td>Specifies nodes not to quantify for cumulative error calculation.<br> <strong>Description</strong>: Optionally excludes specified nodes from quantization, ensuring all others are quantized. Follows similar logic as <code>quantize_node</code> with a nested list.<br> <strong>Range</strong>: All nodes in the calibrated model.<br> <strong>Default</strong>: None</td><td>Optional</td><td></td></tr><tr><td><code>ae_metric</code> or <code>-am</code></td><td>Cumulative error measurement.<br> <strong>Description</strong>: Specifies the method for calculating model error.</td><td>Values: <code>&#x27;cosine-similarity&#x27;</code>, <code>&#x27;mse&#x27;</code>, <code>&#x27;mre&#x27;</code>, <code>&#x27;sqnr&#x27;</code>, <code>&#x27;chebyshev&#x27;</code>.<br> <strong>Default</strong>: <code>&#x27;cosine-similarity&#x27;</code></td><td>Optional</td></tr><tr><td><code>average_mode</code> or <code>-avm</code></td><td>Cumulative error curve output mode.<br> <strong>Description</strong>: Defaults to False. If set to True, returns the average value of the cumulative error as the result.</td><td>Values: <code>True</code>, <code>False</code>.<br> <strong>Default</strong>: <code>False</code></td><td>Optional</td></tr></tbody></table>
<p><strong>API Usage Example</strong>:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> horizon_nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">debug </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> dbg</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">dbg</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">runall</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">model_or_file</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&#x27;calibrated_model.onnx&#x27;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">           calibrated_data</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&#x27;calibration_data&#x27;</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<p><strong>Command Line Usage Example</strong>:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">hmct-debugger runall calibrated_model.onnx calibration_data</span><br></span></code></pre></div></div>
<p>The <code>runall</code> workflow:</p>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/runall.png" alt="Runall Workflow" class="img_ev3q"></p>
<p>When all parameters are set to defaults, the tool performs the following steps:</p>
<ol>
<li>Calculates quantization sensitivity for weight and activation calibration nodes.</li>
<li>Draws data distributions for the top 5 nodes from step 1 for weight and activation calibration.</li>
<li>Creates box plots for channel-wise data distribution for the nodes from step 2.</li>
<li>Plots cumulative error curves when quantizing weights and activations separately.</li>
</ol>
<p>If <code>node_type=&#x27;node&#x27;</code>, the tool retrieves the top 5 nodes and their corresponding calibration nodes, displaying data distributions and box plots.</p>
<p>Based on previous optimization experience, this strategy covers most scenarios. If issues persist, follow the <a href="/rdk_doc/en/FAQ/toolchain#checklist"><strong>Precision Optimization Checklist</strong></a> to gather detailed model configuration information, ensure all troubleshooting steps have been completed, and report the filled checklist, the original float model file, and relevant configuration files to the D-Robotics support team or the <a href="https://developer.d-robotics.cc/" target="_blank" rel="noopener noreferrer"><strong>D-Robotics Official Technical Community</strong></a> for further assistance.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="other-tool-usage-instructions">Other Tool Usage Instructions<a href="#other-tool-usage-instructions" class="hash-link" aria-label="Direct link to Other Tool Usage Instructions" title="Direct link to Other Tool Usage Instructions">​</a></h3>
<p>This section primarily introduces the usage of debugging tools other than model conversion tools. These tools assist developers in model modification, analysis, and data preprocessing tasks. The list of tools is as follows:</p>
<ul>
<li>hb_perf</li>
<li>hb_pack</li>
<li>hb_model_info</li>
<li>hb_model_modifier</li>
<li>hb_model_verifier</li>
<li>hb_eval_preprocess</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="hb_perf-tool"><strong>hb_perf</strong> Tool<a href="#hb_perf-tool" class="hash-link" aria-label="Direct link to hb_perf-tool" title="Direct link to hb_perf-tool">​</a></h4>
<p><strong>hb_perf</strong> is a tool for analyzing the performance of D-Robotics&#x27;s quantized mixed models.</p>
<ul>
<li>Usage</li>
</ul>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">hb_perf [OPTIONS] BIN_FILE</span><br></span></code></pre></div></div>
<ul>
<li>Command Line Arguments</li>
</ul>
<p>hb_perf command-line arguments:</p>
<p>--version<br>
Displays the version and exits.</p>
<p>-m<br>
Followed by the model name. When specifying BIN_FILE as a packed model, only outputs information about the specified model.</p>
<p>--help<br>
Displays help information.</p>
<ul>
<li>Output Explanation</li>
</ul>
<p>The model information will be saved in the <code>hb_perf_result</code> folder in the current directory. There will be a subfolder with the same name as the model, containing an HTML file with its details. The directory structure would look like this example:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  hb_perf_result/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  └── mobilenetv1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      ├── mobilenetv1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      ├── mobilenetv1.html</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      ├── mobilenetv1.png</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      ├── MOBILENET_subgraph_0.html</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      ├── MOBILENET_subgraph_0.json</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      └── temp.hbm</span><br></span></code></pre></div></div>
<p>If the model was not compiled in debug mode (<code>compiler_parameters.debug:True</code>) during the build process, <code>hb_perf</code> may produce the following warning:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">2021-01-12 10:41:40,000 WARNING bpu model don&#x27;t have per-layer perf info.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2021-01-12 10:41:40,000 WARNING if you need per-layer perf info, please enable [compiler_parameters.debug:True] when using makertbin.</span><br></span></code></pre></div></div>
<p>This warning indicates that per-layer performance information is not included in the subgraph information but does not affect the generation of overall model information.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="hb_pack-tool">&quot;hb_pack&quot; Tool<a href="#hb_pack-tool" class="hash-link" aria-label="Direct link to &quot;hb_pack&quot; Tool" title="Direct link to &quot;hb_pack&quot; Tool">​</a></h4>
<p>&quot;hb_pack&quot; is a tool used to package multiple mixed model (*.bin) files into one model file.</p>
<ul>
<li>Usage</li>
</ul>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">hb_pack [OPTIONS] BIN_FILE1 BIN_FILE2 BIN_FILE3 -o comb.bin</span><br></span></code></pre></div></div>
<ul>
<li>Command Line Options</li>
</ul>
<p>Command line options for hb_pack</p>
<p>--version<br>
Display version and exit.</p>
<p>-o, --output_name<br>
The output name for the packed model.</p>
<p>--help<br>
Display help information.</p>
<ul>
<li>Output Description</li>
</ul>
<p>The packed model will be output in the current directory folder, and the model will be named with the specified name &quot;output_name&quot;.
The compilation information and performance information of all sub-models in the packed model can be obtained through &quot;hb_model_info&quot; and &quot;hb_perf&quot;.</p>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><p>Note that &quot;hb_pack&quot; does not support repackaging of a model that has already been packed. Otherwise, the workspace will generate the following prompt:</p></div></div>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">ERROR exception in command: pack</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">ERROR model: xxx.bin is a packed model, it can not be packed again!</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="hb_model_info-tool">&quot;hb_model_info&quot; Tool<a href="#hb_model_info-tool" class="hash-link" aria-label="Direct link to &quot;hb_model_info&quot; Tool" title="Direct link to &quot;hb_model_info&quot; Tool">​</a></h4>
<p>&quot;hb_model_info&quot; is a tool used to parse the dependency and parameter information of the mixed model (*.bin) during compilation.</p>
<ul>
<li>Usage</li>
</ul>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  hb_model_info $`{`model_file`}`</span><br></span></code></pre></div></div>
<ul>
<li>Command Line Options</li>
</ul>
<p>Command line options for hb_model_info</p>
<p>--version<br>Show version and exit.</p>
<p>-m<br>
Followed by the model name. When BIN_FILE is specified as the pack model, only the model compilation information of the specified model will be output.</p>
<p>--help<br>
Display help information.</p>
<ul>
<li>Description of Output</li>
</ul>
<p>The output section will be some input information during model compilation, as shown below:</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><p>The version number information in the code block below will change with the release package version. This is only an example.</p></div></div>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Start hb_model_info....</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">hb_model_info version 1.3.35</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">******** efficient_det_512x512_nv12 info *********</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">############# model deps info #############</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">hb_mapper version   : 1.3.35</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">hbdk version        : 3.23.3</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">hbdk runtime version: 3.13.7</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">horizon_nn version  : 0.10.10</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">############# model_parameters info #############</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">onnx_model          : /release/01_common/model_zoo/mapper/detection/efficient_det/efficientdet_nhwc.onnx</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">BPU march           : bernoulli2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">layer_out_dump      : False</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">working dir         : /release/04_detection/05_efficient_det/mapper/model_output</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">output_model_file_prefix: efficient_det_512x512_nv12</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">############# input_parameters info #############</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">------</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">---------input info : data ---------</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">input_name          : data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">input_type_rt       : nv12</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">input_space&amp;range   : regular</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">input_layout_rt     : None</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">input_type_train    : rgb</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">input_layout_train  : NCHW</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">norm_type           : data_mean_and_scale</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">input_shape         : 1x3x512x512</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">mean_value          : 123.68,116.779,103.939,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">scale_value         : 0.017,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cal_data_dir        : /release/04_detection/05_efficient_det/mapper/calibration_data_rgb_f32</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">---------input info : data end -------</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">------</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">############# calibration_parameters info #############</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">preprocess_on       : False</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">calibration_type    : max</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">############# compiler_parameters info #############hbdk_pass_through_params: --fast --O3</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">input-source        : `{`&#x27;data&#x27;: &#x27;pyramid&#x27;, &#x27;_default_value&#x27;: &#x27;ddr&#x27;`}`</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--------- input/output types -</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model input types   : [&lt;InputDataType.NV12: 7&gt;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model output types  : [&lt;InputDataType.F32: 5&gt;, &lt;InputDataType.F32: 5&gt;, &lt;InputDataType.F32: 5&gt;, &lt;InputDataTye.F32: 5&gt;, &lt;InputDataType.F32: 5&gt;, &lt;InputDataType.F32: 5&gt;, &lt;InputDataType.F32: 5&gt;, &lt;InputDataType.F32: 5&gt;, &lt;InputDataType.F32: 5&gt;, &lt;InpuDataType.F32: 5&gt;]</span><br></span></code></pre></div></div>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><p>When there are deleted nodes in the model, the names of the deleted nodes will be printed at the end of the model information output, and a file named &quot;deleted_nodes_info.txt&quot; will be generated, with each line recording the initial information of the corresponding deleted node. The names of the deleted nodes are shown below:</p></div></div>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">--------- deleted nodes -</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">deleted nodes: spconvretinanethead0_conv91_fwd_chw_HzDequantize</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">deleted nodes: spconvretinanethead0_conv95_fwd_chw_HzDequantize</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">deleted nodes: spconvretinanethead0_conv99_fwd_chw_HzDequantize</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">deleted nodes: spconvretinanethead0_conv103_fwd_chw_HzDequantize</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">deleted nodes: spconvretinanethead0_conv107_fwd_chw_HzDequantize</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">deleted nodes: spconvretinanethead0_conv93_fwd_chw_HzDequantize</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">deleted nodes: spconvretinanethead0_conv97_fwd_chw_HzDequantize</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">deleted nodes: spconvretinanethead0_conv101_fwd_chw_HzDequantize</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">deleted nodes: spconvretinanethead0_conv105_fwd_chw_HzDequantize</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">deleted nodes: spconvretinanethead0_conv109_fwd_chw_HzDequantize</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="hb_model_modifier-tool"><code>hb_model_modifier</code> Tool<a href="#hb_model_modifier-tool" class="hash-link" aria-label="Direct link to hb_model_modifier-tool" title="Direct link to hb_model_modifier-tool">​</a></h4>
<p>The <code>hb_model_modifier</code> tool is used to delete the Transpose, Quantize nodes on the input side and the Transpose, Dequantize, Cast, Reshape, Softmax nodes on the output side in the <code>*.bin</code> model.
The information of the deleted nodes is stored in the BIN model, and can be viewed using <code>hb_model_info</code>.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><ol>
<li>The hb_model_modifier tool can only delete nodes that are adjacent to the model input or output. If there are other nodes after the node to be deleted, it cannot be deleted.</li>
<li>The model node name should not include special characters such as &quot;;&quot; and &quot;,&quot; as this may affect the use of the tool.</li>
<li>The tool does not support processing the packed model, otherwise it will prompt: <code>ERROR pack model is not supported</code>.</li>
<li>The nodes to be deleted will be deleted one by one in order, and the model structure will be dynamically updated. Before deleting the node, it will also check if the node is at the input or output of the model, so the deletion order of the nodes is important.</li>
</ol></div></div>
<p>Since deleting specific nodes will affect the input of the model, the tool is only suitable for cases where there is only one path after the model input, as shown in the figure below.</p>
<p><img decoding="async" loading="lazy" src="https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/intermediate/hb_model_modifier.png" alt="hb_model_modifier" class="img_ev3q"></p>
<ul>
<li>Usage</li>
</ul>
<ol>
<li>View deletable nodes:</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  hb_model_modifier model.bin</span><br></span></code></pre></div></div>
<ol start="2">
<li>Delete a single specified node (node1 is taken as an example)</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  hb_model_modifier model.bin -r node1</span><br></span></code></pre></div></div>
<ol start="3">
<li>Delete multiple specified nodes (e.g., node1, node2, node3):</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  hb_model_modifier model.bin -r node1 -r node2 -r node3</span><br></span></code></pre></div></div>
<ol start="4">
<li>Delete nodes of a certain class (e.g., Dequantize):</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  hb_model_modifier model.bin --all Dequantize</span><br></span></code></pre></div></div>
<ol start="5">
<li>Delete multiple types of nodes (e.g., Reshape, Cast, Dequantize):</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  hb_model_modifier model.bin -a Reshape -a Cast -a Dequantize</span><br></span></code></pre></div></div>
<ol start="6">
<li>Combine usage:</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  hb_model_modifier model.bin -a Reshape -a Cast -a Dequantize -r node1 -r node2 -r node3</span><br></span></code></pre></div></div>
<ul>
<li>Command line arguments</li>
</ul>
<p>Command line arguments for hb_model_modifier</p>
<p>--model_file<br>
The name of the runtime model file.</p>
<p>-r<br>
Followed by the name of the node to be deleted. If multiple nodes need to be deleted, multiple -r options need to be specified.</p>
<p>-o<br>
Followed by the name of the modified model output (only effective when -r option is used).</p>
<p>-a --all<br>
Followed by the node type. Supports one-click deletion of all corresponding types. If multiple type nodes need to be deleted, multiple -a options need to be specified.</p>
<ul>
<li>Output description</li>
</ul>
<p>If the tool is not followed by any parameters, it will print out the deletable nodes available (i.e., all Transpose, Quantize, Dequantize, Cast, Reshape, Softmax nodes located at the input and output positions in the model).</p>
<p>The Quantize node is used to quantize the input data of the model from float type to int8 type, and its calculation formula is as follows:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  qx = clamp(round(x / scale) + zero\_point, -128, 127)</span><br></span></code></pre></div></div>
<p><code>round(x)</code> performs floating-point rounding, and the <code>clamp(x)</code> function clamps the data to integer values between -128 and 127. <code>zero_point</code> i#### `hb_model_verifiers the asymmetrical quantization zero point offset value, and for symmetrical quantization, <code>zero_point = 0</code>.</p>
<p>The reference implementation of C++ is as follows:</p>
<div class="language-cpp codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-cpp codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token keyword" style="color:#00009f">int64_t</span><span class="token plain"> quantized_value </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token keyword" style="color:#00009f">static_cast</span><span class="token operator" style="color:#393A34">/</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token keyword" style="color:#00009f">int64_t</span><span class="token operator" style="color:#393A34">/</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">std</span><span class="token double-colon punctuation" style="color:#393A34">::</span><span class="token function" style="color:#d73a49">round</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">value </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">static_cast</span><span class="token operator" style="color:#393A34">/</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token keyword" style="color:#00009f">double</span><span class="token operator" style="color:#393A34">/</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">scale</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  quantized_value </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> std</span><span class="token double-colon punctuation" style="color:#393A34">::</span><span class="token function" style="color:#d73a49">min</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">std</span><span class="token double-colon punctuation" style="color:#393A34">::</span><span class="token function" style="color:#d73a49">max</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">quantized_value</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> min_int_value</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> max_int_value</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><br></span></code></pre></div></div>
<p>The Dequantize node is used to dequantize the output data of type &quot;int8&quot; or &quot;int32&quot; back to &quot;float&quot; or &quot;double&quot; type, with the following formula:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  deqx = (x - zero\_point) * scale</span><br></span></code></pre></div></div>
<p>The reference implementation in C++ is as follows:</p>
<div class="language-cpp codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-cpp codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token keyword" style="color:#00009f">static_cast</span><span class="token operator" style="color:#393A34">/</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token keyword" style="color:#00009f">float</span><span class="token operator" style="color:#393A34">/</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">value</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> scale</span><br></span></code></pre></div></div>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Remark</div><div class="admonitionContent_BuS1"><p>Currently, the tool supports deletion of the following:</p><ol>
<li>The input side nodes are not Quantize or Transpose nodes;</li>
<li>The output side nodes are not Transpose, Dequantize, Cast, Reshape, or Softmax nodes.</li>
</ol></div></div>
<p>The tool prints the following information:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">hb_model_modifier resnet50_64x56x56_featuremap.bin</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2022-04-21 18:22:30,207 INFO Nodes that can be deleted: [&#x27;data_res2a_branch1_HzQuantize_TransposeInput0&#x27;, &#x27;fc1000_reshape_0&#x27;]</span><br></span></code></pre></div></div>
<p>After specifying the &quot;-r&quot; option, the tool will print the type of this node in the model, the information of the node stored in the bin file, and notify that the specified node has been deleted:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">hb_model_modifier resnet50_64x56x56_featuremap.bin -r data_res2a_branch1_HzQuantize_TransposeInput0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Node &#x27;data_res2a_branch1_HzQuantize_TransposeInput0&#x27; found, its OP type is &#x27;Transpose&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Node &#x27;data_res2a_branch1_HzQuantize_TransposeInput0&#x27; is removed</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">modified model saved as resnet50_64x56x56_featuremap_modified.bin</span><br></span></code></pre></div></div>
<p>Then, you can use the &quot;hb_model_info&quot; tool to view the information of the deleted nodes. The information will be printed at the end of the output, and a &quot;deleted_nodes_info.txt&quot; file will be generated. Each line in the file records the initial information of the corresponding deleted node. The following shows the names of the deleted nodes that are printed:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">hb_model_info resnet50_64x56x56_featuremap_modified.bin</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Start hb_model_info....</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">hb_model_info version 1.7.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">********* resnet50_64x56x56_featuremap info *********</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--------- deleted nodes -</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">deleted nodes: data_res2a_branch1_HzQuantize_TransposeInput0</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="hb_model_verifier-tool"><code>hb_model_verifier</code> Tool<a href="#hb_model_verifier-tool" class="hash-link" aria-label="Direct link to hb_model_verifier-tool" title="Direct link to hb_model_verifier-tool">​</a></h4>
<p>The <code>hb_model_verifier</code> tool is used to verify the results of the specified quantized model and the runtime model.
This tool performs inference on the specified images using the quantized model, as well as the runtime model on the board and on the x86 simulator. If the IP address of the board is specified and the <code>hrt_tools</code> is installed on the board (if not, you can use the <code>install.sh</code> script under <code>package/board</code> in the toolchain SDK package to install it), the tool will also perform inference on the runtime model on the board. Similarly, if the host has installed <code>hrt_tools</code> (if not, you can use the <code>install.sh</code> script under <code>package/host</code> in the toolchain SDK package to install it), the tool will perform inference on the runtime model on the x86 host. After the inference, the tool compares the results of the three models pairwise and gives a conclusion of whether the results match. If no image is specified, the tool will use default images for inference (random tensor data will be generated for feature map models).</p>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><p>To obtain the <code>package</code> data package, please refer to the <a href="#deliverables_instructions"><strong>Deliverables Instructions</strong></a>.</p></div></div>
<ul>
<li>Usage</li>
</ul>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  hb_model_verifier -q $`{`quanti_model`}` \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    -b $`{`bin_model`}` \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    -a $`{`board_ip`}` \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    -i $`{`input_img`}` \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    -d $`{`digits`}`</span><br></span></code></pre></div></div>
<ul>
<li>Command-line Arguments</li>
</ul>
<p>Command-line arguments of hb_model_verifier</p>
<p>-quanti_model, -q<br>
Name of the quantized model.</p>
<p>--bin_model, -b<br>
Name of the bin model.</p>
<p>--arm-board-ip, -a<br>
IP address of the ARM board used for on-board testing.</p>
<p>--input-img, -i<br>
Image used for inference testing. If not specified, default images or random tensors will be used. For binary image files, the suffix should be <code>.bin</code>.</p>
<p>--compare_digits, -d<br>
Number of decimal places to compare the inference results. If not specified, the tool will compare up to five decimal places by default.</p>
<ul>
<li>Output Explanation</li>
</ul>
<p>The results of the comparisons will be displayed in the terminal. The tool compares the ONNX model results, the simulator results, and the on-board results pairwise. If everything is fine, it should display the following:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  Quanti onnx and Arm result Strict check PASSED</span><br></span></code></pre></div></div>
<p>If there are inconsistencies between the quantized model and the runtime model, the specific information of the inconsistencies will be output.</p>
<p>The <code>mismatch line num</code> indicates the number of inconsistencies between the two types of models, including three types of inconsistencies:<code>mismatch.line_miss num</code> is the number of instances where the output results are inconsistent.</p>
<p><code>mismatch.line_diff num</code> is the number of instances where the difference between the output results is significant.</p>
<p><code>mismatch.line_nan num</code> is the number of instances where the output results are NaN.</p>
<p><code>total line num</code> is the total number of output data.</p>
<p><code>mismatch rate</code> is the ratio of inconsistent data instances to the total number of output data.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  INFO mismatch line num: 39</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  INFO ****************************</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  INFO mismatch.line_miss num: 0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  INFO mismatch.line_diff num: 39</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  INFO mismatch.line_nan num: 0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  INFO ****************************</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  INFO total line num: 327680</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  INFO mismatch rate: 0.0001190185546875</span><br></span></code></pre></div></div>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Note</div><div class="admonitionContent_BuS1"><ol>
<li><code>hb_model_verifier</code> currently only supports single-input models.</li>
<li>If the model has multiple outputs, only the results of the first output will be compared.</li>
<li>Validating already-packed *.bin models is not supported yet, otherwise the following prompt will be displayed in the workspace:</li>
</ol></div></div>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  ERROR pack model is not supported</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="hb_eval_preprocess"><code>hb_eval_preprocess</code> Tool <code>{</code>#hb_eval_preprocess<code>}</code><a href="#hb_eval_preprocess" class="hash-link" aria-label="Direct link to hb_eval_preprocess" title="Direct link to hb_eval_preprocess">​</a></h4>
<p>Used for preprocessing image data in an x86 environment before evaluating model accuracy.
Preprocessing refers to specific operations on image data before inputting it into the model.
For example: resizing, cropping, and padding of image data.</p>
<ul>
<li>Usage</li>
</ul>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  hb_eval_preprocess [OPTIONS]</span><br></span></code></pre></div></div>
<ul>
<li>Command-line Options</li>
</ul>
<p>Command-line options for hb_eval_preprocess</p>
<p>--version<br>
Display version and exit.</p>
<p>-m, --model_name<br>
Set the model name. Check the supported model ranges by using <code>hb_eval_preprocess --help</code>.</p>
<p>-i, --image_dir<br>Input image path.</p>
<p>-o, --output_dir<br>
Output directory.</p>
<p>-v, --val_txt<br>
Set the file name of the images required for evaluation. The preprocessed images will correspond to the image names in this file.</p>
<p>-h, --help<br>
Display help information.</p>
<ul>
<li>Output content description</li>
</ul>
<p>The &quot;hb_eval_preprocess&quot; command will generate image binary files in the directory specified by &quot;--output_dir&quot;.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Quick tip</div><div class="admonitionContent_BuS1"><p>For more examples of using the &quot;hb_eval_preprocess&quot; tool in on-board model accuracy evaluation, please refer to the <a href="/rdk_doc/en/Advanced_development/toolchain_development/intermediate/runtime_sample#data_preprocess"><strong>Data Preprocessing</strong></a> section in the &quot;General Model Evaluation Instructions&quot; of Embedded Application Development.</p></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2026-02-09T02:31:58.000Z" itemprop="dateModified">Feb 9, 2026</time></b></span></div></div></footer></article><nav class="pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/rdk_doc/en/Advanced_development/toolchain_development/intermediate/environment_config"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Environment Installation</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/rdk_doc/en/Advanced_development/toolchain_development/intermediate/supported_op_list"><div class="pagination-nav__sublabel">Next </div><div class="pagination-nav__label">supported_op_list</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#PTQ_introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#model_preparation" class="table-of-contents__link toc-highlight">Model Preparation</a></li><li><a href="#model_check" class="table-of-contents__link toc-highlight">Model Validation</a><ul><li><a href="#validate-the-model-using-the-hb_mapper-checker-tool" class="table-of-contents__link toc-highlight">Validate the model using the <code>hb_mapper checker</code> tool</a></li><li><a href="#handling-exceptions-in-checking" class="table-of-contents__link toc-highlight">Handling Exceptions in Checking</a></li><li><a href="#check_result" class="table-of-contents__link toc-highlight">Interpretation of the check results</a></li><li><a href="#optimization-guide-for-checking-results" class="table-of-contents__link toc-highlight">Optimization Guide for Checking Results</a></li></ul></li><li><a href="#model-conversion" class="table-of-contents__link toc-highlight">Model conversion</a><ul><li><a href="#preparing-calibration-data" class="table-of-contents__link toc-highlight">Preparing Calibration Data</a></li><li><a href="#makertbin" class="table-of-contents__link toc-highlight">Convert the Model Using the hb_mapper makertbin Tool</a></li><li><a href="#yaml_config" class="table-of-contents__link toc-highlight">Explanation of Parameters in Model Conversion YAML Configuration</a><ul><li><a href="#custom-operator-parameter-group" class="table-of-contents__link toc-highlight">Custom Operator Parameter Group</a></li><li><a href="#int16_config" class="table-of-contents__link toc-highlight">RDK Ultra int16 Configuration Instructions <code>{</code>#int16_config<code>}</code></a></li><li><a href="#pre_process" class="table-of-contents__link toc-highlight">Pre-processing HzPreprocess Operator Instructions <code>{</code>#pre_process<code>}</code></a></li></ul></li><li><a href="#conversion-internal-process-interpretation" class="table-of-contents__link toc-highlight">Conversion Internal Process Interpretation</a></li><li><a href="#interpretation-of-conversion-results" class="table-of-contents__link toc-highlight">Interpretation of Conversion Results</a></li><li><a href="#conversion_output" class="table-of-contents__link toc-highlight">Conversion Output Interpretation<code>{</code>#conversion_output<code>}</code></a></li></ul></li><li><a href="#performance_evaluation" class="table-of-contents__link toc-highlight">Model Performance Analysis<code>{</code>#performance_evaluation<code>}</code></a><ul><li><a href="#hb_perf" class="table-of-contents__link toc-highlight">Performance Evaluation on Development Machine<code>{</code>#hb_perf<code>}</code></a></li><li><a href="#performance-testing-on-development-board" class="table-of-contents__link toc-highlight">Performance Testing on Development Board</a></li><li><a href="#model-performance-optimization" class="table-of-contents__link toc-highlight">Model Performance Optimization</a><ul><li><a href="#check-yaml-parameters-that-affect-model-performance" class="table-of-contents__link toc-highlight">Check YAML parameters that affect model performance</a></li><li><a href="#handling-cpu-operators" class="table-of-contents__link toc-highlight">Handling CPU operators</a></li><li><a href="#suggestions-for-high-performance-model-design" class="table-of-contents__link toc-highlight">Suggestions for High-Performance Model Design</a></li></ul></li></ul></li><li><a href="#accuracy_evaluation" class="table-of-contents__link toc-highlight">Model Accuracy Analysis <code>{</code>#accuracy_evaluation<code>}</code></a><ul><li><a href="#precision-analysis" class="table-of-contents__link toc-highlight">Precision Analysis</a></li><li><a href="#precision-optimization" class="table-of-contents__link toc-highlight">Precision Optimization</a><ul><li><a href="#minor-accuracy-improvement" class="table-of-contents__link toc-highlight">Minor Accuracy Improvement</a></li><li><a href="#accuracy-debugging-toolsafter-trying-the-above-two-methods-for-accuracy-fine-tuning-if-your-accuracy-still-does-not-meet-expectations-we-provide-an-accuracy-debug-tool-to-help-you-locate-the-problem" class="table-of-contents__link toc-highlight">Accuracy Debugging ToolsAfter trying the above two methods for accuracy fine-tuning, if your accuracy still does not meet expectations, we provide an accuracy debug tool to help you locate the problem.</a></li></ul></li></ul></li><li><a href="#other-tool-usage-instructions" class="table-of-contents__link toc-highlight">Other Tool Usage Instructions</a><ul><li><a href="#hb_perf-tool" class="table-of-contents__link toc-highlight"><strong>hb_perf</strong> Tool</a></li><li><a href="#hb_pack-tool" class="table-of-contents__link toc-highlight">&quot;hb_pack&quot; Tool</a></li><li><a href="#hb_model_info-tool" class="table-of-contents__link toc-highlight">&quot;hb_model_info&quot; Tool</a></li><li><a href="#hb_model_modifier-tool" class="table-of-contents__link toc-highlight"><code>hb_model_modifier</code> Tool</a></li><li><a href="#hb_model_verifier-tool" class="table-of-contents__link toc-highlight"><code>hb_model_verifier</code> Tool</a></li><li><a href="#hb_eval_preprocess" class="table-of-contents__link toc-highlight"><code>hb_eval_preprocess</code> Tool <code>{</code>#hb_eval_preprocess<code>}</code></a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Links</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.guyuehome.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">GuYueHome<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Contact US</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/D-Robotics" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://www.youtube.com/@D-Robotics" target="_blank" rel="noopener noreferrer" class="footer__link-item">Youtube<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 D-Robotics.</div></div></div></footer></div>
</body>
</html>