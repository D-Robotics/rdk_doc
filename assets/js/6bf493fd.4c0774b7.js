"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[91151],{28453:(n,e,s)=>{s.d(e,{R:()=>t,x:()=>l});var i=s(96540);const r={},o=i.createContext(r);function t(n){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:t(n.components),i.createElement(o.Provider,{value:e},n.children)}},29753:(n,e,s)=>{s.r(e),s.d(e,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"Advanced_development/toolchain_development/expert/api_reference","title":"API\u624b\u518c","description":"March","source":"@site/docs/07_Advanced_development/04_toolchain_development/expert/api_reference.md","sourceDirName":"07_Advanced_development/04_toolchain_development/expert","slug":"/Advanced_development/toolchain_development/expert/api_reference","permalink":"/rdk_doc/Advanced_development/toolchain_development/expert/api_reference","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1751891522000,"sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"\u6df1\u5165\u63a2\u7d22","permalink":"/rdk_doc/Advanced_development/toolchain_development/expert/advanced_content"},"next":{"title":"\u9644\u5f55","permalink":"/rdk_doc/Advanced_development/toolchain_development/expert/note"}}');var r=s(74848),o=s(28453);const t={sidebar_position:5},l="API\u624b\u518c",a={},d=[{value:"March",id:"march",level:2},{value:"qconfig",id:"qconfig",level:2},{value:"qconfig \u5b9a\u4e49\u793a\u4f8b",id:"qconfig-\u5b9a\u4e49\u793a\u4f8b",level:3},{value:"\u4f2a\u91cf\u5316\u7b97\u5b50",id:"\u4f2a\u91cf\u5316\u7b97\u5b50",level:2},{value:"QAT",id:"qat",level:2},{value:"ONNX",id:"onnx",level:2},{value:"TorchScript \u6a21\u578b\u4fdd\u5b58\u548c\u52a0\u8f7d",id:"torchscript-\u6a21\u578b\u4fdd\u5b58\u548c\u52a0\u8f7d",level:2},{value:"Horizon \u7b97\u5b50",id:"horizon-\u7b97\u5b50",level:2},{value:"\u6a21\u578b\u7f16\u8bd1",id:"\u6a21\u578b\u7f16\u8bd1",level:2}];function c(n){const e={a:"a",admonition:"admonition",code:"code",del:"del",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"api\u624b\u518c",children:"API\u624b\u518c"})}),"\n",(0,r.jsx)(e.h2,{id:"march",children:"March"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"class horizon_plugin_pytorch.march.March\n"})}),"\n",(0,r.jsx)(e.p,{children:"BPU platform."}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"BAYES"}),": Bayes platform"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"BERNOULLI2"}),": Bernoulli2 platform"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"BAYES_E"}),": Bayes platform"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"qconfig",children:"qconfig"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.quantization.get_default_qconfig(activation_fake_quant: Optional[str] = 'fake_quant', weight_fake_quant: Optional[str] = 'fake_quant', activation_observer: Optional[str] = 'min_max', weight_observer: Optional[str] = 'min_max', activation_qkwargs: Optional[Dict] = None, weight_qkwargs: Optional[Dict] = None)\n\n"})}),"\n",(0,r.jsx)(e.p,{children:"Get default qconfig."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"activation_fake_quant"})," \u2013 FakeQuantize type of activation, default is fake_quant. Avaliable items are fake_quant, lsq, pact."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"weight_fake_quant"})," \u2013 FakeQuantize type of weight, default is fake_quant. Avaliable items are fake_quant, lsq and pact."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"activation_observer"})," \u2013 Observer type of activation, default is min_max. Avaliable items are min_max, fixed_scale, clip, percentile, clip_std, mse, kl."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"weight_observer"})," \u2013 Observer type of weight, default is min_max. Avaliable items are min_max, fixed_scale, clip, percentile, clip_std, mse."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"activation_qkwargs"})," \u2013 A dict contain activation Observer type, args of activation FakeQuantize and args of activation Observer."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"weight_qkwargs"})," \u2013 A dict contain weight Observer type, args of weight FakeQuantize and args of weight Observer."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"qconfig-\u5b9a\u4e49\u793a\u4f8b",children:"qconfig \u5b9a\u4e49\u793a\u4f8b"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"RDK X3"})," \u4f7f\u7528\u793a\u4f8b\u5982\u4e0b\uff1a"]}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'\ndefault_qat_8bit_fake_quant_qconfig = get_default_qconfig(\n    activation_fake_quant="fake_quant",\n    weight_fake_quant="fake_quant",\n    activation_observer="min_max",\n    weight_observer="min_max",\n    activation_qkwargs=None,\n    weight_qkwargs={"qscheme": torch.per_channel_symmetric, "ch_axis": 0,},\n    )\n\ndefault_qat_out_8bit_fake_quant_qconfig = get_default_qconfig(\n    activation_fake_quant=None,\n    weight_fake_quant="fake_quant",\n    activation_observer=None,\n    weight_observer="min_max",\n    activation_qkwargs=None,\n    weight_qkwargs={"qscheme": torch.per_channel_symmetric, "ch_axis": 0,},\n    )\n\ndefault_calib_8bit_fake_quant_qconfig = get_default_qconfig(\n    activation_fake_quant="fake_quant",\n    weight_fake_quant="fake_quant",\n    activation_observer="percentile",\n    weight_observer="min_max",\n    activation_qkwargs=None,\n    weight_qkwargs={"qscheme": torch.per_channel_symmetric, "ch_axis": 0,},\n    )\n\ndefault_calib_out_8bit_fake_quant_qconfig = (\n    default_qat_out_8bit_fake_quant_qconfig\n    )\n\ndefault_qat_8bit_lsq_quant_qconfig = get_default_qconfig(\n    activation_fake_quant="lsq",\n    weight_fake_quant="lsq",\n    activation_observer="min_max",\n    weight_observer="min_max",\n    activation_qkwargs={"use_grad_scaling": True, "averaging_constant": 1.0,},\n    weight_qkwargs={"qscheme": torch.per_channel_symmetric, "ch_axis": 0, "use_grad_scaling": True,"averaging_constant": 1.0,},\n    )\n\n'})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"RDK Ultra"})," \u548c ",(0,r.jsx)(e.strong,{children:"RDK X5"})," \u4f7f\u7528\u793a\u4f8b\u5982\u4e0b\uff1a"]}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'\ndefault_qat_8bit_fake_quant_qconfig = get_default_qconfig(\n        activation_fake_quant="fake_quant",\n        weight_fake_quant="fake_quant",\n        activation_observer="min_max",\n        weight_observer="min_max",\n        activation_qkwargs=None,\n        weight_qkwargs={"qscheme": torch.per_channel_symmetric, "ch_axis": 0,},\n        )\n\n    default_qat_8bit_weight_32bit_out_fake_quant_qconfig = get_default_qconfig(\n        activation_fake_quant=None,\n        weight_fake_quant="fake_quant",\n        activation_observer=None,\n        weight_observer="min_max",\n        activation_qkwargs=None,\n        weight_qkwargs={"qscheme": torch.per_channel_symmetric, "ch_axis": 0,},\n        )\n\n    default_calib_8bit_fake_quant_qconfig = get_default_qconfig(\n        activation_fake_quant="fake_quant",\n        weight_fake_quant="fake_quant",\n        activation_observer="percentile",\n        weight_observer="min_max",\n        activation_qkwargs=None,\n        weight_qkwargs={"qscheme": torch.per_channel_symmetric, "ch_axis": 0,},\n        )\n\n    default_calib_8bit_weight_32bit_out_fake_quant_qconfig = (\n        default_qat_out_8bit_fake_quant_qconfig\n        )\n\n    default_qat_8bit_weight_16bit_act_fake_quant_qconfig = get_default_qconfig(\n        activation_fake_quant="fake_quant",\n        weight_fake_quant="fake_quant",\n        activation_observer="min_max",\n        weight_observer="min_max",\n        activation_qkwargs={"dtype": qint16,},\n        weight_qkwargs={"qscheme": torch.per_channel_symmetric, "ch_axis": 0,},\n    )\n\n    default_calib_8bit_weight_16bit_act_fake_quant_qconfig = get_default_qconfig(\n        activation_fake_quant="fake_quant",\n        weight_fake_quant="fake_quant",\n        activation_observer="percentile",\n        weight_observer="min_max",\n        activation_qkwargs={"dtype": qint16,},\n        weight_qkwargs={"qscheme": torch.per_channel_symmetric, "ch_axis": 0,},\n    )\n\n'})}),"\n",(0,r.jsx)(e.h2,{id:"\u4f2a\u91cf\u5316\u7b97\u5b50",children:"\u4f2a\u91cf\u5316\u7b97\u5b50"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"class horizon_plugin_pytorch.quantization.FakeQuantize(observer: type = <class 'horizon_plugin_pytorch.quantization.observer.MovingAverageMinMaxObserver'>, saturate: bool = None, in_place: bool = False, compat_mask: bool = True, channel_len: int = 1, **observer_kwargs)\n"})}),"\n",(0,r.jsx)(e.p,{children:"Simulate the quantize and dequantize operations in training time."}),"\n",(0,r.jsx)(e.p,{children:"The output of this module is given by"}),"\n",(0,r.jsx)(e.p,{children:"fake_quant_x = clamp(floor(x / scale + 0.5), quant_min, quant_max) * scale # noqa"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:"scale defines the scale factor used for quantization."}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:"zero_point specifies the quantized value to which 0 in floating point maps to"}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:"quant_min specifies the minimum allowable quantized value."}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:"quant_max specifies the maximum allowable quantized value."}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:"fake_quant_enabled controls the application of fake quantization on tensors, note that statistics can still be updated."}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:"observer_enabled controls statistics collection on tensors"}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:"dtype specifies the quantized dtype that is being emulated with fake-quantization, the allowable values is qint8 and qint16. The values of quant_min and quant_max should be chosen to be consistent with the dtype"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"observer"})," \u2013 Module for observing statistics on input tensors and calculating scale and zero-point."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"saturate"})," \u2013 Whether zero out the grad for value out of quanti range."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"in_place"})," \u2013 Whether use in place fake quantize."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"compat_mask"})," \u2013 Whether pack the bool mask into bitfield when saturate = True."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"channel_len"})," \u2013 Size of data at channel dim."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"observer_kwargs"})," \u2013 Arguments for the observer module"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"observer\n"})}),"\n",(0,r.jsx)(e.p,{children:"User provided module that collects statistics on the input tensor and provides a method to calculate scale and zero-point."}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"extra_repr()\n"})}),"\n",(0,r.jsx)(e.p,{children:"Set the extra representation of the module"}),"\n",(0,r.jsx)(e.p,{children:"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable."}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"forward(x)\n"})}),"\n",(0,r.jsx)(e.p,{children:"Defines the computation performed at every call."}),"\n",(0,r.jsx)(e.p,{children:"Should be overridden by all subclasses."}),"\n",(0,r.jsx)(e.admonition,{title:"\u6ce8\u89e3",type:"info",children:(0,r.jsx)(e.p,{children:"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them."})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"set_qparams(scale: Union[torch.Tensor, Sequence, float], zero_point: Optional[Union[torch.Tensor, Sequence, int]] = None)\n"})}),"\n",(0,r.jsx)(e.p,{children:"Set qparams, default symmetric."}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"classmethod with_args(**kwargs)\n"})}),"\n",(0,r.jsx)(e.p,{children:"Wrapper that allows creation of class factories."}),"\n",(0,r.jsx)(e.p,{children:"This can be useful when there is a need to create classes with the same constructor arguments, but different instances. Can be used in conjunction with _callable_args"}),"\n",(0,r.jsx)(e.p,{children:"Example:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'>>> # xdoctest: +SKIP("Undefined vars")\n>>> Foo.with_args = classmethod(_with_args)\n>>> foo_builder = Foo.with_args(a=3, b=4).with_args(answer=42)\n>>> foo_instance1 = foo_builder()\n>>> foo_instance2 = foo_builder()\n>>> id(foo_instance1) == id(foo_instance2)\nFalse\n'})}),"\n",(0,r.jsx)(e.h2,{id:"qat",children:"QAT"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.quantization.convert(module: torch.nn.modules.module.Module, mapping: Optional[Dict[Type[torch.nn.modules.module.Module], Type[torch.nn.modules.module.Module]]] = None, inplace: bool = False, remove_qconfig: bool = True, fast_mode: bool = False)\n"})}),"\n",(0,r.jsx)(e.p,{children:"Convert modules."}),"\n",(0,r.jsx)(e.p,{children:"Convert submodules in input module to a different module according to mapping by calling from_float method on the target module class. And remove qconfig at the end if remove_qconfig is set to True."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"module"})," \u2013 input module"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"mapping"})," \u2013 a dictionary that maps from source module type to target module type, can be overwritten to allow swapping user defined Modules"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"inplace"})," \u2013 carry out model transformations in-place, the original module is mutated"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"fast_mode"})," \u2013 whether to accelerate quantized model forward. If set True, quantized model cannot be compiled"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.quantization.convert_fx(graph_module: torch.fx.graph_module.GraphModule, inplace: bool = False, convert_custom_config_dict: Optional[Dict[str, Any]] = None, _remove_qconfig: bool = True, fast_mode: bool = False) \u2192 horizon_plugin_pytorch.quantization.fx.graph_module.QuantizedGraphModule\n"})}),"\n",(0,r.jsx)(e.p,{children:"Convert a calibrated or trained model to a quantized model."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"graph_module"})," \u2013 A prepared and calibrated/trained model (GraphModule)"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"inplace"})," \u2013 Carry out model transformations in-place, the original module is mutated."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"convert_custom_config_dict"})," \u2013"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"dictionary for custom configurations for convert function:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'convert_custom_config_dict = {\n    # We automativally preserve all attributes, this option is\n    # just in case and not likely to be used.\n    "preserved_attributes": ["preserved_attr"],\n}\n'})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"_remove_qconfig"})," \u2013 Option to remove the qconfig attributes in the model after convert. for internal use only."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"fast_mode"})," \u2013 whether to accelerate quantized model forward. If set True, quantized model cannot be compiled."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"A quantized model (GraphModule)"}),"\n",(0,r.jsx)(e.p,{children:"Example: convert fx example:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"# prepared_model: the model after prepare_fx/prepare_qat_fx and\n# calibration/training\nquantized_model = convert_fx(prepared_model)\n"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.quantization.fuse_fx(model: torch.nn.modules.module.Module, fuse_custom_config_dict: Optional[Dict[str, Any]] = None) \u2192 horizon_plugin_pytorch.quantization.fx.graph_module.GraphModuleWithAttr\n"})}),"\n",(0,r.jsx)(e.p,{children:"Fuse modules like conv+add+bn+relu etc."}),"\n",(0,r.jsx)(e.p,{children:"Fusion rules are defined in horizon_plugin_pytorch.quantization.fx.fusion_pattern.py"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"model"})," \u2013 a torch.nn.Module model"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"fuse_custom_config_dict"})," \u2013"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"Dictionary for custom configurations for fuse_fx, e.g."}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'fuse_custom_config_dict = {\n    # We automativally preserve all attributes, this option is\n    # just in case and not likely to be used.\n    "preserved_attributes": ["preserved_attr"],\n}\n'})}),"\n",(0,r.jsx)(e.p,{children:"Example: fuse_fx example:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"from torch.quantization import fuse_fx\nm = fuse_fx(m)\n"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.quantization.fuse_known_modules(mod_list, is_qat=False, additional_fuser_method_mapping=None)\n"})}),"\n",(0,r.jsx)(e.p,{children:"Fuse modules."}),"\n",(0,r.jsx)(e.p,{children:"Return a list of modules that fuses the operations specified in the input module list."}),"\n",(0,r.jsx)(e.p,{children:"Fuses only the following sequence of modules: conv, bn; conv, bn, relu; conv, relu; conv, bn, add; conv, bn, add, relu; conv, add; conv, add, relu; linear, bn; linear, bn, relu; linear, relu; linear, bn, add; linear, bn, add, relu; linear, add; linear, add, relu. For these sequences, the first element in the output module list performs the fused operation. The rest of the elements are set to nn.Identity()"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.quantization.fuse_modules(model, modules_to_fuse, inplace=False, fuser_func=<function fuse_known_modules>, fuse_custom_config_dict=None)\n"})}),"\n",(0,r.jsx)(e.p,{children:"Fuses a list of modules into a single module."}),"\n",(0,r.jsx)(e.p,{children:"Fuses only the following sequence of modules: conv, bn; conv, bn, relu; conv, relu; conv, bn, add; conv, bn, add, relu; conv, add; conv, add, relu; linear, bn; linear, bn, relu; linear, relu; linear, bn, add; linear, bn, add, relu; linear, add; linear, add, relu. For these sequences, the first element in the output module list performs the fused operation. The rest of the elements are set to nn.Identity()"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"model"})," \u2013 Model containing the modules to be fused"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"modules_to_fuse"})," \u2013 list of list of module names to fuse. Can also be a list of strings if there is only a single list of modules to fuse."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"inplace"})," \u2013 bool specifying if fusion happens in place on the model, by default a new model is returned"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"fuser_func"})," \u2013 Function that takes in a list of modules and outputs a list of fused modules of the same length. For example, fuser_func([convModule, BNModule]) returns the list [ConvBNModule, nn.Identity()] Defaults to torch.ao.quantization.fuse_known_modules"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"fuse_custom_config_dict"})," \u2013 custom configuration for fusion"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# Example of fuse_custom_config_dict\nfuse_custom_config_dict = {\n    # Additional fuser_method mapping\n    "additional_fuser_method_mapping": {\n        (torch.nn.Conv2d, torch.nn.BatchNorm2d): fuse_conv_bn\n    },\n}\n'})}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"model with fused modules. A new copy is created if inplace=True."}),"\n",(0,r.jsx)(e.p,{children:"Examples:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:">>> # xdoctest: +SKIP\n>>> m = M().eval()\n>>> # m is a module containing the sub-modules below\n>>> modules_to_fuse = [ ['conv1', 'bn1', 'relu1'],\n                      ['submodule.conv', 'submodule.relu']]\n>>> fused_m = fuse_modules(\n                m, modules_to_fuse)\n>>> output = fused_m(input)\n\n>>> m = M().eval()\n>>> # Alternately provide a single list of modules to fuse\n>>> modules_to_fuse = ['conv1', 'bn1', 'relu1']\n>>> fused_m = fuse_modules(\n                m, modules_to_fuse)\n>>> output = fused_m(input)\n"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.quantization.prepare_qat(model: torch.nn.modules.module.Module, mapping: Optional[Dict[Type[torch.nn.modules.module.Module], Type[torch.nn.modules.module.Module]]] = None, inplace: bool = False, optimize_graph: bool = False, hybrid: bool = False, optimize_kwargs: Optional[Dict[str, Tuple]] = None, example_inputs: Any = None, qconfig_setter: Optional[Union[Tuple[horizon_plugin_pytorch.quantization.qconfig_template.QconfigSetterBase, ...], horizon_plugin_pytorch.quantization.qconfig_template.QconfigSetterBase]] = None, verbose: int = 0)\n\n"})}),"\n",(0,r.jsx)(e.p,{children:"Prepare qat."}),"\n",(0,r.jsx)(e.p,{children:"Prepare a copy of the model for quantization-aware training and converts it to quantized version."}),"\n",(0,r.jsx)(e.p,{children:"Quantization configuration should be assigned preemptively to individual submodules in .qconfig attribute."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"model"})," \u2013 input model to be modified in-place"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"mapping"})," \u2013 dictionary that maps float modules to quantized modules to be replaced."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"inplace"})," \u2013 carry out model transformations in-place, the original module is mutated"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"optimize_graph"})," \u2013 whether to do some process on origin model for special purpose. Currently only support using torch.fx to fix cat input scale(only used on Bernoulli)"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"hybrid"})," \u2013 whether to generate a hybrid model that some intermediate operation is computed in float. There are some constraints for this functionality now: 1. The hybrid model cannot pass check_model and cannot be compiled. 2. Some quantized operation cannot directly accept input from float operation, user need to manually insert QuantStub."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"optimize_kwargs"})," \u2013"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"a dict for optimize graph with the following format:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'optimize_kwargs = {\n    # optional, specify which type of optimization to do. Only\n    # support "unify_inputs_scale" now\n    "opt_types": ("unify_inputs_scale",),\n\n    # optional, modules start with qualified name to optimize\n    "module_prefixes": ("backbone.conv",),\n\n    # optional, modules in these types will be optimize\n    "module_types": (horizon.nn.qat.conv2d,),\n\n    # optional, functions to optimize\n    "functions": (torch.clamp,),\n\n    # optional, methods to optimize. Only support\n    # FloatFunctional methods now\n    "methods": ("add",),\n}\n'})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"example_inputs"})," \u2013 model inputs. It is used to trace model or check model structure."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"qconfig_setter"})," \u2013 Qconfig setter. Only needed when using qconfig template."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"verbose"})," \u2013 whether check model structure. it has two levels: 0: do nothing 1: check model structure"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"    a. if model has shared ops\n\n    b. if model has unfused operations\n\n    c. model quantization config\n"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"\nhorizon_plugin_pytorch.quantization.prepare_qat_fx(model: Union[torch.nn.modules.module.Module, torch.fx.graph_module.GraphModule], qconfig_dict: Optional[Dict[str, Any]] = None, prepare_custom_config_dict: Optional[Dict[str, Any]] = None, optimize_graph: bool = False, hybrid: bool = False, hybrid_dict: Optional[Dict[str, List]] = None, opset_version: str = 'hbdk3', example_inputs: Any = None, qconfig_setter: Optional[Union[Tuple[horizon_plugin_pytorch.quantization.qconfig_template.QconfigSetterBase, ...], horizon_plugin_pytorch.quantization.qconfig_template.QconfigSetterBase]] = None, verbose: int = 0) \u2192 horizon_plugin_pytorch.quantization.fx.graph_module.ObservedGraphModule\n"})}),"\n",(0,r.jsx)(e.p,{children:"Prepare a model for quantization aware training."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"model"})," \u2013 torch.nn.Module model or GraphModule model (maybe from fuse_fx)"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"qconfig_dict"})," \u2013"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"qconfig_dict is a dictionary with the following configurations:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'\nqconfig_dict = {\n    # optional, global config\n    "": qconfig,\n\n    # optional, used for module types\n    "module_type": [\n        (torch.nn.Conv2d, qconfig),\n        ...,\n    ],\n\n    # optional, used for module names\n    "module_name": [\n        ("foo.bar", qconfig)\n        ...,\n    ],\n    # priority (in increasing order):\n    #   global, module_type, module_name, module.qconfig\n    # qconfig == None means quantization should be\n    # skipped for anything matching the rule.\n    # The qconfig of function or method is the same as the\n    # qconfig of its parent module, if it needs to be set\n    # separately, please wrap this function as a module.\n}\n'})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"prepare_custom_config_dict"})," \u2013"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"customization configuration dictionary for quantization tool:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'prepare_custom_config_dict = {\n    # We automativally preserve all attributes, this option is\n    # just in case and not likely to be used.\n    "preserved_attributes": ["preserved_attr"],\n}\n'})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"optimize_graph"})," \u2013 whether to do some process on origin model for special purpose. Currently only support using torch.fx to fix cat input scale(only used on Bernoulli)"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"hybrid"})," \u2013 Whether prepare model in hybrid mode. Default value is False and model runs on BPU completely. It should be True if the model is quantized by model convert or contains some CPU ops. In hybrid mode, ops which aren\u2019t supported by BPU and ops which are specified by the user will run on CPU. How to set qconfig: Qconfig in hybrid mode is the same as qconfig in non-hybrid mode. For BPU op, we should ensure the input of this op is quantized, the activation qconfig of its previous non-quantstub op should not be None even if its previous non-quantstub op is a CPU op. How to specify CPU op: Define CPU module_name or module_type in hybrid_dict."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"hybrid_dict"})," \u2013"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"hybrid_dict is a dictionary to define user-specified CPU op:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'hybrid_dict = {\n    # optional, used for module types\n    "module_type": [torch.nn.Conv2d, ...],\n\n    # optional, used for module names\n    "module_name": ["foo.bar", ...],\n}\n# priority (in increasing order): module_type, module_name\n# To set a function or method as CPU op, wrap it as a module.\n'})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"opset_version"})," \u2013 opset_version specifics the version of opset that determines the behavior of hybrid mode. Ops that in the quantized opset will be considered as quantized ops and run on BPU, while ops not in the quantized opset but in the float opset will be marked as hybrid (float) ops and run on CPU. Valid options are \u201chbdk3\u201d and \u201chbdk4\u201d."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"hybrid_dict"})," \u2013 model inputs. It is used to trace model or check model structure."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"hybrid_dict"})," \u2013 Qconfig setter. Only needed when using qconfig template."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"hybrid_dict"})," \u2013 whether check model structure. It has three levels: 0: do nothing 1: check qat model structure."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"    a. if model has shared ops\n\n    b. if model has unfused operations\n\n    c. model quantization config\n"})}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"A GraphModule with fake quant modules (configured by qconfig_dict), ready for quantization aware training"}),"\n",(0,r.jsx)(e.p,{children:"Example: prepare_qat_fx example:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'\nimport torch\nfrom horizon_plugin_pytorch.quantization import get_default_qat_qconfig\nfrom horizon_plugin_pytorch.quantization import prepare_qat_fx\n\nqconfig = get_default_qat_qconfig()\ndef train_loop(model, train_data):\n    model.train()\n    for image, target in data_loader:\n        ...\n\nqconfig_dict = {"": qconfig}\nprepared_model = prepare_qat_fx(float_model, qconfig_dict)\n# Run QAT training\ntrain_loop(prepared_model, train_loop)\n'})}),"\n",(0,r.jsx)(e.p,{children:"Extended tracer and wrap of torch.fx."}),"\n",(0,r.jsx)(e.p,{children:"This file defines a inherit tracer of torch.fx.Tracer and a extended wrap to allow wrapping of user-defined Module or method, which help users do some optimization of their own module by torch.fx"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.utils.fx_helper.wrap(skip_compile: bool = False)\n"})}),"\n",(0,r.jsx)(e.p,{children:"Extend torch.fx.warp."}),"\n",(0,r.jsx)(e.p,{children:"This function can be:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"called or used as a decorator on a string to register a builtin function as a \u201cleaf function\u201d"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.ol,{start:"2",children:["\n",(0,r.jsx)(e.li,{children:"called or used as a decorator on a function to register this function as a \u201cleaf function\u201d"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.ol,{start:"3",children:["\n",(0,r.jsx)(e.li,{children:"called or used as a decorator on subclass of torch.nn.Module to register this module as a \u201cleaf module\u201d, and register all user defined method in this class as \u201cleaf method\u201d"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.ol,{start:"4",children:["\n",(0,r.jsx)(e.li,{children:"called or used as a decorator on a class method to register it as \u201cleaf method\u201d"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsx)(e.p,{children:"skip_compile \u2013 Whether the wrapped part should not be compiled."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"The actural decorator."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de\u7c7b\u578b"})}),"\n",(0,r.jsx)(e.p,{children:"wrap_inner"}),"\n",(0,r.jsx)(e.h2,{id:"onnx",children:"ONNX"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.utils.onnx_helper.export_to_onnx(model, args, f, export_params=True, verbose=False, training=<TrainingMode.EVAL: 0>, input_names=None, output_names=None, operator_export_type=<OperatorExportTypes.ONNX_FALLTHROUGH: 3>, opset_version=11, do_constant_folding=True, dynamic_axes=None, keep_initializers_as_inputs=None, custom_opsets=None)\n"})}),"\n",(0,r.jsx)(e.p,{children:"Export a (float or qat)model into ONNX format."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"model"})," (torch.nn.Module/torch.jit.ScriptModule/ScriptFunction) \u2013 the model to be exported."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"args"})," (tuple or torch.Tensor) \u2013"]}),"\n",(0,r.jsx)(e.p,{children:"args can be structured either as:"}),"\n",(0,r.jsx)(e.p,{children:"a. ONLY A TUPLE OF ARGUMENTS:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"    args = (x, y, z)\n"})}),"\n",(0,r.jsx)(e.p,{children:"The tuple should contain model inputs such that model(*args) is a valid invocation of the model. Any non-Tensor arguments will be hard-coded into the exported model; any Tensor arguments will become inputs of the exported model, in the order they occur in the tuple."}),"\n",(0,r.jsx)(e.p,{children:"b. A TENSOR:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"    args = torch.Tensor([1])\n"})}),"\n",(0,r.jsx)(e.p,{children:"This is equivalent to a 1-ary tuple of that Tensor."}),"\n",(0,r.jsx)(e.p,{children:"c. A TUPLE OF ARGUMENTS ENDING WITH A DICTIONARY OF NAMED ARGUMENTS:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"    args = (x,\n            {'y': input_y,\n            'z': input_z})\n"})}),"\n",(0,r.jsx)(e.p,{children:"All but the last element of the tuple will be passed as non-keyword arguments, and named arguments will be set from the last element. If a named argument is not present in the dictionary , it is assigned the default value, or None if a default value is not provided."}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"f"})," \u2013 a file-like object or a string containing a file name. A binary protocol buffer will be written to this file."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"export_params"})," (bool, default True) \u2013 if True, all parameters will be exported."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"verbose"})," (bool, default False) \u2013 if True, prints a description of the model being exported to stdout, doc_string will be added to graph. doc_string may contaion mapping of module scope to node name in future torch onnx."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"training"})," (enum, default TrainingMode.EVAL) \u2013"]}),"\n",(0,r.jsx)(e.p,{children:"if model.training is False and in training mode if model.training is True."}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.code,{children:"TrainingMode.EVAL"}),": export the model in inference mode."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.code,{children:"TrainingMode.PRESERVE"}),": export the model in inference mode"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.code,{children:"TrainingMode.TRAINING"}),": export the model in training mode. Disables optimizations which might interfere with training."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"input_names"})," (list of str, default empty list) \u2013 names to assign to the input nodes of the graph, in order."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"output_names"})," (list of str, default empty list) \u2013 names to assign to the output nodes of the graph, in order."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"operator_export_type"})," (enum, default ONNX_FALLTHROUGH) \u2013"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.code,{children:"OperatorExportTypes.ONNX"}),": Export all ops as regular ONNX ops (in the default opset domain)."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.code,{children:"OperatorExportTypes.ONNX_FALLTHROUGH"}),": Try to convert all ops to standard ONNX ops in the default opset domain."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.code,{children:"OperatorExportTypes.ONNX_ATEN"}),": All ATen ops (in the TorchScript namespace \u201caten\u201d) are exported as ATen ops."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.code,{children:"OperatorExportTypes.ONNX_ATEN_FALLBACK"}),": Try to export each ATen op (in the TorchScript namespace \u201caten\u201d) as a regular ONNX op. If we are unable to do so,fall back to exporting an ATen op."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"opset_version"})," (int, default 11) \u2013 by default we export the model to the opset version of the onnx submodule."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"do_constant_folding"})," (bool, default False) \u2013 Apply the constant-folding optimization. Constant-folding will replace some of the ops that have all constant inputs with pre-computed constant nodes."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"dynamic_axes"})," ",(0,r.jsx)(e.code,{children:"(dict<str, list(int)/dict<int, str>>, default empty dict) \u2013"})]}),"\n",(0,r.jsx)(e.p,{children:"By default the exported model will have the shapes of all input and output tensors set to exactly match those given in args (and example_outputs when that arg is required). To specify axes of tensors as dynamic (i.e. known only at run-time), set dynamic_axes to a dict with schema:"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.code,{children:"KEY (str)"}),": an input or output name. Each name must also be provided in input_names or output_names."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.code,{children:"VALUE (dict or list)"}),": If a dict, keys are axis indices and values are axis names. If a list, each element is an axis index."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"keep_initializers_as_inputs"})," (bool, default None) \u2013 If True, all the initializers (typically corresponding to parameters) in the exported graph will also be added as inputs to the graph. If False, then initializers are not added as inputs to the graph, and only the non-parameter inputs are added as inputs. This may allow for better optimizations (e.g. constant folding) by backends/runtimes."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"custom_opsets"})," ",(0,r.jsx)(e.code,{children:"(dict<str, int>, default empty dict) \u2013"})]}),"\n",(0,r.jsx)(e.p,{children:"A dict with schema:"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.code,{children:"KEY (str)"}),": opset domain name"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.code,{children:"VALUE (int)"}),": opset version"]}),"\n",(0,r.jsx)(e.p,{children:"If a custom opset is referenced by model but not mentioned in this dictionary, the opset version is set to 1."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"torchscript-\u6a21\u578b\u4fdd\u5b58\u548c\u52a0\u8f7d",children:"TorchScript \u6a21\u578b\u4fdd\u5b58\u548c\u52a0\u8f7d"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.jit.load(f, map_location=None, _extra_files=None)\n"})}),"\n",(0,r.jsx)(e.p,{children:"Load ScriptModule previously saved with horizon.jit.save."}),"\n",(0,r.jsx)(e.p,{children:"In addition to loaded plugin version comparsion with current plugin version, this function is same as torch.jit.save."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"f"})," \u2013 a file-like object(has to implement read, readline, tell, and seek), or a string containing a file name"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"map_location"})," (string or torch.device) \u2013 A simplified version of map_location in torch.jit.save used to dynamically remap storages to an alternative set of devices."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"_extra_files"})," (dictionary of filename to content) \u2013 The extra filenames given in the map would be loaded and their content would be stored in the provided map."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"A ScriptModule object."}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.jit.save(m, f, _extra_files=None)\n"})}),"\n",(0,r.jsx)(e.p,{children:"Save ScriptModule."}),"\n",(0,r.jsx)(e.p,{children:"In addition to plugin version saved, this function is same as torch.jit.save."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"m"})," \u2013 A ScriptModule to save."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"f"})," \u2013 A file-like object (has to implement write and flush) or a string containing a file name."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"_extra_files"})," \u2013 Map from filename to contents which will be stored as part of f."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"horizon-\u7b97\u5b50",children:"Horizon \u7b97\u5b50"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.nn.functional.filter(*inputs: Union[Tuple[torch.Tensor], Tuple[horizon_plugin_pytorch.qtensor.QTensor]], threshold: float, idx_range: Optional[Tuple[int, int]] = None) \u2192 List[List[torch.Tensor]]\n"})}),"\n",(0,r.jsx)(e.p,{children:"Filter."}),"\n",(0,r.jsx)(e.p,{children:"The output order is different with bpu, because that the compiler do some optimization and slice input following complex rules, which is hard to be done by plugin."}),"\n",(0,r.jsx)(e.p,{children:"All inputs are filtered along HW by the max value within a range in channel dim of the first input. Each NCHW input is splited, transposed and flattened to List[Tensor[H * W, C]] first. If input is QTensor, the output will be dequantized."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"inputs"})," \u2013 Data in NCHW format. Each input shold have the same size in N, H, W. The output will be selected according to the first input."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"threshold"})," \u2013 Threshold, the lower bound of output."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"idx_range"})," \u2013 The index range of values counted in compare of the first input. Defaults to None which means use all the values."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"A list with same length of batch size, and each element contains:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"max_value"}),": Flattened max value within idx_range in channel dim."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"max_idx"}),": Flattened max value index in channel dim."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"coord"}),": The original coordinates of the output data in the input data in the shape of [M, (h, w)]."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"(multi) data: Filtered data in the shape of [M, C]."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de\u7c7b\u578b"})}),"\n",(0,r.jsx)(e.p,{children:"Union[List[List[Tensor]], List[List[QTensor]]]"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.nn.functional.point_pillars_preprocess(points_list: List[torch.Tensor], pc_range: torch.Tensor, voxel_size: torch.Tensor, max_voxels: int, max_points_per_voxel: int, use_max: bool, norm_range: torch.Tensor, norm_dims: torch.Tensor) \u2192 Tuple[torch.Tensor, torch.Tensor]\n"})}),"\n",(0,r.jsx)(e.p,{children:"Preprocess PointPillars."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"points_list"})," \u2013 [(M1, ndim), (M2, ndim),\u2026], List of PointCloud data."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"pc_range"})," \u2013 (6,), indicate voxel range, format: [x_min, y_min, z_min, x_max, y_max, z_max]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"voxel_size"})," \u2013 (3,), xyz, indicate voxel size."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"max_voxels"})," \u2013 Indicate maximum voxels."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"max_points_per_voxel"})," \u2013 Indicate maximum points contained in a voxel."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"use_max"})," \u2013 Whether to use max_voxels, for deploy should be True."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"norm_range"})," \u2013 Feature range, like [x_min, y_min, z_min, \u2026, x_max, y_max, z_max, \u2026]."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"norm_dims"})," \u2013 Dims to do normalize."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"(features, coords), encoded feature and coordinates in (idx, z, y, x) format."}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"\u8fd4\u56de\u7c7b\u578b"}),"\n(Tensor, Tensor)"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"class horizon_plugin_pytorch.nn.BgrToYuv444(channel_reversal: bool = False)\n"})}),"\n",(0,r.jsx)(e.p,{children:"Convert image color format from bgr to yuv444."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"channel_reversal"})," \u2013 Color channel order, set to True when used on RGB input. Defaults to False."]}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"forward(input: torch.Tensor)\n"})}),"\n",(0,r.jsx)(e.p,{children:"Forward pass of BgrToYuv444."}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"class horizon_plugin_pytorch.nn.Correlation(kernel_size: int = 1, max_displacement: int = 1, stride1: int = 1, stride2: int = 1, pad_size: int = 0, is_multiply: bool = True)\n"})}),"\n",(0,r.jsx)(e.p,{children:"Perform multiplicative patch comparisons between two feature maps."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{src:"https://rdk-doc.oss-cn-beijing.aliyuncs.com/doc/img/07_Advanced_development/04_toolchain_development/expert/qat_correlation.png",alt:"qat_correlation"})}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"kernel_size"})," \u2013 kernel size for Correlation must be an odd number"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"max_displacement"})," \u2013 Max displacement of Correlation"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"stride1"})," \u2013 stride1 quantize data1 globally"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"stride2"})," \u2013 stride2 quantize data2 within neighborhood centered around data1"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"pad_size"})," \u2013 pad for Correlation"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"is_multiply"})," \u2013 operation type is either multiplication or subduction, only support True now"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"forward(data1: Union[torch.Tensor, horizon_plugin_pytorch.qtensor.QTensor], data2: Union[torch.Tensor, horizon_plugin_pytorch.qtensor.QTensor]) \u2192 torch.Tensor\n"})}),"\n",(0,r.jsx)(e.p,{children:"Forward for Horizon Correlation."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"data1"})," \u2013 shape of [N,C,H,W]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"data2"})," \u2013 shape of [N,C,H,W]"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"output"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de\u7c7b\u578b"})}),"\n",(0,r.jsx)(e.p,{children:"Tensor"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"class horizon_plugin_pytorch.nn.DetectionPostProcess(score_threshold: int = 0, regression_scale: Optional[Tuple[float, float, float, float]] = None, background_class_idx: Optional[int] = None, size_threshold: Optional[float] = None, image_size: Optional[Tuple[int, int]] = None, pre_decode_top_n: Optional[int] = None, post_decode_top_n: Optional[int] = None, iou_threshold: Optional[float] = None, pre_nms_top_n: Optional[int] = None, post_nms_top_n: Optional[int] = None, nms_on_each_level: bool = False, mode: str = 'normal')\n"})}),"\n",(0,r.jsx)(e.p,{children:"General post process for object detection models."}),"\n",(0,r.jsx)(e.p,{children:"Compatible with YOLO, SSD, RetinaNet, Faster-RCNN (RPN & RCNN), etc. Note that this is a float OP, please use after DequantStubs."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"score_threshold"})," \u2013 Filter boxes whose score is lower than this. Defaults to 0."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"regression_scale"})," \u2013 Scale to be multiplyed to box regressions. Defaults to None."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"background_class_idx"})," \u2013 Specify the class index to be ignored. Defaults to None."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"size_threshold"})," \u2013 Filter bixes whose height or width smaller than this. Defaults to None."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"image_size"})," \u2013 Clip boxes to image sizes. Defaults to None."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"pre_decode_top_n"})," \u2013 Get top n boxes by objectness (first element in the score vector) before decode. Defaults to None."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"post_decode_top_n"})," \u2013 Get top n boxes by score after decode. Defaults to None."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"iou_threshold"})," \u2013 IoU threshold for nms. Defaults to None."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"pre_nms_top_n"})," \u2013 Get top n boxes by score before nms. Defaults to None."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"post_nms_top_n"})," \u2013 Get top n boxes by score after nms. Defaults to None."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"nms_on_each_level"})," \u2013 Whether do nms on each level seperately. Defaults to False."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"mode"})," \u2013 Only support \u2018normal\u2019 and \u2018yolo\u2019. If set to \u2018yolo\u2019: 1. Box will be filtered by objectness rathen than classification scores. 2. dx, dy in regressions will be treated as absolute offset. 3. Objectness will be multiplyed to classification scores. Defaults to \u2018normal\u2019."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"forward(boxes: List[torch.Tensor], scores: List[torch.Tensor], regressions: List[torch.Tensor], image_shapes: Optional[torch.Tensor] = None) \u2192 Tuple[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[torch.Tensor]]\n"})}),"\n",(0,r.jsx)(e.p,{children:"Forward pass of DetectionPostProcess."}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"class horizon_plugin_pytorch.nn.DetectionPostProcessV1(num_classes: int, box_filter_threshold: float, class_offsets: List[int], use_clippings: bool, image_size: Tuple[int, int], nms_threshold: float, pre_nms_top_k: int, post_nms_top_k: int, nms_padding_mode: Optional[str] = None, nms_margin: float = 0.0, use_stable_sort: Optional[bool] = None, bbox_min_hw: Tuple[float, float] = (0, 0))\n"})}),"\n",(0,r.jsx)(e.p,{children:"Post process for object detection models. Only supported on bernoulli2."}),"\n",(0,r.jsx)(e.p,{children:"This operation is implemented on BPU, thus is expected to be faster than cpu implementation. This operation requires input_scale = 1 / 2 ** 4, or a rescale will be applied to the input data. So you can manually set the output scale of previous op (Conv2d for example) to 1 / 2 ** 4 to avoid the rescale and get best performance and accuracy."}),"\n",(0,r.jsx)(e.p,{children:"Major differences with DetectionPostProcess:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"Each anchor will generate only one pred bbox totally, but in DetectionPostProcess each anchor will generate one bbox for each class (num_classes bboxes totally)."}),"\n",(0,r.jsx)(e.li,{children:"NMS has a margin param, box2 will only be supressed by box1 when box1.score - box2.score > margin (box1.score > box2.score in DetectionPostProcess)."}),"\n",(0,r.jsx)(e.li,{children:"A offset can be added to the output class indices ( using class_offsets)."}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"num_classes"})," \u2013 Class number."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"box_filter_threshold"})," \u2013 Default threshold to filter box by max score."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"class_offsets"})," \u2013 Offset to be added to output class index for each branch."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"use_clippings"})," \u2013 Whether clip box to image size. If input is padded, you can clip box to real content by providing image size."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"image_size"})," \u2013 Fixed image size in (h, w), set to None if input have different sizes."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"nms_threshold"})," \u2013 IoU threshold for nms."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"nms_margin"})," \u2013 Only supress box2 when box1.score - box2.score > nms_margin"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"pre_nms_top_k"})," \u2013 Maximum number of bounding boxes in each image before nms."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"post_nms_top_k"})," \u2013 Maximum number of output bounding boxes in each image."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"nms_padding_mode"})," \u2013 The way to pad bbox to match the number of output bounding bouxes to post_nms_top_k, can be None, \u201cpad_zero\u201d or \u201crollover\u201d."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"bbox_min_hw"})," \u2013 Minimum height and width of selected bounding boxes."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"exp_overwrite"})," \u2013 Overwrite the exp func in box decode."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"input_shift"})," \u2013 Customize input shift of quantized DPP."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"forward(data: List[torch.Tensor], anchors: List[torch.Tensor], image_sizes: Tuple[int, int] = None) \u2192 torch.Tensor\n"})}),"\n",(0,r.jsx)(e.p,{children:"Forward pass of DetectionPostProcessV1."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"data"})," \u2013 (N, (4 + num_classes) * anchor_num, H, W)"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"anchors"})," \u2013 (N, anchor_num * 4, H, W)"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"image_sizes"})," \u2013 Defaults to None."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"list of (bbox (x1, y1, x2, y2), score, class_idx)."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de\u7c7b\u578b"})}),"\n",(0,r.jsx)(e.p,{children:"List[Tuple[Tensor, Tensor, Tensor]]"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"class horizon_plugin_pytorch.nn.MultiScaleDeformableAttention(embed_dims: int = 256, num_heads: int = 8, num_levels: int = 4, num_points: int = 4, im2col_step: int = 64, dropout: float = 0.1, batch_first: bool = False, value_proj_ratio: float = 1.0)\n"})}),"\n",(0,r.jsx)(e.p,{children:"An attention module used in Deformable-Detr."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2010.04159",children:"Deformable DETR: Deformable Transformers for End-to-End Object Detection."})}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"embed_dims"})," \u2013 The embedding dimension of Attention. Default: 256."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"num_heads"})," \u2013 Parallel attention heads. Default: 8."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"num_levels"})," \u2013 The number of feature map used in Attention. Default: 4."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"num_points"})," \u2013 The number of sampling points for each query in each head. Default: 4."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"im2col_step"})," \u2013 The step used in image_to_column. Default: 64."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"dropout"})," \u2013 A Dropout layer on inp_identity. Default: 0.1."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"batch_first"})," \u2013 Key, Query and Value are shape of (batch, n, embed_dim) or (n, batch, embed_dim). Default to False."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"value_proj_ratio"})," \u2013 The expansion ratio of value_proj. Default: 1.0."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"forward(query: Union[torch.Tensor, horizon_plugin_pytorch.qtensor.QTensor], key: Optional[Union[torch.Tensor, horizon_plugin_pytorch.qtensor.QTensor]] = None, value: Optional[Union[torch.Tensor, horizon_plugin_pytorch.qtensor.QTensor]] = None, identity: Optional[Union[torch.Tensor, horizon_plugin_pytorch.qtensor.QTensor]] = None, query_pos: Optional[Union[torch.Tensor, horizon_plugin_pytorch.qtensor.QTensor]] = None, key_padding_mask: Optional[torch.Tensor] = None, reference_points: Optional[Union[torch.Tensor, horizon_plugin_pytorch.qtensor.QTensor]] = None, spatial_shapes: Optional[torch.Tensor] = None) \u2192 torch.Tensor\n"})}),"\n",(0,r.jsx)(e.p,{children:"Forward Function of MultiScaleDeformAttention."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"query"})," \u2013 Query of Transformer with shape (num_query, bs, embed_dims)."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"key"})," \u2013 The key tensor with shape (num_key, bs, embed_dims)."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"value"})," \u2013 The value tensor with shape (num_key, bs, embed_dims)."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"identity"})," \u2013 The tensor used for addition, with the same shape as query. Default None. If None, query will be used."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"query_pos"})," \u2013 The positional encoding for query. Default: None."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"key_padding_mask"})," \u2013 ByteTensor for query, with shape [bs, num_key]."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"reference_points"})," \u2013 The normalized reference points with shape (bs, num_query, num_levels, 2), all elements is range in [0, 1], top-left (0,0), bottom-right (1, 1), including padding area. or (bs, num_query, num_levels, 4), add additional two dimensions is (w, h) to form reference boxes."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"spatial_shapes"})," \u2013 Spatial shape of features in different levels. int tensor with shape (num_levels, 2), last dimension represents (h, w)."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"the same shape with query."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de\u7c7b\u578b"})}),"\n",(0,r.jsx)(e.p,{children:"Tensor"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"class horizon_plugin_pytorch.nn.PointPillarsScatter(output_shape=None)\n\nforward(voxel_features: torch.Tensor, coords: torch.Tensor, output_shape: Optional[Union[torch.Tensor, list, tuple]] = None) \u2192 torch.Tensor\n"})}),"\n",(0,r.jsx)(e.p,{children:"Forward of Horizon PointPillarsScatter."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"voxel_features"})," \u2013 [M, \u2026], dimention after M will be flattened."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"coords"})," \u2013 [M, (n, \u2026, y, x)], only indices on N, H and W are used."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"output_shape"})," \u2013 Expected output shape. Defaults to None."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"The NCHW pseudo image."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de\u7c7b\u578b"})}),"\n",(0,r.jsx)(e.p,{children:"Tensor"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"class horizon_plugin_pytorch.nn.RcnnPostProcess(image_size: Tuple[int, int] = (1024, 1024), nms_threshold: float = 0.3, box_filter_threshold: float = 0.1, num_classes: int = 1, post_nms_top_k: int = 100, delta_mean: List[float] = (0.0, 0.0, 0.0, 0.0), delta_std: List[float] = (1.0, 1.0, 1.0, 1.0))\n"})}),"\n",(0,r.jsx)(e.p,{children:"Post Process of RCNN output."}),"\n",(0,r.jsx)(e.p,{children:"Given bounding boxes and corresponding scores and deltas, decodes bounding boxes and performs NMS. In details, it consists of:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:"Argmax on multi-class scores"}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:"Filter out those belows the given threshold"}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:"Non-linear Transformation, convert box deltas to original image coordinates"}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:"Bin-sort remaining boxes on score"}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:"Apply class-aware NMS and return the firstnms_output_box_num of boxes"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"image_size"})," \u2013 a int tuple of (h, w), for fixed image size"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"nms_threshold"})," \u2013 bounding boxes of IOU greater than nms_threshold will be suppressed"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"box_filter_threshold"})," \u2013 bounding boxes of scores less than box_filter_threshold will be discarded"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"num_classes"})," \u2013 total number of classes"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"post_nms_top_k"})," \u2013 number of bounding boxes after NMS in each image"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"delta_mean"})," \u2013 a float list of size 4"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"delta_std"})," \u2013 a float list of size 4"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"forward(boxes: List[torch.Tensor], scores: torch.Tensor, deltas: torch.Tensor, image_sizes: Optional[torch.Tensor] = None)\n"})}),"\n",(0,r.jsx)(e.p,{children:"Forward of RcnnPostProcess."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"boxes"})," \u2013 list of box of shape [box_num, (x1, y1, x2, y2)]. can be Tensor(float), QTensor(float, int)"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"scores"})," \u2013 shape is [num_batch * num_box, num_classes + 1, 1, 1,], dtype is float32"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"deltas"})," \u2013 shape is [num_batch * num_box, (num_classes + 1) * 4, 1, 1,], dtype is float32"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"image_sizes"})," \u2013 shape is [num_batch, 2], dtype is int32, for dynamic image size, can be None. Defaults to None"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"output data in format\n[x1, y1, x2, y2, score, class_index], dtype is float32 if the output boxes number is less than post_nms_top_k, they are padded with -1.0"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de\u7c7b\u578b"})}),"\n",(0,r.jsx)(e.p,{children:"Tensor[num_batch, post_nms_top_k, 6]"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"class horizon_plugin_pytorch.nn.SoftmaxBernoulli2(dim: int = None, max_value_only: Optional[bool] = False)\n"})}),"\n",(0,r.jsx)(e.p,{children:"SoftmaxBernoulli2 is designed to run on Bernoulli2."}),"\n",(0,r.jsx)(e.p,{children:"This operator is considered hacky and should not been used by most users."}),"\n",(0,r.jsx)(e.p,{children:"The calculation logic of this operator is as follows roughly:\ny = exp(x - x.max(dim)) / sum(exp(x - x.max(dim)), dim)"}),"\n",(0,r.jsx)(e.p,{children:"The output of this operator is float type and cannot be fed into other quantized operators."}),"\n",(0,r.jsx)(e.p,{children:"In the FLOAT phase, users can set qconfig to this operator as usual. However, there are some peculiarities in QAT and QUANTIZED inference phases. Please read the following carefully."}),"\n",(0,r.jsx)(e.p,{children:"In the QAT phase, the operator only applies fake quantization to exp(x), then computes the division in the float domain and returns the unfakequantized(float) result directly. This operator will ignore the qconfig set by users or propagated from the parent module. However, to integrate this into the workflow of converting QAT models to QUANTIZED models, a reasonable qconfig is needed."}),"\n",(0,r.jsx)(e.p,{children:"In the QUANTIZED inference phase, the operator retrieves the result of exp(x) from a lookup table and computes the division in the float domain."}),"\n",(0,r.jsx)(e.p,{children:"When max_value_only is set to True, the maximum value of softmax along dim will be returned, which is equal to max(softmax(x, dim), dim). We combine softmax and max in this op because the hbdk compiler requires it to optimize performance without the effort of graph analysis. This argument is only intended for this specific purpose and should not be used in other cases."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"dim"})," \u2013 The dimension along which Softmax will be computed. only supports dim=1."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"max_value_only"})," \u2013 If True, return the max value along dim, if False, equal to normal softmax. Refer to the above for more information."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.bgr2centered_gray(input: torch.Tensor) \u2192 torch.Tensor\n"})}),"\n",(0,r.jsx)(e.p,{children:"Convert color space."}),"\n",(0,r.jsx)(e.p,{children:"Convert images from BGR format to centered gray"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"input"})," \u2013 input image in BGR format of shape [N, 3, H, W], ranging 0~255"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"centered gray image of shape [N, 1, H, W], ranging -128~127"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de\u7c7b\u578b"})}),"\n",(0,r.jsx)(e.p,{children:"Tensor"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.bgr2centered_yuv(input: torch.Tensor, swing: str = 'studio') \u2192 torch.Tensor\n"})}),"\n",(0,r.jsx)(e.p,{children:"Convert color space."}),"\n",(0,r.jsx)(e.p,{children:"Convert images from BGR format to centered YUV444 BT.601"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"input"})," \u2013 input image in BGR format, ranging 0~255"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"swing"})," \u2013 \u201cstudio\u201d for YUV studio swing (Y: -112",(0,r.jsx)(e.del,{children:"107, U, V: -112"}),"112). \u201cfull\u201d for YUV full swing (Y, U, V: -128~127). default is \u201cstudio\u201d"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"centered YUV image"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de\u7c7b\u578b"})}),"\n",(0,r.jsx)(e.p,{children:"Tensor"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.bgr2gray(input: torch.Tensor) \u2192 torch.Tensor\n"})}),"\n",(0,r.jsx)(e.p,{children:"Convert color space."}),"\n",(0,r.jsx)(e.p,{children:"Convert images from BGR format to gray"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"input"})," \u2013 input image in BGR format of shape [N, 3, H, W], ranging 0~255"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"gray image of shape [N, 1, H, W], ranging 0~255"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de\u7c7b\u578b"})}),"\n",(0,r.jsx)(e.p,{children:"Tensor"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.bgr2rgb(input: torch.Tensor) \u2192 torch.Tensor\n"})}),"\n",(0,r.jsx)(e.p,{children:"Convert color space."}),"\n",(0,r.jsx)(e.p,{children:"Convert images from BGR format to RGB"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"input"})," \u2013 image in BGR format with shape [N, 3, H, W]"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"image in RGB format with shape [N, 3, H, W]"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de\u7c7b\u578b"})}),"\n",(0,r.jsx)(e.p,{children:"Tensor"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.bgr2yuv(input: torch.Tensor, swing: str = 'studio') \u2192 torch.Tensor\n"})}),"\n",(0,r.jsx)(e.p,{children:"Convert color space."}),"\n",(0,r.jsx)(e.p,{children:"Convert images from BGR format to YUV444 BT.601"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"input"})," \u2013 input image in BGR format, ranging 0~255"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"swing"})," \u2013 \u201cstudio\u201d for YUV studio swing (Y: 16",(0,r.jsx)(e.del,{children:"235, U, V: 16"}),"240). \u201cfull\u201d for YUV full swing (Y, U, V: 0~255). default is \u201cstudio\u201d"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"YUV image"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de\u7c7b\u578b"})}),"\n",(0,r.jsx)(e.p,{children:"Tensor"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.centered_yuv2bgr(input: horizon_plugin_pytorch.qtensor.QTensor, swing: str = 'studio', mean: Union[List[float], torch.Tensor] = (128.0,), std: Union[List[float], torch.Tensor] = (128.0,), q_scale: Union[float, torch.Tensor] = 0.0078125) \u2192 horizon_plugin_pytorch.qtensor.QTensor\n"})}),"\n",(0,r.jsx)(e.p,{children:"Convert color space."}),"\n",(0,r.jsx)(e.p,{children:"Convert images from centered YUV444 BT.601 format to transformed and quantized BGR. Only use this operator in the quantized model. Insert it after QuantStub. Pass the scale of QuantStub to the q_scale argument and set scale of QuantStub to 1 afterwards."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"input"})," \u2013 Input images in centered YUV444 BT.601 format, centered by the pyramid with -128."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"swing"})," \u2013 \u201cstudio\u201d for YUV studio swing (Y: -112",(0,r.jsx)(e.del,{children:"107, U, V: -112"}),"112). \u201cfull\u201d for YUV full swing (Y, U, V: -128~127). default is \u201cstudio\u201d"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"mean"})," \u2013 BGR mean, a list of float, or torch.Tensor, can be a scalar [float], or [float, float, float] for per-channel mean."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"std"})," \u2013 BGR standard deviation, a list of float, or torch.Tensor, can be a scalar [float], or [float, float, float] for per-channel std."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"q_scale"})," \u2013 BGR quantization scale."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"Transformed and quantized image in BGR color, dtype is qint8."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de\u7c7b\u578b"})}),"\n",(0,r.jsx)(e.p,{children:"QTensor"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.centered_yuv2rgb(input: horizon_plugin_pytorch.qtensor.QTensor, swing: str = 'studio', mean: Union[List[float], torch.Tensor] = (128.0,), std: Union[List[float], torch.Tensor] = (128.0,), q_scale: Union[float, torch.Tensor] = 0.0078125) \u2192 horizon_plugin_pytorch.qtensor.QTensor\n"})}),"\n",(0,r.jsx)(e.p,{children:"Convert color space."}),"\n",(0,r.jsx)(e.p,{children:"Convert images from centered YUV444 BT.601 format to transformed and quantized RGB. Only use this operator in the quantized model. Insert it after QuantStub. Pass the scale of QuantStub to the q_scale argument and set scale of QuantStub to 1 afterwards."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"input"})," \u2013 Input images in centered YUV444 BT.601 format, centered by the pyramid with -128."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"swing"})," \u2013 \u201cstudio\u201d for YUV studio swing (Y: -112",(0,r.jsx)(e.del,{children:"107, U, V: -112"}),"112). \u201cfull\u201d for YUV full swing (Y, U, V: -128~127). default is \u201cstudio\u201d"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"mean"})," \u2013 RGB mean, a list of float, or torch.Tensor, can be a scalar [float], or [float, float, float] for per-channel mean."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"std"})," \u2013 RGB standard deviation, a list of float, or torch.Tensor, can be a scalar [float], or [float, float, float] for per-channel std."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"q_scale"})," \u2013 RGB quantization scale."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"Transformed and quantized image in RGB color, dtype is qint8."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de\u7c7b\u578b"})}),"\n",(0,r.jsx)(e.p,{children:"QTensor"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.rgb2bgr(input: torch.Tensor) \u2192 torch.Tensor\n"})}),"\n",(0,r.jsx)(e.p,{children:"Convert color space."}),"\n",(0,r.jsx)(e.p,{children:"Convert images from RGB format to BGR"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"input"})," \u2013 image in RGB format with shape [N, 3, H, W]"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"image in BGR format with shape [N, 3, H, W]"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de\u7c7b\u578b"})}),"\n",(0,r.jsx)(e.p,{children:"Tensor"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.rgb2centered_gray(input: torch.Tensor) \u2192 torch.Tensor\n"})}),"\n",(0,r.jsx)(e.p,{children:"Convert color space."}),"\n",(0,r.jsx)(e.p,{children:"Convert images from RGB format to centered gray"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"input"})," \u2013 input image in RGB format of shape [N, 3, H, W], ranging 0~255"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"centered gray image of shape [N, 1, H, W], ranging -128~127"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de\u7c7b\u578b"})}),"\n",(0,r.jsx)(e.p,{children:"Tensor"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.rgb2centered_yuv(input: torch.Tensor, swing: str = 'studio') \u2192 torch.Tensor\n"})}),"\n",(0,r.jsx)(e.p,{children:"Convert color space."}),"\n",(0,r.jsx)(e.p,{children:"Convert images from RGB format to centered YUV444 BT.601"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"input"})," \u2013 input image in RGB format, ranging 0~255"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"swing"})," \u2013 \u201cstudio\u201d for YUV studio swing (Y: -112",(0,r.jsx)(e.del,{children:"107, U, V: -112"}),"112). \u201cfull\u201d for YUV full swing (Y, U, V: -128~127). default is \u201cstudio\u201d"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"centered YUV image"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de\u7c7b\u578b"})}),"\n",(0,r.jsx)(e.p,{children:"Tensor"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.rgb2gray(input: torch.Tensor) \u2192 torch.Tensor\n"})}),"\n",(0,r.jsx)(e.p,{children:"Convert color space."}),"\n",(0,r.jsx)(e.p,{children:"Convert images from RGB format to gray"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"input"})," \u2013 input image in RGB format of shape [N, 3, H, W], ranging 0~255"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"gray image of shape [N, 1, H, W], ranging 0~255"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de\u7c7b\u578b"})}),"\n",(0,r.jsx)(e.p,{children:"Tensor"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.rgb2yuv(input: torch.Tensor, swing: str = 'studio') \u2192 torch.Tensor\n"})}),"\n",(0,r.jsx)(e.p,{children:"Convert color space."}),"\n",(0,r.jsx)(e.p,{children:"Convert images from RGB format to YUV444 BT.601"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"input"})," \u2013 input image in RGB format, ranging 0~255"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"swing"})," \u2013 \u201cstudio\u201d for YUV studio swing (Y: 16",(0,r.jsx)(e.del,{children:"235, U, V: 16"}),"240). \u201cfull\u201d for YUV full swing (Y, U, V: 0~255). default is \u201cstudio\u201d"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"YUV image"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de\u7c7b\u578b"})}),"\n",(0,r.jsx)(e.p,{children:"Tensor"}),"\n",(0,r.jsx)(e.h2,{id:"\u6a21\u578b\u7f16\u8bd1",children:"\u6a21\u578b\u7f16\u8bd1"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.quantization.check_model(module: Union[torch.jit._script.ScriptModule, torch.nn.modules.module.Module], example_inputs: tuple, march: Optional[str] = None, input_source: Union[Sequence[str], str] = 'ddr', advice: Optional[int] = None, check_quanti_param: bool = True)\n"})}),"\n",(0,r.jsx)(e.p,{children:"Check if nn.Module or jit.ScriptModule can be compiled by HBDK."}),"\n",(0,r.jsx)(e.p,{children:"Dump advices for improving performance on BPU."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"module"})," (nn.Module or jit.ScriptModule.) \u2013"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"example_inputs"})," (A tuple of example inputs, in torch.tensor format.) \u2013 For jit.trace and shape inference."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"march"})," (Specify the target march of bpu.) \u2013 Valid options are bayes and bernoulli2 and bayes-e. If not provided, use horizon plugin global march."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"input_source"})," (Specify input features' sources(ddr/resizer/pyramid)) \u2013"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"advice"})," (Print HBDK compiler advices for improving the utilization of the) \u2013 model on bpu if layers of the model become slow by more than the specified time (in microseconds)"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"check_quanti_param"})," (Check quanti param) \u2013"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"flag \u2013 0 if pass, otherwise not."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de\u7c7b\u578b"})}),"\n",(0,r.jsx)(e.p,{children:"int"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.quantization.compile_model(module: Union[torch.jit._script.ScriptModule, torch.nn.modules.module.Module], example_inputs: tuple, hbm: str, march: Optional[str] = None, name: Optional[str] = None, input_source: Union[Sequence[str], str] = 'ddr', input_layout: Optional[str] = None, output_layout: str = 'NCHW', opt: Union[str, int] = 'O2', balance_factor: int = 2, progressbar: bool = True, jobs: int = 16, debug: bool = True, extra_args: Optional[list] = None)\n"})}),"\n",(0,r.jsx)(e.p,{children:"Compile the nn.Module or jit.ScriptModule."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"module"})," (nn.Module or jit.ScriptModule.) \u2013"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"example_inputs"})," (A tuple of example inputs, in torch.tensor format.) \u2013 For jit.trace and shape inference."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"hbm"})," (Specify the output path of hbdk-cc.) \u2013"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"march"})," (Specify the target march of bpu.) \u2013 Valid options are bayes and bernoulli2 and bayes-e. If not provided, use horizon plugin global march."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"name"})," (Name of the model, recorded in hbm.) \u2013 Can be obtained by hbdk-disas or hbrtGetModelNamesInHBM in runtime."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"input_source"})," (Specify input features' sources(ddr/resizer/pyramid)) \u2013"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"input_layout"})," (Specify input layout of all model inputs.) \u2013 Available layouts are NHWC, NCHW, BPU_RAW."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"output_layout"})," (Specify input layout of all model inputs.) \u2013 Available layouts are NHWC, NCHW, BPU_RAW."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"opt"})," (Specify optimization options.) \u2013 Available options are O0, O1, O2, O3, ddr, fast, balance."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"balance_factor"})," (Specify the balance ratio when optimization options is) \u2013 \u2018balance\u2019."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"progressbar"})," (Show compilation progress to alleviate anxiety.) \u2013"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"jobs"})," (Specify number of threads launched during compiler optimization.) \u2013 Default is \u201816\u2019. 0 means use all available hardware concurrency."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"debug"})," (Enable debugging info in hbm.) \u2013"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"extra_args"}),' (specify extra args listed in "hbdk-cc -h".) \u2013 format in list of string: e.g. [\u2019\u2013ability-entry\u2019, str(entry_value), \u2026]']}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"})}),"\n",(0,r.jsx)(e.p,{children:"flag \u2013 0 if pass, otherwise not."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u8fd4\u56de\u7c7b\u578b"})}),"\n",(0,r.jsx)(e.p,{children:"int"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.quantization.export_hbir(module: Union[torch.jit._script.ScriptModule, torch.nn.modules.module.Module], example_inputs: tuple, hbir: str, march: Optional[str] = None)\n"})}),"\n",(0,r.jsx)(e.p,{children:"Export the nn.Module or jit.ScriptModule to hbdk3.HBIR."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"module"})," (nn.Module or jit.ScriptModule.) \u2013"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"example_inputs"})," (A tuple of example inputs, in torch.tensor format.) \u2013 For jit.trace and shape inference."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"hbir"})," (Specify the output path of hbir.) \u2013"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"march"})," (Specify march to export hbir.) \u2013 Valid options are bayes and bernoulli2 and bayes-e. If not provided, use horizon plugin global march."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"}),"\n",(0,r.jsx)(e.strong,{children:"\u8fd4\u56de\u7c7b\u578b"})]}),"\n",(0,r.jsx)(e.p,{children:"input names and output names"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.quantization.perf_model(module: Union[torch.jit._script.ScriptModule, torch.nn.modules.module.Module], example_inputs: tuple, march: Optional[str] = None, out_dir: str = '.', name: Optional[str] = None, hbm: Optional[str] = None, input_source: Union[Sequence[str], str] = 'ddr', input_layout: Optional[str] = None, output_layout: str = 'NCHW', opt: Union[str, int] = 'O3', balance_factor: int = 2, progressbar: bool = True, jobs: int = 16, layer_details: bool = False, extra_args: Optional[list] = None)\n"})}),"\n",(0,r.jsx)(e.p,{children:"Estimate the performance of nn.Module or jit.ScriptModule."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"module"})," (nn.Module or jit.ScriptModule.) \u2013"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"example_inputs"})," (A tuple of example inputs, in torch.tensor format.) \u2013 For jit.trace and shape inference."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"march"})," (Specify the target march of bpu.) \u2013 Valid options are bayes and bernoulli2 and bayes-e. If not provided, use horizon plugin global march."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"out_dir"})," (Specify the output directry to hold the performance results.) \u2013"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"name"})," (Name of the model, recorded in hbm.) \u2013 Can be obtained by hbdk-disas or hbrtGetModelNamesInHBM in runtime."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"hbm"})," (Specify the output path of hbdk-cc.) \u2013"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"input_source"})," (Specify input features' sources(ddr/resizer/pyramid)) \u2013"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"input_layout"})," (Specify input layout of all model inputs.) \u2013 Available layouts are NHWC, NCHW, BPU_RAW."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"output_layout"})," (Specify input layout of all model inputs.) \u2013 Available layouts are NHWC, NCHW, BPU_RAW."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"opt"})," (Specify optimization options.) \u2013 Available options are O0, O1, O2, O3, ddr, fast, balance."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"balance_factor"})," (Specify the balance ratio when optimization options is) \u2013 \u2018balance\u2019."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"progressbar"})," (Show compilation progress to alleviate anxiety.) \u2013"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"jobs"})," (Specify number of threads launched during compiler optimization.) \u2013 Default is \u201816\u2019. 0 means use all available hardware concurrency."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"layer_details"})," (show layer performance details. (dev use only)) \u2013"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"extra_args"}),' (specify extra args listed in "hbdk-cc -h".) \u2013 format in list of string: e.g. [\u2019\u2013ability-entry\u2019, str(entry_value), \u2026]']}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"}),"\n",(0,r.jsx)(e.strong,{children:"\u8fd4\u56de\u7c7b\u578b"})]}),"\n",(0,r.jsx)(e.p,{children:"Performance details in json dict. Or error code when fail."}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"horizon_plugin_pytorch.quantization.visualize_model(module: Union[torch.jit._script.ScriptModule, torch.nn.modules.module.Module], example_inputs: tuple, march: Optional[str] = None, save_path: Optional[str] = None, show: bool = True)\n"})}),"\n",(0,r.jsx)(e.p,{children:"Visualize nn.Module or jit.ScriptModule at the view of HBDK."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"\u53c2\u6570"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"module"})," (nn.Module or jit.ScriptModule.) \u2013"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"example_inputs"})," (A tuple of example inputs, in torch.tensor format.) \u2013 For jit.trace and shape inference."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"march"})," (Specify the target march of bpu.) \u2013 Valid options are bayes and bernoulli2 and bayes-e. If not provided, use horizon plugin global march."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"save_path"})," (Specify path to save the plot image.) \u2013"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"show"})," (Display the plotted image via display.) \u2013 Make sure X-server is correctly configured."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"\u8fd4\u56de"}),"\n",(0,r.jsx)(e.strong,{children:"\u8fd4\u56de\u7c7b\u578b"})]}),"\n",(0,r.jsx)(e.p,{children:"None"})]})}function h(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(c,{...n})}):c(n)}}}]);